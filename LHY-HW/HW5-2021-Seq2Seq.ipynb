{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "s4uNq4OUsqkS"
   },
   "source": [
    "# **Homework 5 - Sequence-to-sequence**\n",
    "\n",
    "2021 年 HW5\n",
    "\n",
    "若有任何問題，歡迎來信至助教信箱 ntu-ml-2021spring-ta@googlegroups.com\n",
    "\n",
    "- Link to [tutorial]() (TBA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4b3pTBNsqke"
   },
   "source": [
    "# Sequence-to-Sequence 介紹\n",
    "- 大多數常見的 seq2seq model 為 encoder-decoder model，主要由兩個部分組成，分別是 encoder 和 decoder，而這兩個部可以使用 recurrent neural network (RNN)或 transformer 來實作，主要是用來解決輸入和輸出的長度不一樣的情況\n",
    "- **Encoder** 是將一連串的輸入，如文字、影片、聲音訊號等，編碼為單個向量，這單個向量可以想像為是整個輸入的抽象表示，包含了整個輸入的資訊\n",
    "- **Decoder** 是將 encoder 輸出的單個向量逐步解碼，一次輸出一個結果，直到將最後目標輸出被產生出來為止，每次輸出會影響下一次的輸出，一般會在開頭加入 \"< BOS >\" 來表示開始解碼，會在結尾輸出 \"< EOS >\" 來表示輸出結束\n",
    "\n",
    "\n",
    "![seq2seq](https://i.imgur.com/0zeDyuI.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBtwjLeZsqkg"
   },
   "source": [
    "# 作業介紹\n",
    "- 英文翻譯中文\n",
    "  - 輸入： 一句英文 （e.g.\t\ttom is a student .） \n",
    "  - 輸出： 中文翻譯 （e.g. \t\t湯姆 是 個 學生 。）\n",
    "\n",
    "- TODO\n",
    "  - 訓練一個 RNN 模型達到 Seq2seq 翻譯\n",
    "  - 訓練一個 Transformer 大幅提升效能\n",
    "  - 實作 Back-translation 大幅提升效能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujbwECV9sqkh"
   },
   "source": [
    "# 下載和引入需要的函式庫"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "z0rSekkNsqkh"
   },
   "source": [
    "第三方库安装：\n",
    "\n",
    "```bash\n",
    "pip install 'torch>=1.6.0' editdistance matplotlib sacrebleu sacremoses sentencepiece tqdm wandb\n",
    "pip install --upgrade jupyter ipywidgets\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "QKgTjk7Psqki"
   },
   "source": [
    "```bash\n",
    "git clone https://github.com/pytorch/fairseq.git\n",
    "cd fairseq && git checkout 9a1c497\n",
    "pip install --upgrade ./fairseq/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "q2hPnvIpsqkj"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pdb\n",
    "import pprint\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import tqdm.auto as tqdm\n",
    "from pathlib import Path\n",
    "from argparse import Namespace\n",
    "from fairseq import utils\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0SokhAvEsqkj"
   },
   "source": [
    "# 設定種子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JtsGxPFZsqkk"
   },
   "outputs": [],
   "source": [
    "seed = 73\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "np.random.seed(seed)  \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lpx0mMKvsqkk"
   },
   "source": [
    "# 資料集介紹\n",
    "\n",
    "## 英轉繁雙語資料\n",
    "* [TED2020](#reimers-2020-multilingual-sentence-bert)\n",
    "    - 原始資料量: 398,066句    \n",
    "    - 處理後資料: 393,980句\n",
    "    \n",
    "\n",
    "## 測試資料\n",
    "- 資料量: 4,000句\n",
    "- **中文部分不公開，提供的檔案為假翻譯，全部都是句點。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6ICndcwsqkl"
   },
   "source": [
    "# 資料下載"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rz8ejiDysqkl"
   },
   "source": [
    "### 安裝megatools (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "speRSkeosqkl"
   },
   "outputs": [],
   "source": [
    "#!apt-get install megatools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBoh6yBksqkm"
   },
   "source": [
    "## 下載檔案並解壓縮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qdb_grqAsqkm"
   },
   "outputs": [],
   "source": [
    "# 数据位置：/root/yubin/dataset/lhy-homework/ML2021-HW5/rawdata/ted2020\n",
    "\n",
    "data_dir = './DATA/rawdata'\n",
    "dataset_name = 'ted2020'\n",
    "urls = (\n",
    "    '\"https://onedrive.live.com/download?cid=3E549F3B24B238B4&resid=3E549F3B24B238B4%214989&authkey=AGgQ-DaR8eFSl1A\"', \n",
    "    '\"https://onedrive.live.com/download?cid=3E549F3B24B238B4&resid=3E549F3B24B238B4%214987&authkey=AA4qP_azsicwZZM\"',\n",
    "# # If the above links die, use the following instead. \n",
    "#     \"https://www.csie.ntu.edu.tw/~r09922057/ML2021-hw5/ted2020.tgz\",\n",
    "#     \"https://www.csie.ntu.edu.tw/~r09922057/ML2021-hw5/test.tgz\",\n",
    "# # If the above links die, use the following instead. \n",
    "#     \"https://mega.nz/#!vEcTCISJ!3Rw0eHTZWPpdHBTbQEqBDikDEdFPr7fI8WxaXK9yZ9U\",\n",
    "#     \"https://mega.nz/#!zNcnGIoJ!oPJX9AvVVs11jc0SaK6vxP_lFUNTkEcK2WbxJpvjU5Y\",\n",
    ")\n",
    "file_names = (\n",
    "    'ted2020.tgz', # train & dev\n",
    "    'test.tgz', # test\n",
    ")\n",
    "prefix = Path(data_dir).absolute() / dataset_name\n",
    "\n",
    "prefix.mkdir(parents=True, exist_ok=True)\n",
    "for u, f in zip(urls, file_names):\n",
    "    path = prefix/f\n",
    "    if not path.exists():\n",
    "        if 'mega' in u:\n",
    "            !megadl {u} --path {path}\n",
    "        else:\n",
    "            !wget {u} -O {path}\n",
    "    if path.suffix == \".tgz\":\n",
    "        !tar -xvf {path} -C {prefix}\n",
    "    elif path.suffix == \".zip\":\n",
    "        !unzip -o {path} -d {prefix}\n",
    "!mv {prefix/'raw.en'} {prefix/'train_dev.raw.en'}\n",
    "!mv {prefix/'raw.zh'} {prefix/'train_dev.raw.zh'}\n",
    "!mv {prefix/'test.en'} {prefix/'test.raw.en'}\n",
    "!mv {prefix/'test.zh'} {prefix/'test.raw.zh'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djXV5NPFsqkn"
   },
   "source": [
    "## 設定語言"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ld-Bphtrsqkn"
   },
   "outputs": [],
   "source": [
    "src_lang = 'en'\n",
    "tgt_lang = 'zh'\n",
    "\n",
    "data_prefix = r'/root/yubin/dataset/lhy-homework/ML2021-HW5/rawdata/ted2020/train_dev.raw'\n",
    "test_prefix = r'/root/yubin/dataset/lhy-homework/ML2021-HW5/rawdata/ted2020/test.raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4gejawdXsqko"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you so much, Chris.\n",
      "And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\n",
      "I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night.\n",
      "And I say that sincerely, partly because  I need that.\n",
      "Put yourselves in my position.\n",
      "非常謝謝你，克里斯。能有這個機會第二度踏上這個演講台\n",
      "真是一大榮幸。我非常感激。\n",
      "這個研討會給我留下了極為深刻的印象，我想感謝大家 對我之前演講的好評。\n",
      "我是由衷的想這麼說，有部份原因是因為 —— 我真的有需要!\n",
      "請你們設身處地為我想一想！\n"
     ]
    }
   ],
   "source": [
    "!head {data_prefix+'.'+src_lang} -n 5\n",
    "!head {data_prefix+'.'+tgt_lang} -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvH3snDqsqko"
   },
   "source": [
    "## 檔案前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2GZQLH9ksqko"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def strQ2B(ustring):\n",
    "    \"\"\"把字串全形轉半形\"\"\"\n",
    "    # 參考來源:https://ithelp.ithome.com.tw/articles/10233122\n",
    "    ss = []\n",
    "    for s in ustring:\n",
    "        rstring = \"\"\n",
    "        for uchar in s:\n",
    "            inside_code = ord(uchar)\n",
    "            if inside_code == 12288:  # 全形空格直接轉換\n",
    "                inside_code = 32\n",
    "            elif (inside_code >= 65281 and inside_code <= 65374):  # 全形字元（除空格）根據關係轉化\n",
    "                inside_code -= 65248\n",
    "            rstring += chr(inside_code)\n",
    "        ss.append(rstring)\n",
    "    return ''.join(ss)\n",
    "                \n",
    "def clean_s(s, lang):\n",
    "    if lang == 'en':\n",
    "        s = re.sub(r\"\\([^()]*\\)\", \"\", s) # remove ([text])\n",
    "        s = s.replace('-', '') # remove '-'\n",
    "        s = re.sub('([.,;!?()\\\"])', r' \\1 ', s) # keep punctuation\n",
    "    elif lang == 'zh':\n",
    "        s = strQ2B(s) # Q2B\n",
    "        s = re.sub(r\"\\([^()]*\\)\", \"\", s) # remove ([text])\n",
    "        s = s.replace(' ', '')\n",
    "        s = s.replace('—', '')\n",
    "        s = s.replace('“', '\"')\n",
    "        s = s.replace('”', '\"')\n",
    "        s = s.replace('_', '')\n",
    "        s = re.sub('([。,;!?()\\\"~「」])', r' \\1 ', s) # keep punctuation\n",
    "    s = ' '.join(s.strip().split())\n",
    "    return s\n",
    "\n",
    "def len_s(s, lang):\n",
    "    if lang == 'zh':\n",
    "        return len(s)\n",
    "    return len(s.split())\n",
    "\n",
    "def clean_corpus(prefix, l1, l2, ratio=9, max_len=1000, min_len=1):\n",
    "    if Path(f'{prefix}.clean.{l1}').exists() and Path(f'{prefix}.clean.{l2}').exists():\n",
    "        print(f'{prefix}.clean.{l1} & {l2} exists. skipping clean.')\n",
    "        return\n",
    "    with open(f'{prefix}.{l1}', 'r') as l1_in_f:\n",
    "        with open(f'{prefix}.{l2}', 'r') as l2_in_f:\n",
    "            with open(f'{prefix}.clean.{l1}', 'w') as l1_out_f:\n",
    "                with open(f'{prefix}.clean.{l2}', 'w') as l2_out_f:\n",
    "                    for s1 in l1_in_f:\n",
    "                        s1 = s1.strip()\n",
    "                        s2 = l2_in_f.readline().strip()\n",
    "                        s1 = clean_s(s1, l1)\n",
    "                        s2 = clean_s(s2, l2)\n",
    "                        s1_len = len_s(s1, l1)\n",
    "                        s2_len = len_s(s2, l2)\n",
    "                        if min_len > 0: # remove short sentence\n",
    "                            if s1_len < min_len or s2_len < min_len:\n",
    "                                continue\n",
    "                        if max_len > 0: # remove long sentence\n",
    "                            if s1_len > max_len or s2_len > max_len:\n",
    "                                continue\n",
    "                        if ratio > 0: # remove by ratio of length\n",
    "                            if s1_len/s2_len > ratio or s2_len/s1_len > ratio:\n",
    "                                continue\n",
    "                        print(s1, file=l1_out_f)\n",
    "                        print(s2, file=l2_out_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1i1JadfRsqks"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/yubin/dataset/lhy-homework/ML2021-HW5/rawdata/ted2020/train_dev.raw.clean.en & zh exists. skipping clean.\n",
      "/root/yubin/dataset/lhy-homework/ML2021-HW5/rawdata/ted2020/test.raw.clean.en & zh exists. skipping clean.\n"
     ]
    }
   ],
   "source": [
    "clean_corpus(data_prefix, src_lang, tgt_lang)\n",
    "clean_corpus(test_prefix, src_lang, tgt_lang, ratio=-1, min_len=-1, max_len=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1wrLAc41sqkt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you so much , Chris .\n",
      "And it's truly a great honor to have the opportunity to come to this stage twice ; I'm extremely grateful .\n",
      "I have been blown away by this conference , and I want to thank all of you for the many nice comments about what I had to say the other night .\n",
      "And I say that sincerely , partly because I need that .\n",
      "Put yourselves in my position .\n",
      "非常謝謝你 , 克里斯 。 能有這個機會第二度踏上這個演講台\n",
      "真是一大榮幸 。 我非常感激 。\n",
      "這個研討會給我留下了極為深刻的印象 , 我想感謝大家對我之前演講的好評 。\n",
      "我是由衷的想這麼說 , 有部份原因是因為我真的有需要 !\n",
      "請你們設身處地為我想一想 !\n"
     ]
    }
   ],
   "source": [
    "!head {data_prefix+'.clean.'+src_lang} -n 5\n",
    "!head {data_prefix+'.clean.'+tgt_lang} -n 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "GNXEL5jTsqku"
   },
   "source": [
    "## 切出 train/valid set\n",
    "\n",
    "原来数据集里只有一个文本文件，并没有被划分为 train 和 valid，这里读取 `train_dev.raw.clean.en` 并将其切分为两部分，分别写到 `train.clean.en` 和 `valid.clean.en` 中，中文的 `..zh` 也是这么干。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = Path('/root/yubin/dataset/lhy-homework/ML2021-HW5/rawdata/ted2020')\n",
    "data_dir = '/root/yubin/dataset/lhy-homework/ML2021-HW5/rawdata'\n",
    "dataset_name = 'ted2020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "wi5OiMDDsqkv"
   },
   "outputs": [],
   "source": [
    "valid_ratio = 0.01 # 3000~4000句就夠了\n",
    "train_ratio = 1 - valid_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "bUx2nQsJsqkv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/valid splits exists. skipping split.\n"
     ]
    }
   ],
   "source": [
    "if (prefix/f'train.clean.{src_lang}').exists() \\\n",
    "and (prefix/f'train.clean.{tgt_lang}').exists() \\\n",
    "and (prefix/f'valid.clean.{src_lang}').exists() \\\n",
    "and (prefix/f'valid.clean.{tgt_lang}').exists():\n",
    "    print(f'train/valid splits exists. skipping split.')\n",
    "else:\n",
    "    line_num = sum(1 for line in open(f'{data_prefix}.clean.{src_lang}'))\n",
    "    labels = list(range(line_num))\n",
    "    random.shuffle(labels)\n",
    "    for lang in [src_lang, tgt_lang]:\n",
    "        train_f = open(os.path.join(data_dir, dataset_name, f'train.clean.{lang}'), 'w')\n",
    "        valid_f = open(os.path.join(data_dir, dataset_name, f'valid.clean.{lang}'), 'w')\n",
    "        count = 0\n",
    "        for line in open(f'{data_prefix}.clean.{lang}', 'r'):\n",
    "            if labels[count]/line_num < train_ratio:\n",
    "                train_f.write(line)\n",
    "            else:\n",
    "                valid_f.write(line)\n",
    "            count += 1\n",
    "        train_f.close()\n",
    "        valid_f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Bpkexvlgsqkv"
   },
   "source": [
    "## Subword Units \n",
    "翻譯存在的一大問題是未登錄詞(out of vocabulary)，可以使用 subword units 作為斷詞單位來解決。\n",
    "- 使用 [sentencepiece](#kudo-richardson-2018-sentencepiece) 套件\n",
    "- 用 unigram 或 byte-pair encoding (BPE)\n",
    "\n",
    "这里对英语的划分方法可以参考：[whitespace-is-treated-as-a-basic-symbol](https://github.com/google/sentencepiece#whitespace-is-treated-as-a-basic-symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Fd700Px8sqkw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/yubin/dataset/lhy-homework/ML2021-HW5/rawdata/ted2020/spm8000.model exists. skipping spm_train.\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "vocab_size = 8000\n",
    "if (prefix/f'spm{vocab_size}.model').exists():\n",
    "    print(f'{prefix}/spm{vocab_size}.model exists. skipping spm_train.')\n",
    "else:\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=','.join([f'{prefix}/train.clean.{src_lang}',\n",
    "                        f'{prefix}/valid.clean.{src_lang}',\n",
    "                        f'{prefix}/train.clean.{tgt_lang}',\n",
    "                        f'{prefix}/valid.clean.{tgt_lang}']),\n",
    "        model_prefix=prefix/f'spm{vocab_size}',\n",
    "        vocab_size=vocab_size,\n",
    "        character_coverage=1,\n",
    "        model_type='unigram', # 'bpe' 也可\n",
    "        input_sentence_size=1e6,\n",
    "        shuffle_input_sentence=True,\n",
    "        normalization_rule_name='nmt_nfkc_cf',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "knA04up1sqkx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/yubin/dataset/lhy-homework/ML2021-HW5/rawdata/ted2020/train.en exists. skipping spm_encode.\n",
      "/root/yubin/dataset/lhy-homework/ML2021-HW5/rawdata/ted2020/train.zh exists. skipping spm_encode.\n",
      "/root/yubin/dataset/lhy-homework/ML2021-HW5/rawdata/ted2020/valid.en exists. skipping spm_encode.\n",
      "/root/yubin/dataset/lhy-homework/ML2021-HW5/rawdata/ted2020/valid.zh exists. skipping spm_encode.\n",
      "/root/yubin/dataset/lhy-homework/ML2021-HW5/rawdata/ted2020/test.en exists. skipping spm_encode.\n",
      "/root/yubin/dataset/lhy-homework/ML2021-HW5/rawdata/ted2020/test.zh exists. skipping spm_encode.\n"
     ]
    }
   ],
   "source": [
    "spm_model = spm.SentencePieceProcessor(model_file=str(prefix/f'spm{vocab_size}.model'))\n",
    "in_tag = {\n",
    "    'train': 'train.clean',\n",
    "    'valid': 'valid.clean',\n",
    "    'test': 'test.raw.clean',\n",
    "}\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    for lang in [src_lang, tgt_lang]:\n",
    "        out_path = prefix/f'{split}.{lang}'\n",
    "        if out_path.exists():\n",
    "            print(f\"{out_path} exists. skipping spm_encode.\")\n",
    "        else:\n",
    "            with open(prefix/f'{split}.{lang}', 'w') as out_f:\n",
    "                with open(prefix/f'{in_tag[split]}.{lang}', 'r') as in_f:\n",
    "                    for line in in_f:\n",
    "                        line = line.strip()\n",
    "                        tok = spm_model.encode(line, out_type=str)\n",
    "                        print(' '.join(tok), file=out_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "D9YEuLYZsqkx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁thank ▁you ▁so ▁much ▁, ▁chris ▁.\n",
      "▁and ▁it ' s ▁ t ru ly ▁a ▁great ▁ho n or ▁to ▁have ▁the ▁ op port un ity ▁to ▁come ▁to ▁this ▁st age ▁ t wi ce ▁; ▁i ' m ▁ex t re me ly ▁gr ate ful ▁.\n",
      "▁i ▁have ▁been ▁ bl ow n ▁away ▁by ▁this ▁con fer ence ▁, ▁and ▁i ▁want ▁to ▁thank ▁all ▁of ▁you ▁for ▁the ▁many ▁ ni ce ▁ com ment s ▁about ▁what ▁i ▁had ▁to ▁say ▁the ▁other ▁night ▁.\n",
      "▁and ▁i ▁say ▁that ▁since re ly ▁, ▁part ly ▁because ▁i ▁need ▁that ▁.\n",
      "▁put ▁your s el ve s ▁in ▁my ▁po s ition ▁.\n",
      "▁ 非常 謝 謝 你 ▁, ▁ 克 里 斯 ▁。 ▁ 能 有 這個 機會 第二 度 踏 上 這個 演講 台\n",
      "▁ 真 是 一 大 榮 幸 ▁。 ▁我 非常 感 激 ▁。\n",
      "▁這個 研 討 會 給我 留 下 了 極 為 深 刻 的 印 象 ▁, ▁我想 感 謝 大家 對我 之前 演講 的 好 評 ▁。\n",
      "▁我 是由 衷 的 想 這麼 說 ▁, ▁有 部份 原因 是因為 我 真的 有 需要 ▁!\n",
      "▁ 請 你們 設 身 處 地 為 我想 一 想 ▁!\n"
     ]
    }
   ],
   "source": [
    "!head {data_dir+'/'+dataset_name+'/train.'+src_lang} -n 5\n",
    "!head {data_dir+'/'+dataset_name+'/train.'+tgt_lang} -n 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的数据中，`▁` 部分代表原始句子的一个 space。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cY-7ym0Nsqky"
   },
   "source": [
    "## 用 fairseq 將資料轉為 binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/sharespace/yubin/github/MLtoys/LHY-HW\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "H4uvbYrusqky"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/yubin/dataset/lhy-homework/ML2021-HW5/data-bin/ted2020 exists, will not overwrite!\n"
     ]
    }
   ],
   "source": [
    "binpath = Path('/root/yubin/dataset/lhy-homework/ML2021-HW5/data-bin', dataset_name)\n",
    "if binpath.exists():\n",
    "    print(binpath, \"exists, will not overwrite!\")\n",
    "else:\n",
    "    !/root/miniconda3/envs/LHY/bin/python -m fairseq_cli.preprocess \\\n",
    "        --source-lang {src_lang}\\\n",
    "        --target-lang {tgt_lang}\\\n",
    "        --trainpref {prefix/'train'}\\\n",
    "        --validpref {prefix/'valid'}\\\n",
    "        --testpref {prefix/'test'}\\\n",
    "        --destdir {binpath}\\\n",
    "        --joined-dictionary\\\n",
    "        --workers 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zln6Zku8sqky"
   },
   "source": [
    "# 實驗的參數設定表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "yzEG28ypsqkz"
   },
   "outputs": [],
   "source": [
    "config = Namespace(\n",
    "    datadir = \"/root/yubin/dataset/lhy-homework/ML2021-HW5/data-bin/ted2020\",\n",
    "    savedir = \"/root/yubin/checkpoint/lhy-homework/ML2021-HW5/rnn\",\n",
    "    source_lang = \"en\",\n",
    "    target_lang = \"zh\",\n",
    "    \n",
    "    # cpu threads when fetching & processing data.\n",
    "    num_workers=2,  \n",
    "    # batch size in terms of tokens. gradient accumulation increases the effective batchsize.\n",
    "    max_tokens=8192,\n",
    "    accum_steps=2,\n",
    "    \n",
    "    # the lr s calculated from Noam lr scheduler. you can tune the maximum lr by this factor.\n",
    "    lr_factor=2.,\n",
    "    lr_warmup=4000,\n",
    "    \n",
    "    # clipping gradient norm helps alleviate gradient exploding\n",
    "    clip_norm=1.0,\n",
    "    \n",
    "    # maximum epochs for training\n",
    "    max_epoch=30,\n",
    "    start_epoch=1,\n",
    "    \n",
    "    # beam size for beam search\n",
    "    beam=5, \n",
    "    # generate sequences of maximum length ax + b, where x is the source length\n",
    "    max_len_a=1.2, \n",
    "    max_len_b=10,\n",
    "    # when decoding, post process sentence by removing sentencepiece symbols.\n",
    "    post_process = \"sentencepiece\",\n",
    "    \n",
    "    # checkpoints\n",
    "    keep_last_epochs=5,\n",
    "    resume=None, # if resume from checkpoint name (under config.savedir)\n",
    "    \n",
    "    # logging\n",
    "    use_wandb=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVJ6D_T3sqkz"
   },
   "source": [
    "# Logging\n",
    "- logging 套件紀錄一般訊息\n",
    "- wandb 紀錄續練過程 loss, bleu, model weight 等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "jP-CfPkosqkz",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    level=\"INFO\", # \"DEBUG\" \"WARNING\" \"ERROR\"\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "proj = \"hw5.seq2seq\"\n",
    "logger = logging.getLogger(proj)\n",
    "if config.use_wandb:\n",
    "    import wandb\n",
    "    wandb.init(project=proj, name=Path(config.savedir).stem, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcWmOCMPsqk0"
   },
   "source": [
    "# CUDA環境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "6L2wRcbYsqk0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:05:59 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2023-01-20 23:05:59 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 44.488 GB ; name = Quadro RTX 8000                         \n",
      "2023-01-20 23:05:59 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n"
     ]
    }
   ],
   "source": [
    "cuda_env = utils.CudaEnvironment()\n",
    "utils.CudaEnvironment.pretty_print_cuda_env_list([cuda_env])\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUth0pCWsqk0"
   },
   "source": [
    "# 讀取資料集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQacez3dsqk1"
   },
   "source": [
    "## 借用 fairseq 的 TranslationTask\n",
    "* 用來讀進上面 binarized 的檔案\n",
    "* 有現成的 data iterator (dataloader)\n",
    "* 字典 task.source_dictionary 和 task.target_dictionary 也很好用 \n",
    "* 有實做 beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "A6a3rsGFsqk1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:06:09 | INFO | fairseq.tasks.translation | [en] dictionary: 8000 types\n",
      "2023-01-20 23:06:09 | INFO | fairseq.tasks.translation | [zh] dictionary: 8000 types\n"
     ]
    }
   ],
   "source": [
    "from fairseq.tasks.translation import TranslationConfig, TranslationTask\n",
    "\n",
    "## setup task\n",
    "task_cfg = TranslationConfig(\n",
    "    data=config.datadir,\n",
    "    source_lang=config.source_lang,\n",
    "    target_lang=config.target_lang,\n",
    "    train_subset=\"train\",\n",
    "    required_seq_len_multiple=8,\n",
    "    dataset_impl=\"mmap\",\n",
    "    upsample_primary=1,\n",
    ")\n",
    "task = TranslationTask.setup_task(task_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eos index: 2\n",
      "pad index: 1\n"
     ]
    }
   ],
   "source": [
    "# show the special token\n",
    "print('eos index:', task.source_dictionary.eos())\n",
    "print('pad index:', task.source_dictionary.pad())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "SFxG6JpOsqk1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:06:15 | INFO | hw5.seq2seq | loading data for epoch 1\n",
      "2023-01-20 23:06:15 | INFO | fairseq.data.data_utils | loaded 390,041 examples from: /root/yubin/dataset/lhy-homework/ML2021-HW5/data-bin/ted2020/train.en-zh.en\n",
      "2023-01-20 23:06:15 | INFO | fairseq.data.data_utils | loaded 390,041 examples from: /root/yubin/dataset/lhy-homework/ML2021-HW5/data-bin/ted2020/train.en-zh.zh\n",
      "2023-01-20 23:06:15 | INFO | fairseq.tasks.translation | /root/yubin/dataset/lhy-homework/ML2021-HW5/data-bin/ted2020 train en-zh 390041 examples\n",
      "2023-01-20 23:06:15 | INFO | fairseq.data.data_utils | loaded 3,939 examples from: /root/yubin/dataset/lhy-homework/ML2021-HW5/data-bin/ted2020/valid.en-zh.en\n",
      "2023-01-20 23:06:15 | INFO | fairseq.data.data_utils | loaded 3,939 examples from: /root/yubin/dataset/lhy-homework/ML2021-HW5/data-bin/ted2020/valid.en-zh.zh\n",
      "2023-01-20 23:06:15 | INFO | fairseq.tasks.translation | /root/yubin/dataset/lhy-homework/ML2021-HW5/data-bin/ted2020 valid en-zh 3939 examples\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"loading data for epoch 1\")\n",
    "task.load_dataset(split=\"train\", epoch=1, combine=True) # combine if you have back-translation data.\n",
    "task.load_dataset(split=\"valid\", epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "TyZ4mnSmsqk1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1,\n",
      " 'source': tensor([  11,   60,   14,    6, 2496,   26,   18,   85,   14,    6,   13, 1616,\n",
      "        2406,   15,   19,   79,  685,    5, 1243,   52,   92, 1294,    4,   18,\n",
      "         106,   48,  656,   25,   12, 2453,   98,  142,    4,   11,  880,   98,\n",
      "         142,    4,   11, 2500,   98,  142,    7,   11,   17,   94, 2745,    4,\n",
      "          39,  936,    4,   68,  592,   14,    6,  213,  255,  113,    5, 1676,\n",
      "          94,   15,    8, 1918,  239,    5,  740,    6,   15, 1862, 1833,   11,\n",
      "        1673,  107,  174,   11,  343,  753,   29,  208,    6,   12,   94,   15,\n",
      "           8,  770,  239,    7,   11,   17, 1362,    4,  495,  508,   75,  944,\n",
      "           6,   52, 1732, 1833,  127,  436,   59,    6,    8, 1167,  344,   11,\n",
      "          13,  274,    6, 2407,    6,   17,  365, 1709,    7,    2]),\n",
      " 'target': tensor([   5,  718, 1730, 1323,  329,    9, 2812, 1046,  513,    4,    5,  569,\n",
      "         249,  254,   49,  996, 3341, 1662,   65, 1040,    4, 1035,  254,  248,\n",
      "          49,  419,  352,  996, 2040,   10,    5, 1237,  864,  830,    4,    5,\n",
      "         653, 1879, 1879,  101, 1483,  706,    4,    5,  705, 1735,   51, 4105,\n",
      "        2102,  263, 2790,  905,   65, 3326, 3697,  248, 3054, 3551,  905,    9,\n",
      "         363, 1162, 1131,  437,  205,  189, 1187, 1451,  741, 3333, 2007,   72,\n",
      "        1967,  550, 1451,  741, 1348,   10,  150, 1562,    4,    5,  210,  317,\n",
      "         552,  619,  263, 1574,  845, 1730,  251, 2040,  247,  317,  522,  352,\n",
      "         202,  619,  527, 3196, 2423,  251, 4105, 2102,  527, 3196,  905,    9,\n",
      "        2040,   51,  317,   10,    2])}\n",
      "(\"Source: and what's happening is that there's a globalization of illness \"\n",
      " 'occurring , that people are starting to eat like us , and live like us , and '\n",
      " \"die like us . and in one generation , for example , asia's gone from having \"\n",
      " 'one of the lowest rates of heart disease and obesity and diabetes to one of '\n",
      " 'the highest . and in africa , cardiovascular disease equals the hiv and aids '\n",
      " 'deaths in most countries .')\n",
      "('Target: 一種疾病全球化的現象正在發生 , 人們開始以我們的方式飲食、生活 , 也以與我們相同的方式死亡 。 舉例來說 , 才短短一個世代 , '\n",
      " '亞洲人罹患心臟病、肥胖與糖尿病的機率便已經從最低族群爬升到領先族群之一 。 在非洲 , '\n",
      " '多數國家因為心血管疾病而死亡的人數等同於因為愛滋病毒而罹患愛滋病的死亡人數 。')\n"
     ]
    }
   ],
   "source": [
    "sample = task.dataset(\"valid\")[1]\n",
    "pprint.pprint(sample)\n",
    "pprint.pprint(\n",
    "    \"Source: \" + \\\n",
    "    task.source_dictionary.string(\n",
    "        sample['source'],\n",
    "        config.post_process,\n",
    "    )\n",
    ")\n",
    "pprint.pprint(\n",
    "    \"Target: \" + \\\n",
    "    task.target_dictionary.string(\n",
    "        sample['target'],\n",
    "        config.post_process,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁and ▁what'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 看看做了啥\n",
    "s = torch.tensor([11, 60])\n",
    "task.source_dictionary.string(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDz-RErhsqk2"
   },
   "source": [
    "## Dataset Iterator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "GZt8yQafsqk2"
   },
   "source": [
    "* 將每個 batch 控制在 N 個 token 讓 GPU 記憶體更有效被利用\n",
    "* 讓 training set 每個 epoch 有不同 shuffling\n",
    "* 濾掉長度太長的句子\n",
    "* **將每個 batch 內的句子 pad 成一樣長**，好讓 GPU 平行運算\n",
    "* `prev_output_tokens` 是 `target` 的 right shift 一格处理的结果，开头是一个 eos\n",
    "    - teacher forcing: 為了訓練模型根據prefix生成下個字，decoder的輸入會是輸出目標序列往右shift一格。\n",
    "    - 一般是會在輸入開頭加個bos token (如下圖)\n",
    "    - fairseq 則是直接把 eos 挪到 beginning，訓練起來效果其實差不多。例如: \n",
    "    ```\n",
    "    # 輸出目標 (target) 和 Decoder輸入 (prev_output_tokens): \n",
    "                   eos = 2\n",
    "                target = 419,  711,  238,  888,  792,   60,  968,    8,    2\n",
    "    prev_output_tokens = 2,  419,  711,  238,  888,  792,   60,  968,    8\n",
    "    ```\n",
    "\n",
    "![seq2seq](https://i.imgur.com/0zeDyuI.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Paco1lhXsqk2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:14:52 | WARNING | fairseq.tasks.fairseq_task | 2,548 samples have invalid sizes and will be skipped, max_positions=(20, 20), first few sample ids=[558, 3702, 2689, 1084, 255, 2474, 792, 3772, 1931, 3046]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': tensor([3472]),\n",
       " 'nsentences': 1,\n",
       " 'ntokens': 11,\n",
       " 'net_input': {'src_tokens': tensor([[   1,    1,    1,    1,    1,    1,    1,   24,  296,  338,   12, 1986,\n",
       "            698,   20,    7,    2]]),\n",
       "  'src_lengths': tensor([9]),\n",
       "  'prev_output_tokens': tensor([[   2,  161,  501,  118,  190,  895, 1031,  234, 1244,  960,    4,    1,\n",
       "              1,    1,    1,    1]])},\n",
       " 'target': tensor([[ 161,  501,  118,  190,  895, 1031,  234, 1244,  960,    4,    2,    1,\n",
       "             1,    1,    1,    1]])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data_iterator(task, split, epoch=1, max_tokens=4000, num_workers=1, cached=True):\n",
    "    batch_iterator = task.get_batch_iterator(\n",
    "        dataset=task.dataset(split),\n",
    "        max_tokens=max_tokens,\n",
    "        max_sentences=None,\n",
    "        max_positions=utils.resolve_max_positions(\n",
    "            task.max_positions(),\n",
    "            max_tokens,\n",
    "        ),\n",
    "        ignore_invalid_inputs=True,\n",
    "        seed=seed,\n",
    "        num_workers=num_workers,\n",
    "        epoch=epoch,\n",
    "        disable_iterator_cache=not cached,\n",
    "        # Set this to False to speed up. However, if set to False, changing max_tokens beyond \n",
    "        # first call of this method has no effect. \n",
    "    )\n",
    "    return batch_iterator\n",
    "\n",
    "demo_epoch_obj = load_data_iterator(task, \"valid\", epoch=1, max_tokens=20, num_workers=1, cached=False)\n",
    "demo_iter = demo_epoch_obj.next_epoch_itr(shuffle=True)\n",
    "sample = next(demo_iter)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuG0Youlsqk2"
   },
   "source": [
    "* 每個 batch 是一個字典，key 是字串，value 是 Tensor，內容說明如下\n",
    "```python\n",
    "batch = {\n",
    "    \"id\": id, # 每個 example 的 id\n",
    "    \"nsentences\": len(samples), # batch size 句子數\n",
    "    \"ntokens\": ntokens, # batch size 字數\n",
    "    \"net_input\": {\n",
    "        \"src_tokens\": src_tokens, # 來源語言的序列\n",
    "        \"src_lengths\": src_lengths, # 每句話沒有 pad 過的長度\n",
    "        \"prev_output_tokens\": prev_output_tokens, # 上面提到右 shift 一格後的目標序列\n",
    "    },\n",
    "    \"target\": target, # 目標序列\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzHJ6yIBsqk3"
   },
   "source": [
    "# 定義模型架構\n",
    "* 我們一樣繼承 fairseq 的 encoder, decoder 和 model, 這樣測試階段才能直接用他寫好的 beam search 函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "i4MzO2_csqk3"
   },
   "outputs": [],
   "source": [
    "from fairseq.models import (\n",
    "    FairseqEncoder, \n",
    "    FairseqIncrementalDecoder,\n",
    "    FairseqEncoderDecoderModel\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZb9bQbDsqk4"
   },
   "source": [
    "## Encoder 編碼器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OuABfCXsqk4"
   },
   "source": [
    "- seq2seq 模型的編碼器為 RNN 或 Transformer Encoder，以下說明以 RNN 為例，Transformer 略有不同。對於每個輸入，Encoder 會輸出一個向量和一個隱藏狀態(hidden state)，並將隱藏狀態用於下一個輸入。換句話說，Encoder 會逐步讀取輸入序列，並在每個 timestep 輸出單個向量，以及在最後 timestep 輸出最終隱藏狀態(content vector)\n",
    "- 參數:\n",
    "  - *args*\n",
    "      - encoder_embed_dim 是 embedding 的維度，主要將 one-hot vector 的單詞向量壓縮到指定的維度，主要是為了降維和濃縮資訊的功用\n",
    "      - encoder_ffn_embed_dim 是 RNN 輸出和隱藏狀態的維度(hidden dimension)\n",
    "      - encoder_layers 是 RNN 要疊多少層\n",
    "      - dropout 是決定有多少的機率會將某個節點變為 0，主要是為了防止 overfitting ，一般來說是在訓練時使用，測試時則不使用\n",
    "  - *dictionary*: fairseq 幫我們做好的 dictionary. 在此用來得到 padding index，好用來得到 encoder padding mask. \n",
    "  - *embed_tokens*: 事先做好的詞嵌入 (nn.Embedding)\n",
    "\n",
    "- 輸入: \n",
    "    - *src_tokens*: 英文的整數序列 e.g. 1, 28, 29, 205, 2 \n",
    "- 輸出: \n",
    "    - *outputs*: 最上層 RNN 每個 timestep 的輸出，<u>後續可以用 Attention 再進行處理</u>\n",
    "    - *final_hiddens*: 每層最終 timestep 的隱藏狀態，將傳遞到 Decoder 進行解碼\n",
    "    - *encoder_padding_mask*: 告訴我們哪些是位置的資訊不重要。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "aN5JjOzYsqk5"
   },
   "outputs": [],
   "source": [
    "class RNNEncoder(FairseqEncoder):\n",
    "    def __init__(self, args, dictionary, embed_tokens):\n",
    "        super().__init__(dictionary)\n",
    "        self.embed_tokens = embed_tokens\n",
    "        \n",
    "        self.embed_dim = args.encoder_embed_dim\n",
    "        self.hidden_dim = args.encoder_ffn_embed_dim\n",
    "        self.num_layers = args.encoder_layers\n",
    "        \n",
    "        self.dropout_in_module = nn.Dropout(args.dropout)\n",
    "        self.rnn = nn.GRU(\n",
    "            self.embed_dim, \n",
    "            self.hidden_dim, \n",
    "            self.num_layers, \n",
    "            dropout=args.dropout, \n",
    "            batch_first=False, \n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout_out_module = nn.Dropout(args.dropout)\n",
    "        \n",
    "        self.padding_idx = dictionary.pad()  # `pad()`: get the index of pad symbol (<pad>) in dictionary.\n",
    "        \n",
    "    def combine_bidir(self, outs, bsz: int):\n",
    "        \"\"\"\n",
    "        将 RNN 两个方向得到的 hidden state 拼接到一起\n",
    "        bsz: batch size\n",
    "        \"\"\"\n",
    "        out = outs.view(self.num_layers, 2, bsz, -1).transpose(1, 2).contiguous()  # continuous 是防止接下来再进行 view 操作时出错误，它本身对 tensor 的值没有改变\n",
    "        return out.view(self.num_layers, bsz, -1)\n",
    "\n",
    "    def forward(self, src_tokens, **unused):\n",
    "        \"\"\"\n",
    "        src_tokens: [Batch, Seqlen]\n",
    "        \"\"\"\n",
    "        bsz, seqlen = src_tokens.size()\n",
    "        \n",
    "        # get embeddings\n",
    "        x = self.embed_tokens(src_tokens)  # [B, S, dim]\n",
    "        x = self.dropout_in_module(x)\n",
    "\n",
    "        # B x T x C -> T x B x C\n",
    "        x = x.transpose(0, 1)\n",
    "        \n",
    "        # 過雙向RNN\n",
    "        h0 = x.new_zeros(2 * self.num_layers, bsz, self.hidden_dim)  # 方便 h0 的 dtype 和 device 都与 x 一样\n",
    "        x, final_hiddens = self.rnn(x, h0)\n",
    "        outputs = self.dropout_out_module(x)\n",
    "        # outputs = [sequence len, batch size, hid dim * directions] 是最上層RNN的輸出\n",
    "        # hidden =  [num_layers * directions, batch size  , hid dim]\n",
    "        \n",
    "        # 因為 Encoder 是雙向的RNN，所以需要將同一層兩個方向的 hidden state 接在一起\n",
    "        final_hiddens = self.combine_bidir(final_hiddens, bsz)\n",
    "        # hidden =  [num_layers x batch x num_directions*hidden]\n",
    "        \n",
    "        encoder_padding_mask = src_tokens.eq(self.padding_idx).t()  # seq 中的 <pad> 的部分要被之后的处理给 mask 掉\n",
    "        return tuple(\n",
    "            (\n",
    "                outputs,  # seq_len x batch x hidden，后面交给 Attention\n",
    "                final_hiddens,  # num_layers x batch x num_directions*hidden\n",
    "                encoder_padding_mask,  # seq_len x batch\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def reorder_encoder_out(self, encoder_out, new_order):\n",
    "        # 這個beam search時會用到，意義並不是很重要\n",
    "        return tuple(\n",
    "            (\n",
    "                encoder_out[0].index_select(1, new_order),\n",
    "                encoder_out[1].index_select(1, new_order),\n",
    "                encoder_out[2].index_select(1, new_order),\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XVwBreMsqk5"
   },
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVS1ovJmsqk5"
   },
   "source": [
    "- 當輸入過長，或是單獨靠 “content vector” 無法取得整個輸入的意思時，用 Attention Mechanism 來提供 Decoder 更多的資訊\n",
    "- 根據現在 **Decoder embeddings** ，去計算在 **Encoder outputs** 中，那些與其有較高的關係，根據關係的數值來把 Encoder outputs 平均起來作為 **Decoder** RNN 的輸入 \n",
    "- 常見 Attention 的實作是用 Neural Network / Dot Product 來算 **key** (decoder embeddings) 和 **query** (Encoder outputs) 之間的關係，再對所有算出來的數值做 **softmax** 得到分佈，最後根據這個分佈對 **values** (Encoder outputs) 做 **weight sum**\n",
    "\n",
    "- 參數:\n",
    "  - *input_embed_dim*: key 的維度，應是 decoder 要做 attend 時的向量的維度\n",
    "  - *source_embed_dim*: query 的維度，應是要被 attend 的向量(encoder outputs)的維度\n",
    "  - *output_embed_dim*: value 的維度，應是做完 attention 後，下一層預期的向量維度\n",
    "\n",
    "- 輸入: \n",
    "    - *inputs*: 就是 key，要 attend 別人的向量\n",
    "    - *encoder_outputs*: 是 query/value，被 attend 的向量，它来自于 Encoder 最上层 RNN 的输出，shape：[Seqlen, Batch, Hidden]\n",
    "    - *encoder_padding_mask*: 告訴我們哪些是位置的資訊不重要。\n",
    "- 輸出: \n",
    "    - *output*: 做完 attention 後的 context vector\n",
    "    - *attention score*: attention 的分布\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "R0_f9UjHsqk6"
   },
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_embed_dim, source_embed_dim, output_embed_dim, bias=False):\n",
    "        \"\"\"\n",
    "\n",
    "        :param input_embed_dim: Decoder embedding 的 dim，对应于 query\n",
    "        :param source_embed_dim: Encoder 最上层 RNN 的输出的 hidden dim，它其实是 Encoder self.hidden_dim * 2，因为 encoder 中是双向 RNN。这也是 query 和 value 的 dim\n",
    "        :param output_embed_dim: context vector 的 dim\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_proj = nn.Linear(input_embed_dim, source_embed_dim, bias=bias)\n",
    "        self.output_proj = nn.Linear(\n",
    "            input_embed_dim + source_embed_dim, output_embed_dim, bias=bias\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, encoder_outputs, encoder_padding_mask):\n",
    "        # inputs: T, B, dim   这里 T 是指时间长度，即 Decoder 端 input seq 的长度，在一个 sentence 中\n",
    "        # encoder_outputs: S x B x dim    一条 sentence 的 h_s 就是 S x dim，共 B 个 sentence，这里的 dim 与 inputs 的 dim 不是一个 dim\n",
    "        # padding mask:  S x B    哪个元素为 1，哪个 token 就被 mask 掉\n",
    "        \n",
    "        # convert all to batch first\n",
    "        inputs = inputs.transpose(1,0) # B, T, dim\n",
    "        encoder_outputs = encoder_outputs.transpose(1,0) # B, S, dim\n",
    "        encoder_padding_mask = encoder_padding_mask.transpose(1,0) # B, S\n",
    "        \n",
    "        # 投影到encoder_outputs的維度\n",
    "        x = self.input_proj(inputs)  # Attention 的 query，[B. T, dim]\n",
    "\n",
    "        # 計算attention\n",
    "        # (B, T, dim) x (B, dim, S) = (B, T, S)\n",
    "        attn_scores = torch.bmm(x, encoder_outputs.transpose(1,2))  # 每一个元素 score 代表两个 vector 之间的 attention weight\n",
    "\n",
    "        # 擋住padding位置的attention\n",
    "        if encoder_padding_mask is not None:\n",
    "            # 利用broadcast  B, S -> (B, 1, S)\n",
    "            encoder_padding_mask = encoder_padding_mask.unsqueeze(1)\n",
    "            attn_scores = (\n",
    "                attn_scores.float()\n",
    "                .masked_fill_(encoder_padding_mask, float(\"-inf\"))   # 不需要关系的部分 attn score 设置为无穷小\n",
    "                .type_as(attn_scores)\n",
    "            )  # FP16 support: cast to float and back\n",
    "\n",
    "        # 在source對應維度softmax\n",
    "        attn_scores = F.softmax(attn_scores, dim=-1)  # 其实就是正规化，让 score 的分数为 1\n",
    "\n",
    "        # 形狀 (B, T, S) x (B, S, dim) = (B, T, dim) 加權平均\n",
    "        x = torch.bmm(attn_scores, encoder_outputs)\n",
    "\n",
    "        # (B, T, dim)\n",
    "        x = torch.cat((x, inputs), dim=-1)  # x: [B, T, input_embed_dim + source_embed_dim]\n",
    "        x = torch.tanh(self.output_proj(x)) # concat + linear + tanh\n",
    "        \n",
    "        # x: 回復形狀 (B, T, dim) -> (T, B, dim)\n",
    "        # attn_scores: [B, T, S]\n",
    "        return x.transpose(1,0), attn_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGCeUiNzsqk6"
   },
   "source": [
    "## Decoder 解碼器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCUdUREHsqk6"
   },
   "source": [
    "* 解碼器的 hidden states 會用編碼器最終隱藏狀態來初始化(content vector)\n",
    "* 解碼器同時也根據目前 timestep 的輸入(也就是前幾個 timestep 的 output)，改變 hidden states，並輸出結果 \n",
    "* 如果加入 attention 可以使表現更好\n",
    "* 我們把 seq2seq 步驟寫在解碼器裡，好讓等等 Seq2Seq 這個型別可以通用 RNN 和 Transformer，而不用再改寫\n",
    "- 參數:\n",
    "  - *args*\n",
    "      - decoder_embed_dim 是解碼器 embedding 的維度，類同 encoder_embed_dim，\n",
    "      - decoder_ffn_embed_dim 是解碼器 RNN 的隱藏維度，類同 encoder_ffn_embed_dim\n",
    "      - decoder_layers 解碼器 RNN 的層數\n",
    "      - share_decoder_input_output_embed 通常 decoder 最後輸出的投影矩陣會和輸入 embedding 共用參數\n",
    "  - *dictionary*: fairseq 幫我們做好的 dictionary.\n",
    "  - *embed_tokens*: 事先做好的詞嵌入(nn.Embedding)\n",
    "- 輸入: \n",
    "    - *prev_output_tokens*: 英文的整數序列 e.g. 1, 28, 29, 205, 2 已經 shift 一格的 target\n",
    "    - *encoder_out*: 編碼器的輸出\n",
    "    - *incremental_state*: 這是測試階段為了加速，所以會記錄每個 timestep 的 hidden state 詳見 forward\n",
    "- 輸出: \n",
    "    - *outputs*: decoder 每個 timestep 的 logits，還沒經過 softmax 的分布\n",
    "    - *extra*: 沒用到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "vIG203d9sqk7"
   },
   "outputs": [],
   "source": [
    "class RNNDecoder(FairseqIncrementalDecoder):\n",
    "    def __init__(self, args, dictionary, embed_tokens):\n",
    "        super().__init__(dictionary)\n",
    "        self.embed_tokens = embed_tokens\n",
    "        \n",
    "        assert args.decoder_layers == args.encoder_layers, f\"\"\"seq2seq rnn requires that encoder \n",
    "        and decoder have same layers of rnn. got: {args.encoder_layers, args.decoder_layers}\"\"\"\n",
    "        \n",
    "        # 因为 Encoder 中使用了双向 RNN，因此它的最终输出的 hidden_dim 是 encoder_ffn_embed_dim 的 2 倍\n",
    "        assert args.decoder_ffn_embed_dim == args.encoder_ffn_embed_dim*2, f\"\"\"seq2seq-rnn requires \n",
    "        that decoder hidden to be 2*encoder hidden dim. got: {args.decoder_ffn_embed_dim, args.encoder_ffn_embed_dim*2}\"\"\"\n",
    "        \n",
    "        self.embed_dim = args.decoder_embed_dim\n",
    "        self.hidden_dim = args.decoder_ffn_embed_dim\n",
    "        self.num_layers = args.decoder_layers\n",
    "        \n",
    "        \n",
    "        self.dropout_in_module = nn.Dropout(args.dropout)\n",
    "        self.rnn = nn.GRU(\n",
    "            self.embed_dim, \n",
    "            self.hidden_dim, \n",
    "            self.num_layers, \n",
    "            dropout=args.dropout, \n",
    "            batch_first=False, \n",
    "            bidirectional=False\n",
    "        )\n",
    "        self.attention = AttentionLayer(\n",
    "            self.embed_dim, self.hidden_dim, self.embed_dim, bias=False\n",
    "        ) \n",
    "        # self.attention = None\n",
    "        self.dropout_out_module = nn.Dropout(args.dropout)\n",
    "        \n",
    "        if self.hidden_dim != self.embed_dim:\n",
    "            self.project_out_dim = nn.Linear(self.hidden_dim, self.embed_dim)\n",
    "        else:\n",
    "            self.project_out_dim = None\n",
    "        \n",
    "        # self.output_projection: 投影为 vocab 大小的 vector\n",
    "        if args.share_decoder_input_output_embed:\n",
    "            self.output_projection = nn.Linear(\n",
    "                self.embed_tokens.weight.shape[1],   # embed_dim\n",
    "                self.embed_tokens.weight.shape[0],   # tgt_vocab_size\n",
    "                bias=False,\n",
    "            )\n",
    "            self.output_projection.weight = self.embed_tokens.weight\n",
    "        else:\n",
    "            self.output_projection = nn.Linear(\n",
    "                self.output_embed_dim, len(dictionary), bias=False\n",
    "            )\n",
    "            nn.init.normal_(\n",
    "                self.output_projection.weight, mean=0, std=self.output_embed_dim ** -0.5\n",
    "            )\n",
    "        \n",
    "    def forward(self, prev_output_tokens, encoder_out, incremental_state=None, **unused):\n",
    "        \"\"\"在训练中，Decoder 被期待输入整个右移了一位的 target sequence，输出在 vocab 上的 logits.\n",
    "\n",
    "        :param prev_output_tokens: the entire target sequence(shifted right by one position), begin with the end-of-sentence symbol `dictionary.eos()`, followed by the target sequence\n",
    "        :type prev_output_tokens: [batch, tgt_len], for teacher forcing.\n",
    "        :param encoder_out: used for encode-side attention\n",
    "        :param incremental_state: _description_, defaults to None\n",
    "        :type incremental_state: _type_, optional\n",
    "        :return: _description_\n",
    "        :rtype: _type_\n",
    "        \"\"\"\n",
    "        # 取出encoder的輸出\n",
    "        encoder_outputs, encoder_hiddens, encoder_padding_mask = encoder_out\n",
    "        # outputs:          seq_len x batch x num_directions*hidden\n",
    "        # encoder_hiddens:  num_layers x batch x num_directions*encoder_hidden\n",
    "        # padding_mask:     seq_len x batch\n",
    "        \n",
    "        if incremental_state is not None and len(incremental_state) > 0:\n",
    "            # 有上個timestep留下的資訊，讀進來就可以繼續decode，不用從bos重來\n",
    "            # If the *incremental_state* argument is not ``None`` then we are\n",
    "            # in incremental inference mode. While *prev_output_tokens* will\n",
    "            # still contain the entire decoded prefix, we will only use the\n",
    "            # last step and assume that the rest of the state is cached.\n",
    "            prev_output_tokens = prev_output_tokens[:, -1:]  # 取每一行的最后一个元素，[batch, 1]\n",
    "            cache_state = self.get_incremental_state(incremental_state, \"cached_state\")\n",
    "            prev_hiddens = cache_state[\"prev_hiddens\"]\n",
    "        else:\n",
    "            # 沒有incremental state代表這是training或者是test time時的第一步\n",
    "            # 準備seq2seq: 把encoder_hiddens pass進去decoder的hidden states\n",
    "            prev_hiddens = encoder_hiddens\n",
    "        \n",
    "        bsz, seqlen = prev_output_tokens.size()\n",
    "        \n",
    "        # Embed the target sequence, which has been shifted right by one\n",
    "        # position and now starts with the end-of-sentence symbol `<EOS>`.\n",
    "        x = self.embed_tokens(prev_output_tokens)  # [B, T, dim]\n",
    "        x = self.dropout_in_module(x)\n",
    "\n",
    "        # B x T x C -> T x B x C\n",
    "        x = x.transpose(0, 1)\n",
    "                \n",
    "        # 做decoder-to-encoder attention\n",
    "        if self.attention is not None:\n",
    "            x, attn = self.attention(x, encoder_outputs, encoder_padding_mask)  # attention 输出的 x 是经过计算后的 context vector，也就是 value 做 attention weighted sum 的结果\n",
    "\n",
    "        # 過單向RNN\n",
    "        x, final_hiddens = self.rnn(x, prev_hiddens)\n",
    "        # outputs = [sequence len, batch size, hid dim]\n",
    "        # hidden =  [num_layers * directions, batch size  , hid dim]\n",
    "        x = self.dropout_out_module(x)\n",
    "                \n",
    "        # 投影到embedding size (如果hidden 和embed size不一樣，然後share_embedding又設成True,需要額外project一次)\n",
    "        if self.project_out_dim != None:\n",
    "            x = self.project_out_dim(x)\n",
    "        \n",
    "        # 投影到vocab size 的分佈\n",
    "        x = self.output_projection(x)  # [seqlen, batch, vocab_size]\n",
    "        \n",
    "        # T x B x C -> B x T x C\n",
    "        x = x.transpose(1, 0)\n",
    "        \n",
    "        # 如果是Incremental, 記錄這個timestep的hidden states, 下個timestep讀回來\n",
    "        cache_state = {\n",
    "            \"prev_hiddens\": final_hiddens,\n",
    "        }\n",
    "        self.set_incremental_state(incremental_state, \"cached_state\", cache_state)\n",
    "        \n",
    "        return x, None\n",
    "    \n",
    "    def reorder_incremental_state(\n",
    "        self,\n",
    "        incremental_state,\n",
    "        new_order,\n",
    "    ):\n",
    "        # 這個beam search時會用到，意義並不是很重要\n",
    "        cache_state = self.get_incremental_state(incremental_state, \"cached_state\")\n",
    "        prev_hiddens = cache_state[\"prev_hiddens\"]\n",
    "        prev_hiddens = [p.index_select(0, new_order) for p in prev_hiddens]\n",
    "        cache_state = {\n",
    "            \"prev_hiddens\": torch.stack(prev_hiddens),\n",
    "        }\n",
    "        self.set_incremental_state(incremental_state, \"cached_state\", cache_state)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y55fockAsqk7"
   },
   "source": [
    "## Seq2Seq\n",
    "- 由 **Encoder** 和 **Decoder** 組成\n",
    "- 接收輸入並傳給 **Encoder** \n",
    "- 將 **Encoder** 的輸出傳給 **Decoder**\n",
    "- **Decoder** 根據前幾個 timestep 的輸出和 **Encoder** 輸出進行解碼  \n",
    "- 當解碼完成後，將 **Decoder** 的輸出傳回 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "P07--YsJsqk7"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(FairseqEncoderDecoderModel):\n",
    "    def __init__(self, args, encoder, decoder):\n",
    "        super().__init__(encoder, decoder)\n",
    "        self.args = args\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        src_tokens,\n",
    "        src_lengths,\n",
    "        prev_output_tokens,\n",
    "        return_all_hiddens: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Run the forward pass for an encoder-decoder model.\n",
    "        \"\"\"\n",
    "        encoder_out = self.encoder(\n",
    "            src_tokens, src_lengths=src_lengths, return_all_hiddens=return_all_hiddens\n",
    "        )\n",
    "        logits, extra = self.decoder(\n",
    "            prev_output_tokens,\n",
    "            encoder_out=encoder_out,\n",
    "            src_lengths=src_lengths,\n",
    "            return_all_hiddens=return_all_hiddens,\n",
    "        )\n",
    "        return logits, extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSggF_v8sqk8"
   },
   "source": [
    "# 模型初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "kxI2oy9Fsqk8"
   },
   "outputs": [],
   "source": [
    "# # HINT: transformer 架構\n",
    "# from fairseq.models.transformer import (\n",
    "#     TransformerEncoder, \n",
    "#     TransformerDecoder,\n",
    "# )\n",
    "\n",
    "def build_model(args, task):\n",
    "    \"\"\" 按照參數設定建置模型 \"\"\"\n",
    "    src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n",
    "\n",
    "    # 詞嵌入\n",
    "    # 这里 nn.Embedding 的第三个参数为 padding_idx，假如 seq 为 [3, 4]，padding_idx=0，又规定序列长度统一为 4，那么这个 seq 需要 padding，即 seq = [3, 4, 0, 0]，也就是在原来的 seq 后面补上 0\n",
    "    encoder_embed_tokens = nn.Embedding(len(src_dict), args.encoder_embed_dim, src_dict.pad())\n",
    "    decoder_embed_tokens = nn.Embedding(len(tgt_dict), args.decoder_embed_dim, tgt_dict.pad())\n",
    "    \n",
    "    # 編碼器與解碼器\n",
    "    # TODO: 替換成 TransformerEncoder 和 TransformerDecoder\n",
    "    encoder = RNNEncoder(args, src_dict, encoder_embed_tokens)\n",
    "    decoder = RNNDecoder(args, tgt_dict, decoder_embed_tokens)\n",
    "    \n",
    "    # 序列到序列模型\n",
    "    model = Seq2Seq(args, encoder, decoder)\n",
    "    \n",
    "    # 序列到序列模型的初始化很重要 需要特別處理\n",
    "    def init_params(module):\n",
    "        from fairseq.modules import MultiheadAttention\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        if isinstance(module, MultiheadAttention):\n",
    "            module.q_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            module.k_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            module.v_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if isinstance(module, nn.RNNBase):\n",
    "            for name, param in module.named_parameters():\n",
    "                if \"weight\" in name or \"bias\" in name:\n",
    "                    param.data.uniform_(-0.1, 0.1)\n",
    "            \n",
    "    # 初始化模型\n",
    "    model.apply(init_params)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HZDLRf2sqk_"
   },
   "source": [
    "## 設定模型相關參數\n",
    "參考參數\n",
    "\n",
    "|model|embedding dim|encoder ffn|encoder layers|decoder ffn|decoder layers|\n",
    "|-|-|-|-|-|-|\n",
    "|RNN|256|512|1|1024|1|\n",
    "|Transformer|256|1024|4|1024|4|\n",
    "\n",
    "Strong baseline 用的參數可以參考 [Attention is all you need](#vaswani2017) 的 Table 3 的 transformer-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Nrmks6QcsqlA"
   },
   "outputs": [],
   "source": [
    "arch_args = Namespace(\n",
    "    encoder_embed_dim=256,\n",
    "    encoder_ffn_embed_dim=512,\n",
    "    encoder_layers=1,\n",
    "    decoder_embed_dim=256,\n",
    "    decoder_ffn_embed_dim=1024,\n",
    "    decoder_layers=1,\n",
    "    share_decoder_input_output_embed=True,\n",
    "    dropout=0.3,\n",
    ")\n",
    "\n",
    "# # HINT: 補上Transformer用的參數\n",
    "# def add_transformer_args(args):\n",
    "#     args.encoder_attention_heads=4\n",
    "#     args.encoder_normalize_before=True\n",
    "    \n",
    "#     args.decoder_attention_heads=4\n",
    "#     args.decoder_normalize_before=True\n",
    "    \n",
    "#     args.activation_fn=\"relu\"\n",
    "#     args.max_source_positions=1024\n",
    "#     args.max_target_positions=1024\n",
    "    \n",
    "#     # 補上我們沒有設定的Transformer預設參數\n",
    "#     from fairseq.models.transformer import base_architecture \n",
    "#     base_architecture(arch_args)\n",
    "\n",
    "# add_transformer_args(arch_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "nPtuo_h5sqlA"
   },
   "outputs": [],
   "source": [
    "if config.use_wandb:\n",
    "    wandb.config.update(vars(arch_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "KshsZuGisqlB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/LHY/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:20:59 | INFO | hw5.seq2seq | Seq2Seq(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embed_tokens): Embedding(8000, 256, padding_idx=1)\n",
      "    (dropout_in_module): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): GRU(256, 512, dropout=0.3, bidirectional=True)\n",
      "    (dropout_out_module): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (decoder): RNNDecoder(\n",
      "    (embed_tokens): Embedding(8000, 256, padding_idx=1)\n",
      "    (dropout_in_module): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): GRU(256, 1024, dropout=0.3)\n",
      "    (attention): AttentionLayer(\n",
      "      (input_proj): Linear(in_features=256, out_features=1024, bias=False)\n",
      "      (output_proj): Linear(in_features=1280, out_features=256, bias=False)\n",
      "    )\n",
      "    (dropout_out_module): Dropout(p=0.3, inplace=False)\n",
      "    (project_out_dim): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (output_projection): Linear(in_features=256, out_features=8000, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = build_model(arch_args, task)\n",
    "logger.info(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUmk-a8tsqlB"
   },
   "source": [
    "# Optimization 最佳化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvqT93nNsqlB"
   },
   "source": [
    "## Loss: Label Smoothing Regularization\n",
    "* 讓模型學習輸出較不集中的分佈，防止模型過度自信\n",
    "* 有時候Ground Truth並非唯一答案，所以在算loss時，我們會保留一部份機率給正確答案以外的label\n",
    "* 可以有效防止過度擬合\n",
    "\n",
    "code [source](https://fairseq.readthedocs.io/en/latest/_modules/fairseq/criterions/label_smoothed_cross_entropy.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "TKjsrlJCsqlC"
   },
   "outputs": [],
   "source": [
    "class LabelSmoothedCrossEntropyCriterion(nn.Module):\n",
    "    def __init__(self, smoothing, ignore_index=None, reduce=True):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.ignore_index = ignore_index\n",
    "        self.reduce = reduce\n",
    "    \n",
    "    def forward(self, lprobs, target):\n",
    "        if target.dim() == lprobs.dim() - 1:\n",
    "            target = target.unsqueeze(-1)\n",
    "        # nll: Negative log likelihood，當目標是one-hot時的cross-entropy loss. 以下同 F.nll_loss\n",
    "        nll_loss = -lprobs.gather(dim=-1, index=target)\n",
    "        # 將一部分正確答案的機率分配給其他label 所以當計算cross-entropy時等於把所有label的log prob加起來\n",
    "        smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n",
    "        if self.ignore_index is not None:\n",
    "            pad_mask = target.eq(self.ignore_index)\n",
    "            nll_loss.masked_fill_(pad_mask, 0.0)\n",
    "            smooth_loss.masked_fill_(pad_mask, 0.0)\n",
    "        else:\n",
    "            nll_loss = nll_loss.squeeze(-1)\n",
    "            smooth_loss = smooth_loss.squeeze(-1)\n",
    "        if self.reduce:\n",
    "            nll_loss = nll_loss.sum()\n",
    "            smooth_loss = smooth_loss.sum()\n",
    "        # 計算cross-entropy時 加入分配給其他label的loss\n",
    "        eps_i = self.smoothing / lprobs.size(-1)\n",
    "        loss = (1.0 - self.smoothing) * nll_loss + eps_i * smooth_loss\n",
    "        return loss\n",
    "\n",
    "# 一般都用0.1效果就很好了\n",
    "criterion = LabelSmoothedCrossEntropyCriterion(\n",
    "    smoothing=0.1,\n",
    "    ignore_index=task.target_dictionary.pad(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BsgL03ssqlC"
   },
   "source": [
    "## Optimizer: Adam + lr scheduling\n",
    "Inverse square root 排程對於訓練 Transformer 時的穩定性很重要，後來也用在 RNN 上。\n",
    "根據底下公式來更新 learning rate，前期線性增長，後期根據更新步數方根的倒數來遞減。\n",
    "$$lrate = d_{\\text{model}}^{-0.5}\\cdot\\min({step\\_num}^{-0.5},{step\\_num}\\cdot{warmup\\_steps}^{-1.5})$$\n",
    "code [source](https://nlp.seas.harvard.edu/2018/04/03/attention.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "n6itJcIDsqlC"
   },
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "    \n",
    "    @property\n",
    "    def param_groups(self):\n",
    "        return self.optimizer.param_groups\n",
    "        \n",
    "    def multiply_grads(self, c):\n",
    "        \"\"\"Multiplies grads by a constant *c*.\"\"\"                \n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    p.grad.data.mul_(c)\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return 0 if not step else self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbR_x6mesqlC"
   },
   "source": [
    "## 排程視覺化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "4pH_JrP9sqlD"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = NoamOpt(\n",
    "    model_size=arch_args.encoder_embed_dim, \n",
    "    factor=config.lr_factor, \n",
    "    warmup=config.lr_warmup, \n",
    "    optimizer=torch.optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.0001))\n",
    "plt.plot(np.arange(1, 100000), [optimizer.rate(i) for i in range(1, 100000)])\n",
    "plt.legend([f\"{optimizer.model_size}:{optimizer.warmup}\"])\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVLxbyMjsqlD"
   },
   "source": [
    "# 訓練步驟"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6-c3sxPsqlD"
   },
   "source": [
    "## Training 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "1ELSNqtdsqlD"
   },
   "outputs": [],
   "source": [
    "from fairseq.data import iterators\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "def train_one_epoch(epoch_itr, model, task, criterion, optimizer, accum_steps=1):\n",
    "    itr = epoch_itr.next_epoch_itr(shuffle=True)\n",
    "    itr = iterators.GroupedIterator(itr, accum_steps) # 梯度累積: 每 accum_steps 個 sample 更新一次\n",
    "    \n",
    "    stats = {\"loss\": []}\n",
    "    scaler = GradScaler() # 混和精度訓練 automatic mixed precision (amp) \n",
    "    \n",
    "    model.train()\n",
    "    progress = tqdm.tqdm(itr, desc=f\"train epoch {epoch_itr.epoch}\", leave=False)\n",
    "    for samples in progress:\n",
    "        model.zero_grad()\n",
    "        accum_loss = 0\n",
    "        sample_size = 0\n",
    "        # 梯度累積: 每 accum_steps 個 sample 更新一次\n",
    "        for i, sample in enumerate(samples):\n",
    "            if i == 1:\n",
    "                # emptying the CUDA cache after the first step can reduce the chance of OOM\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            sample = utils.move_to_cuda(sample, device=device)\n",
    "            target = sample[\"target\"]\n",
    "            sample_size_i = sample[\"ntokens\"]\n",
    "            sample_size += sample_size_i\n",
    "            \n",
    "            # 混和精度訓練 \n",
    "            with autocast():\n",
    "                net_output = model.forward(**sample[\"net_input\"])\n",
    "                lprobs = F.log_softmax(net_output[0], -1)            \n",
    "                loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1))\n",
    "                \n",
    "                # logging\n",
    "                accum_loss += loss.item()\n",
    "                # back-prop\n",
    "                scaler.scale(loss).backward()                \n",
    "        \n",
    "        scaler.unscale_(optimizer)\n",
    "        optimizer.multiply_grads(1 / (sample_size or 1.0)) # (sample_size or 1.0) handles the case of a zero gradient\n",
    "        gnorm = nn.utils.clip_grad_norm_(model.parameters(), config.clip_norm) # 梯度裁剪 防止梯度爆炸\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # logging\n",
    "        loss_print = accum_loss/sample_size\n",
    "        stats[\"loss\"].append(loss_print)\n",
    "        progress.set_postfix(loss=loss_print)\n",
    "        if config.use_wandb:\n",
    "            wandb.log({\n",
    "                \"train/loss\": loss_print,\n",
    "                \"train/grad_norm\": gnorm.item(),\n",
    "                \"train/lr\": optimizer.rate(),\n",
    "                \"train/sample_size\": sample_size,\n",
    "            })\n",
    "        \n",
    "    loss_print = np.mean(stats[\"loss\"])\n",
    "    logger.info(f\"training loss: {loss_print:.4f}\")\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0Tc_VhxsqlE"
   },
   "source": [
    "## Validation & Inference 檢驗和推論\n",
    "為防止訓練發生過度擬合，每過一段時間要做一次檢測，計算模型在未看過的資料上的表現。\n",
    "- 過程基本上和training一樣，另外加上 inference\n",
    "- 檢驗完畢可順便儲存模型參數\n",
    "\n",
    "單看 validation loss，我們很難知道模型真實的效能\n",
    "- 直接用當前模型去生成翻譯結果 (hypothesis)，再和正確答案 (reference) 計算 BLEU score\n",
    "- 也可用肉眼看翻譯結果的好壞\n",
    "- 我們用 fairseq 寫好的 sequence generator 來進行 beam search 生成翻譯結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "bvysFsA7sqlE"
   },
   "outputs": [],
   "source": [
    "# fairseq 的 beam search generator\n",
    "# 給定模型和輸入序列，用 beam search 生成翻譯結果\n",
    "sequence_generator = task.build_generator([model], config)\n",
    "\n",
    "def decode(toks, dictionary):\n",
    "    # 從 Tensor 轉成人看得懂的句子\n",
    "    s = dictionary.string(\n",
    "        toks.int().cpu(),\n",
    "        config.post_process,\n",
    "    )\n",
    "    return s if s else \"<unk>\"\n",
    "\n",
    "def inference_step(sample, model):\n",
    "    gen_out = sequence_generator.generate([model], sample)\n",
    "    srcs = []\n",
    "    hyps = []\n",
    "    refs = []\n",
    "    for i in range(len(gen_out)):\n",
    "        # 對於每個 sample, 收集輸入，輸出和參考答案，稍後計算 BLEU\n",
    "        srcs.append(decode(\n",
    "            utils.strip_pad(sample[\"net_input\"][\"src_tokens\"][i], task.source_dictionary.pad()), \n",
    "            task.source_dictionary,\n",
    "        ))\n",
    "        hyps.append(decode(\n",
    "            gen_out[i][0][\"tokens\"], # 0 代表取出 beam 內分數第一的輸出結果\n",
    "            task.target_dictionary,\n",
    "        ))\n",
    "        refs.append(decode(\n",
    "            utils.strip_pad(sample[\"target\"][i], task.target_dictionary.pad()), \n",
    "            task.target_dictionary,\n",
    "        ))\n",
    "    return srcs, hyps, refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "xesFn8vWsqlE"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import sacrebleu\n",
    "\n",
    "def validate(model, task, criterion, log_to_wandb=True):\n",
    "    logger.info('begin validation')\n",
    "    itr = load_data_iterator(task, \"valid\", 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n",
    "    \n",
    "    stats = {\"loss\":[], \"bleu\": 0, \"srcs\":[], \"hyps\":[], \"refs\":[]}\n",
    "    srcs = []\n",
    "    hyps = []\n",
    "    refs = []\n",
    "    \n",
    "    model.eval()\n",
    "    progress = tqdm.tqdm(itr, desc=f\"validation\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(progress):\n",
    "            # validation loss\n",
    "            sample = utils.move_to_cuda(sample, device=device)\n",
    "            net_output = model.forward(**sample[\"net_input\"])\n",
    "\n",
    "            lprobs = F.log_softmax(net_output[0], -1)\n",
    "            target = sample[\"target\"]\n",
    "            sample_size = sample[\"ntokens\"]\n",
    "            loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1)) / sample_size\n",
    "            progress.set_postfix(valid_loss=loss.item())\n",
    "            stats[\"loss\"].append(loss)\n",
    "            \n",
    "            # 進行推論\n",
    "            s, h, r = inference_step(sample, model)\n",
    "            srcs.extend(s)\n",
    "            hyps.extend(h)\n",
    "            refs.extend(r)\n",
    "            \n",
    "    tok = 'zh' if task.cfg.target_lang == 'zh' else '13a'\n",
    "    stats[\"loss\"] = torch.stack(stats[\"loss\"]).mean().item()\n",
    "    stats[\"bleu\"] = sacrebleu.corpus_bleu(hyps, [refs], tokenize=tok) # 計算BLEU score\n",
    "    stats[\"srcs\"] = srcs\n",
    "    stats[\"hyps\"] = hyps\n",
    "    stats[\"refs\"] = refs\n",
    "    \n",
    "    if config.use_wandb and log_to_wandb:\n",
    "        wandb.log({\n",
    "            \"valid/loss\": stats[\"loss\"],\n",
    "            \"valid/bleu\": stats[\"bleu\"].score,\n",
    "        }, commit=False)\n",
    "    \n",
    "    showid = np.random.randint(len(hyps))\n",
    "    logger.info(\"example source: \" + srcs[showid])\n",
    "    logger.info(\"example hypothesis: \" + hyps[showid])\n",
    "    logger.info(\"example reference: \" + refs[showid])\n",
    "    \n",
    "    # show bleu results\n",
    "    logger.info(f\"validation loss:\\t{stats['loss']:.4f}\")\n",
    "    logger.info(stats[\"bleu\"].format())\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-Vql872sqlF"
   },
   "source": [
    "# 儲存及載入模型參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "hoi1rGy8sqlF"
   },
   "outputs": [],
   "source": [
    "def validate_and_save(model, task, criterion, optimizer, epoch, save=True):   \n",
    "    stats = validate(model, task, criterion)\n",
    "    bleu = stats['bleu']\n",
    "    loss = stats['loss']\n",
    "    if save:\n",
    "        # save epoch checkpoints\n",
    "        savedir = Path(config.savedir).absolute()\n",
    "        savedir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        check = {\n",
    "            \"model\": model.state_dict(),\n",
    "            \"stats\": {\"bleu\": bleu.score, \"loss\": loss},\n",
    "            \"optim\": {\"step\": optimizer._step}\n",
    "        }\n",
    "        torch.save(check, savedir/f\"checkpoint{epoch}.pt\")\n",
    "        shutil.copy(savedir/f\"checkpoint{epoch}.pt\", savedir/f\"checkpoint_last.pt\")\n",
    "        logger.info(f\"saved epoch checkpoint: {savedir}/checkpoint{epoch}.pt\")\n",
    "    \n",
    "        # save epoch samples\n",
    "        with open(savedir/f\"samples{epoch}.{config.source_lang}-{config.target_lang}.txt\", \"w\") as f:\n",
    "            for s, h in zip(stats[\"srcs\"], stats[\"hyps\"]):\n",
    "                f.write(f\"{s}\\t{h}\\n\")\n",
    "\n",
    "        # get best valid bleu    \n",
    "        if getattr(validate_and_save, \"best_bleu\", 0) < bleu.score:\n",
    "            validate_and_save.best_bleu = bleu.score\n",
    "            torch.save(check, savedir/f\"checkpoint_best.pt\")\n",
    "            \n",
    "        del_file = savedir / f\"checkpoint{epoch - config.keep_last_epochs}.pt\"\n",
    "        if del_file.exists():\n",
    "            del_file.unlink()\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def try_load_checkpoint(model, optimizer=None, name=None):\n",
    "    name = name if name else \"checkpoint_last.pt\"\n",
    "    checkpath = Path(config.savedir)/name\n",
    "    if checkpath.exists():\n",
    "        check = torch.load(checkpath)\n",
    "        model.load_state_dict(check[\"model\"])\n",
    "        stats = check[\"stats\"]\n",
    "        step = \"unknown\"\n",
    "        if optimizer != None:\n",
    "            optimizer._step = step = check[\"optim\"][\"step\"]\n",
    "        logger.info(f\"loaded checkpoint {checkpath}: step={step} loss={stats['loss']} bleu={stats['bleu']}\")\n",
    "    else:\n",
    "        logger.info(f\"no checkpoints found at {checkpath}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuXYIch4sqlG"
   },
   "source": [
    "# 主程式\n",
    "## 訓練迴圈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "8OAgZXIosqlG"
   },
   "outputs": [],
   "source": [
    "model = model.to(device=device)\n",
    "criterion = criterion.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "a76jGLZdsqlG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan 20 23:37:02 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.57       Driver Version: 515.57       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 8000     Off  | 00000000:5E:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    60W / 250W |    974MiB / 46080MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "GlFkx_FbsqlG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:37:03 | INFO | hw5.seq2seq | task: TranslationTask\n",
      "2023-01-20 23:37:03 | INFO | hw5.seq2seq | encoder: RNNEncoder\n",
      "2023-01-20 23:37:03 | INFO | hw5.seq2seq | decoder: RNNDecoder\n",
      "2023-01-20 23:37:03 | INFO | hw5.seq2seq | criterion: LabelSmoothedCrossEntropyCriterion\n",
      "2023-01-20 23:37:03 | INFO | hw5.seq2seq | optimizer: NoamOpt\n",
      "2023-01-20 23:37:03 | INFO | hw5.seq2seq | num. model params: 11,251,968 (num. trained: 11,251,968)\n",
      "2023-01-20 23:37:03 | INFO | hw5.seq2seq | max tokens per batch = 8192, accumulate steps = 2\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"task: {}\".format(task.__class__.__name__))\n",
    "logger.info(\"encoder: {}\".format(model.encoder.__class__.__name__))\n",
    "logger.info(\"decoder: {}\".format(model.decoder.__class__.__name__))\n",
    "logger.info(\"criterion: {}\".format(criterion.__class__.__name__))\n",
    "logger.info(\"optimizer: {}\".format(optimizer.__class__.__name__))\n",
    "logger.info(\n",
    "    \"num. model params: {:,} (num. trained: {:,})\".format(\n",
    "        sum(p.numel() for p in model.parameters()),\n",
    "        sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "    )\n",
    ")\n",
    "logger.info(f\"max tokens per batch = {config.max_tokens}, accumulate steps = {config.accum_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "PyUw2n3csqlH",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:37:04 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[326732]\n",
      "2023-01-20 23:37:04 | INFO | hw5.seq2seq | no checkpoints found at /root/yubin/checkpoint/lhy-homework/ML2021-HW5/rnn/checkpoint_last.pt!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f75476d52824053b50dd13afdf9a532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 1:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:38:02 | INFO | hw5.seq2seq | training loss: 7.1110\n",
      "2023-01-20 23:38:02 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1746970202a84525b7ce6d9b0035cdfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:38:22 | INFO | hw5.seq2seq | example source: released it on the streets of manhattan .\n",
      "2023-01-20 23:38:22 | INFO | hw5.seq2seq | example hypothesis: , 我們 , 我們 , 我們 ,  ,  ,  ,  ,  , 你 。\n",
      "2023-01-20 23:38:22 | INFO | hw5.seq2seq | example reference: 然後把機器人放在曼哈頓的街上 。\n",
      "2023-01-20 23:38:22 | INFO | hw5.seq2seq | validation loss:\t6.5894\n",
      "2023-01-20 23:38:22 | INFO | hw5.seq2seq | BLEU = 0.11 7.4/0.7/0.1/0.0 (BP = 0.921 ratio = 0.924 hyp_len = 102188 ref_len = 110574)\n",
      "2023-01-20 23:38:22 | INFO | hw5.seq2seq | saved epoch checkpoint: /root/yubin/checkpoint/lhy-homework/ML2021-HW5/rnn/checkpoint1.pt\n",
      "2023-01-20 23:38:22 | INFO | hw5.seq2seq | end of epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3dae39a6eb48c09b1ccb3fa05eb6c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 2:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:39:19 | INFO | hw5.seq2seq | training loss: 6.3825\n",
      "2023-01-20 23:39:19 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a521dcfeda92411999f5b0fb6c58d3d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:39:35 | INFO | hw5.seq2seq | example source: and he's here also today .\n",
      "2023-01-20 23:39:35 | INFO | hw5.seq2seq | example hypothesis: 嗯: 「 嗯 。\n",
      "2023-01-20 23:39:35 | INFO | hw5.seq2seq | example reference: 他本人今天也在場 。\n",
      "2023-01-20 23:39:35 | INFO | hw5.seq2seq | validation loss:\t6.0976\n",
      "2023-01-20 23:39:35 | INFO | hw5.seq2seq | BLEU = 0.80 20.7/2.6/0.5/0.1 (BP = 0.664 ratio = 0.710 hyp_len = 78467 ref_len = 110574)\n",
      "2023-01-20 23:39:35 | INFO | hw5.seq2seq | saved epoch checkpoint: /root/yubin/checkpoint/lhy-homework/ML2021-HW5/rnn/checkpoint2.pt\n",
      "2023-01-20 23:39:35 | INFO | hw5.seq2seq | end of epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f93bf2e0eb946d9a57452c0f3e961fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 3:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:40:32 | INFO | hw5.seq2seq | training loss: 5.8596\n",
      "2023-01-20 23:40:32 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "676ffdb3a18c4ac486cbbe0d62f46491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:40:51 | INFO | hw5.seq2seq | example source: so i stopped hanging out with most of them .\n",
      "2023-01-20 23:40:51 | INFO | hw5.seq2seq | example hypothesis: 所以 , 我把它們放出來 。\n",
      "2023-01-20 23:40:51 | INFO | hw5.seq2seq | example reference: 所以我和大多數好友斷絕了關係 。\n",
      "2023-01-20 23:40:51 | INFO | hw5.seq2seq | validation loss:\t5.4661\n",
      "2023-01-20 23:40:51 | INFO | hw5.seq2seq | BLEU = 1.90 19.9/3.5/0.9/0.3 (BP = 0.953 ratio = 0.954 hyp_len = 105444 ref_len = 110574)\n",
      "2023-01-20 23:40:51 | INFO | hw5.seq2seq | saved epoch checkpoint: /root/yubin/checkpoint/lhy-homework/ML2021-HW5/rnn/checkpoint3.pt\n",
      "2023-01-20 23:40:51 | INFO | hw5.seq2seq | end of epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f6a3f1427c54c08bf7a8390b83e9fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 4:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:41:49 | INFO | hw5.seq2seq | training loss: 5.4142\n",
      "2023-01-20 23:41:49 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ef6f4d52314f86beeccc64e3fe2b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:42:09 | INFO | hw5.seq2seq | example source: for example , women have more sexual fantasies about men other than their longterm partner , during the fertile part of the cycle , and that's particularly true if the longterm partner has physical signs of being lower in genetic quality , like he's less physically attractive .\n",
      "2023-01-20 23:42:09 | INFO | hw5.seq2seq | example hypothesis: 舉例來說 , 大多數人士 , 特別是 , 特別是 , 特別是 , 特別是 , 最重要的是 , 特別是 , 最重要的是 , 最重要的是 , 最重要的是 , 最重要的是 , 最重要的是 , 最重要的是 , 最重要的是 , 最重要的是 , 特別是 , 特別是 , 特別是 , 特別是 , 特別是 , 特別是 , 特別是 , 特別是 , 特別是 , 特別是 , 特別是 , 特別是 , 更糟糕 , 更糟糕 。\n",
      "2023-01-20 23:42:09 | INFO | hw5.seq2seq | example reference: 比如說 , 女性在排卵期時較常對其他男性有性幻想而較少幻想自己的長期伴侶尤其是當長期伴侶的生理表徵顯示出其基因較不優秀例如外表體態不具吸引力等\n",
      "2023-01-20 23:42:09 | INFO | hw5.seq2seq | validation loss:\t5.0999\n",
      "2023-01-20 23:42:09 | INFO | hw5.seq2seq | BLEU = 2.66 19.1/4.6/1.4/0.4 (BP = 1.000 ratio = 1.234 hyp_len = 136439 ref_len = 110574)\n",
      "2023-01-20 23:42:09 | INFO | hw5.seq2seq | saved epoch checkpoint: /root/yubin/checkpoint/lhy-homework/ML2021-HW5/rnn/checkpoint4.pt\n",
      "2023-01-20 23:42:09 | INFO | hw5.seq2seq | end of epoch 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "febac56695fe43ab9264278960229829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 5:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:43:06 | INFO | hw5.seq2seq | training loss: 5.0891\n",
      "2023-01-20 23:43:06 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee408866c6d24a258246b97e569dcc09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:43:25 | INFO | hw5.seq2seq | example source: i'm often asked if i plan to \" go back \" to ghana .\n",
      "2023-01-20 23:43:25 | INFO | hw5.seq2seq | example hypothesis: 我常常問我: 「 回到巴西亞 。\n",
      "2023-01-20 23:43:25 | INFO | hw5.seq2seq | example reference: 我常被問到有沒有 「 回去 」 迦納的打算 。\n",
      "2023-01-20 23:43:25 | INFO | hw5.seq2seq | validation loss:\t4.7072\n",
      "2023-01-20 23:43:25 | INFO | hw5.seq2seq | BLEU = 8.31 35.8/13.9/6.4/3.1 (BP = 0.836 ratio = 0.848 hyp_len = 93798 ref_len = 110574)\n",
      "2023-01-20 23:43:25 | INFO | hw5.seq2seq | saved epoch checkpoint: /root/yubin/checkpoint/lhy-homework/ML2021-HW5/rnn/checkpoint5.pt\n",
      "2023-01-20 23:43:26 | INFO | hw5.seq2seq | end of epoch 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4eed97bd163486d8fd8506e9406ca7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 6:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:44:22 | INFO | hw5.seq2seq | training loss: 4.7928\n",
      "2023-01-20 23:44:22 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c9011151c01438dadbe6e67d74673a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:44:42 | INFO | hw5.seq2seq | example source: they saw everything .\n",
      "2023-01-20 23:44:42 | INFO | hw5.seq2seq | example hypothesis: 他們看到所有東西 。\n",
      "2023-01-20 23:44:42 | INFO | hw5.seq2seq | example reference: 他們看到了一切 。\n",
      "2023-01-20 23:44:42 | INFO | hw5.seq2seq | validation loss:\t4.4475\n",
      "2023-01-20 23:44:42 | INFO | hw5.seq2seq | BLEU = 10.00 32.1/13.7/6.7/3.4 (BP = 1.000 ratio = 1.112 hyp_len = 122987 ref_len = 110574)\n",
      "2023-01-20 23:44:42 | INFO | hw5.seq2seq | saved epoch checkpoint: /root/yubin/checkpoint/lhy-homework/ML2021-HW5/rnn/checkpoint6.pt\n",
      "2023-01-20 23:44:42 | INFO | hw5.seq2seq | end of epoch 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c969cdc8a38446d8fd3dbd49c6d6de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 7:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:45:40 | INFO | hw5.seq2seq | training loss: 4.6055\n",
      "2023-01-20 23:45:40 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d647dd7aa5244cea9b25e92b8547d550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:46:00 | INFO | hw5.seq2seq | example source: i was wrong .\n",
      "2023-01-20 23:46:00 | INFO | hw5.seq2seq | example hypothesis: 我錯了 。\n",
      "2023-01-20 23:46:00 | INFO | hw5.seq2seq | example reference: 但我錯了 。\n",
      "2023-01-20 23:46:00 | INFO | hw5.seq2seq | validation loss:\t4.3251\n",
      "2023-01-20 23:46:00 | INFO | hw5.seq2seq | BLEU = 11.47 34.4/15.4/7.8/4.2 (BP = 1.000 ratio = 1.103 hyp_len = 122001 ref_len = 110574)\n",
      "2023-01-20 23:46:00 | INFO | hw5.seq2seq | saved epoch checkpoint: /root/yubin/checkpoint/lhy-homework/ML2021-HW5/rnn/checkpoint7.pt\n",
      "2023-01-20 23:46:00 | INFO | hw5.seq2seq | end of epoch 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ea0f539c4f43bda8eba421d57153ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 8:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:46:58 | INFO | hw5.seq2seq | training loss: 4.4881\n",
      "2023-01-20 23:46:58 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4faca965bf0469aabf422163abfd053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:47:16 | INFO | hw5.seq2seq | example source: gravity is one thing that we can't see and which we don't understand .\n",
      "2023-01-20 23:47:16 | INFO | hw5.seq2seq | example hypothesis: 重力是我們無法理解的 。\n",
      "2023-01-20 23:47:16 | INFO | hw5.seq2seq | example reference: 重力就是一個我們看不見的東西我們也無法理解它\n",
      "2023-01-20 23:47:16 | INFO | hw5.seq2seq | validation loss:\t4.2226\n",
      "2023-01-20 23:47:16 | INFO | hw5.seq2seq | BLEU = 13.16 39.8/18.7/9.7/5.2 (BP = 0.947 ratio = 0.948 hyp_len = 104832 ref_len = 110574)\n",
      "2023-01-20 23:47:17 | INFO | hw5.seq2seq | saved epoch checkpoint: /root/yubin/checkpoint/lhy-homework/ML2021-HW5/rnn/checkpoint8.pt\n",
      "2023-01-20 23:47:17 | INFO | hw5.seq2seq | end of epoch 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadbd10838ec45f6aaaf6404cbdfa8c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 9:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:48:14 | INFO | hw5.seq2seq | training loss: 4.4023\n",
      "2023-01-20 23:48:14 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6deba8ca2fda40ab84b78672faa8f784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:48:32 | INFO | hw5.seq2seq | example source: that's the second largest population in the world .\n",
      "2023-01-20 23:48:32 | INFO | hw5.seq2seq | example hypothesis: 這是世界上第二個大型人口 。\n",
      "2023-01-20 23:48:32 | INFO | hw5.seq2seq | example reference: 這是世界人第二人口大國的情況\n",
      "2023-01-20 23:48:32 | INFO | hw5.seq2seq | validation loss:\t4.1503\n",
      "2023-01-20 23:48:32 | INFO | hw5.seq2seq | BLEU = 14.56 41.6/19.9/10.5/5.7 (BP = 0.976 ratio = 0.976 hyp_len = 107949 ref_len = 110574)\n",
      "2023-01-20 23:48:32 | INFO | hw5.seq2seq | saved epoch checkpoint: /root/yubin/checkpoint/lhy-homework/ML2021-HW5/rnn/checkpoint9.pt\n",
      "2023-01-20 23:48:33 | INFO | hw5.seq2seq | end of epoch 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b125db926a8342fbbcfa702edb8c5d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 10:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:49:31 | INFO | hw5.seq2seq | training loss: 4.3385\n",
      "2023-01-20 23:49:31 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb132189929946509d66e77f5d16e2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:49:50 | INFO | hw5.seq2seq | example source: so who are we ? brothers of masculine chimps , sisters of feminine bonobos ? we are all of them , and more .\n",
      "2023-01-20 23:49:50 | INFO | hw5.seq2seq | example hypothesis: 所以 , 誰是誰 ? 我們都是誰 ? 我們是誰 ? 我們是誰 ? 我們是誰 ? 我們是誰 ? 我們是誰 ? 我們是誰 ? 我們是誰 ? 我們是誰 ?\n",
      "2023-01-20 23:49:50 | INFO | hw5.seq2seq | example reference: 那我們又是誰 ? 雄黑猩猩的兄弟 。 雌倭黑猩猩的姐妹 。 我們所有的人 , 還有更多 。\n",
      "2023-01-20 23:49:50 | INFO | hw5.seq2seq | validation loss:\t4.1000\n",
      "2023-01-20 23:49:50 | INFO | hw5.seq2seq | BLEU = 14.87 42.0/20.3/10.8/6.0 (BP = 0.973 ratio = 0.973 hyp_len = 107599 ref_len = 110574)\n",
      "2023-01-20 23:49:50 | INFO | hw5.seq2seq | saved epoch checkpoint: /root/yubin/checkpoint/lhy-homework/ML2021-HW5/rnn/checkpoint10.pt\n",
      "2023-01-20 23:49:50 | INFO | hw5.seq2seq | end of epoch 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa43b8063af4430a949af8d593ece4a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 11:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:50:46 | INFO | hw5.seq2seq | training loss: 4.2881\n",
      "2023-01-20 23:50:46 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9155ab70940641ed9ff55662eb5c510f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:51:05 | INFO | hw5.seq2seq | example source: the same , when diagnosed at stage four , which is late , is below one percent .\n",
      "2023-01-20 23:51:05 | INFO | hw5.seq2seq | example hypothesis: 相同的 , 當診斷在階段四歲時 , 只有百分之十四歲 。\n",
      "2023-01-20 23:51:05 | INFO | hw5.seq2seq | example reference: 如果在第四期 , 也就是很晚期才診斷出來 , 五年存活率是就不到1% 。\n",
      "2023-01-20 23:51:05 | INFO | hw5.seq2seq | validation loss:\t4.0607\n",
      "2023-01-20 23:51:05 | INFO | hw5.seq2seq | BLEU = 15.57 42.3/20.6/11.0/6.1 (BP = 0.999 ratio = 0.999 hyp_len = 110434 ref_len = 110574)\n",
      "2023-01-20 23:51:05 | INFO | hw5.seq2seq | saved epoch checkpoint: /root/yubin/checkpoint/lhy-homework/ML2021-HW5/rnn/checkpoint11.pt\n",
      "2023-01-20 23:51:06 | INFO | hw5.seq2seq | end of epoch 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2844736d36514a3ab0c9b2bbefc18b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 12:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:52:02 | INFO | hw5.seq2seq | training loss: 4.2468\n",
      "2023-01-20 23:52:02 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68fb8696d0649ca8e8ffcad3d4ce473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:52:21 | INFO | hw5.seq2seq | example source: and then i traveled with the team , with the world bank team , as more like someone to help out with organizational matters .\n",
      "2023-01-20 23:52:21 | INFO | hw5.seq2seq | example hypothesis: 接著 , 我和世界銀行團隊合作 , 像是有人幫忙組織物質 。\n",
      "2023-01-20 23:52:21 | INFO | hw5.seq2seq | example reference: 接著我和團隊一起旅行 , 和世界銀行團隊一起旅行 , 我比較像是協助處理組織事務的人 。\n",
      "2023-01-20 23:52:21 | INFO | hw5.seq2seq | validation loss:\t4.0348\n",
      "2023-01-20 23:52:21 | INFO | hw5.seq2seq | BLEU = 15.52 45.9/22.6/12.1/6.9 (BP = 0.905 ratio = 0.909 hyp_len = 100495 ref_len = 110574)\n",
      "2023-01-20 23:52:21 | INFO | hw5.seq2seq | saved epoch checkpoint: /root/yubin/checkpoint/lhy-homework/ML2021-HW5/rnn/checkpoint12.pt\n",
      "2023-01-20 23:52:21 | INFO | hw5.seq2seq | end of epoch 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e76c28d88394aa083326a25745eb6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 13:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:53:18 | INFO | hw5.seq2seq | training loss: 4.2119\n",
      "2023-01-20 23:53:18 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e8a426d43ae4a85ab4f233fc246ffce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:53:37 | INFO | hw5.seq2seq | example source: when someone who is physically dependent on opioids stops taking them abruptly , that balance is disrupted .\n",
      "2023-01-20 23:53:37 | INFO | hw5.seq2seq | example hypothesis: 當某人對鴉片類鴉片類鴉片類藥物停止時 , 平衡就被破壞了 。\n",
      "2023-01-20 23:53:37 | INFO | hw5.seq2seq | example reference: 如果一個人的身體已經在依賴鴉片類藥物 , 卻突然停止這類藥物 , 這種平衡就會中斷 。\n",
      "2023-01-20 23:53:37 | INFO | hw5.seq2seq | validation loss:\t4.0094\n",
      "2023-01-20 23:53:37 | INFO | hw5.seq2seq | BLEU = 16.03 42.6/21.1/11.4/6.4 (BP = 1.000 ratio = 1.002 hyp_len = 110786 ref_len = 110574)\n",
      "2023-01-20 23:53:37 | INFO | hw5.seq2seq | saved epoch checkpoint: /root/yubin/checkpoint/lhy-homework/ML2021-HW5/rnn/checkpoint13.pt\n",
      "2023-01-20 23:53:37 | INFO | hw5.seq2seq | end of epoch 13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7476562c50e4c22bb1d31b208358962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 14:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:54:33 | INFO | hw5.seq2seq | training loss: 4.1831\n",
      "2023-01-20 23:54:33 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc968576ba24d0087ba3437bd5b16a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:54:52 | INFO | hw5.seq2seq | example source: it presents , not just the complexities found in a procession of different human beings , but the complexities found within each individual person .\n",
      "2023-01-20 23:54:52 | INFO | hw5.seq2seq | example hypothesis: 它不僅僅是複雜的複雜性 , 而是在每個人身上的複雜性 。\n",
      "2023-01-20 23:54:52 | INFO | hw5.seq2seq | example reference: 這體現出這件事情的複雜性 , 不僅體現在不同的人群之間 , 也體現在每個獨立的個人之間 。\n",
      "2023-01-20 23:54:52 | INFO | hw5.seq2seq | validation loss:\t3.9968\n",
      "2023-01-20 23:54:52 | INFO | hw5.seq2seq | BLEU = 15.45 50.2/25.4/13.8/7.9 (BP = 0.799 ratio = 0.817 hyp_len = 90337 ref_len = 110574)\n",
      "2023-01-20 23:54:52 | INFO | hw5.seq2seq | saved epoch checkpoint: /root/yubin/checkpoint/lhy-homework/ML2021-HW5/rnn/checkpoint14.pt\n",
      "2023-01-20 23:54:52 | INFO | hw5.seq2seq | end of epoch 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72faa57b86b6456a93c9a4e021372495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 15:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:55:47 | INFO | hw5.seq2seq | training loss: 4.1612\n",
      "2023-01-20 23:55:47 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e026bd8c944b42b1575f6131b56e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:56:09 | INFO | hw5.seq2seq | example source: shortly after this incident , harriet decided to tell me , her chemistry teacher , all about it .\n",
      "2023-01-20 23:56:09 | INFO | hw5.seq2seq | example hypothesis: 很久之後 , 哈莉特決定要告訴我 , 她的化學老師 , 所有關於它 。\n",
      "2023-01-20 23:56:10 | INFO | hw5.seq2seq | example reference: 事發不久後哈莉特決定把經過通通告訴她的化學老師也就是我\n",
      "2023-01-20 23:56:10 | INFO | hw5.seq2seq | validation loss:\t3.9612\n",
      "2023-01-20 23:56:10 | INFO | hw5.seq2seq | BLEU = 16.26 44.8/22.2/12.0/6.8 (BP = 0.964 ratio = 0.965 hyp_len = 106673 ref_len = 110574)\n",
      "2023-01-20 23:56:10 | INFO | hw5.seq2seq | saved epoch checkpoint: /root/yubin/checkpoint/lhy-homework/ML2021-HW5/rnn/checkpoint15.pt\n",
      "2023-01-20 23:56:10 | INFO | hw5.seq2seq | end of epoch 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f554b2bfd84a05b37895e2ac66606d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 16:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:57:07 | INFO | hw5.seq2seq | training loss: 4.1335\n",
      "2023-01-20 23:57:07 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20af413158f04f47b231ae245c31df0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:57:26 | INFO | hw5.seq2seq | example source: one is: if 80 to 90 percent of what we're finding in the ocean of the marine debris that we're finding in the ocean is plastic , then why don't we call it what it is .\n",
      "2023-01-20 23:57:26 | INFO | hw5.seq2seq | example hypothesis: 第一:如果80%到90%的海洋中發現的海洋中 , 我們發現海洋的海洋中發現的海洋是塑膠 , 那為什麼我們不打電話給它 。\n",
      "2023-01-20 23:57:26 | INFO | hw5.seq2seq | example reference: 其中之一為:如果我們在海裡找到的垃圾裏百分之80到90都是塑膠那麽乾脆就稱之爲\n",
      "2023-01-20 23:57:26 | INFO | hw5.seq2seq | validation loss:\t3.9461\n",
      "2023-01-20 23:57:26 | INFO | hw5.seq2seq | BLEU = 16.23 43.1/21.4/11.5/6.5 (BP = 1.000 ratio = 1.017 hyp_len = 112439 ref_len = 110574)\n",
      "2023-01-20 23:57:26 | INFO | hw5.seq2seq | saved epoch checkpoint: /root/yubin/checkpoint/lhy-homework/ML2021-HW5/rnn/checkpoint16.pt\n",
      "2023-01-20 23:57:26 | INFO | hw5.seq2seq | end of epoch 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b961d02cc8f4f4ca6597a90dc168c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 17:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:58:25 | INFO | hw5.seq2seq | training loss: 4.1154\n",
      "2023-01-20 23:58:25 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6738d30dd6cb4c778c65abbef5a41254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:58:44 | INFO | hw5.seq2seq | example source: his name is tom sietas .\n",
      "2023-01-20 23:58:44 | INFO | hw5.seq2seq | example hypothesis: 他的名字是湯姆.\n",
      "2023-01-20 23:58:44 | INFO | hw5.seq2seq | example reference: 他叫湯姆.席耶塔斯 ,\n",
      "2023-01-20 23:58:44 | INFO | hw5.seq2seq | validation loss:\t3.9312\n",
      "2023-01-20 23:58:44 | INFO | hw5.seq2seq | BLEU = 16.66 44.1/21.9/11.8/6.7 (BP = 1.000 ratio = 1.008 hyp_len = 111478 ref_len = 110574)\n",
      "2023-01-20 23:58:44 | INFO | hw5.seq2seq | saved epoch checkpoint: /root/yubin/checkpoint/lhy-homework/ML2021-HW5/rnn/checkpoint17.pt\n",
      "2023-01-20 23:58:44 | INFO | hw5.seq2seq | end of epoch 17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438cfc857a424677ab88c1186ebc41c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 18:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 23:59:41 | INFO | hw5.seq2seq | training loss: 4.0958\n",
      "2023-01-20 23:59:41 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03149d2144af4205b802aceb93a15fb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-21 00:00:00 | INFO | hw5.seq2seq | example source: so people know that it's there .\n",
      "2023-01-21 00:00:00 | INFO | hw5.seq2seq | example hypothesis: 人們知道它是在那裡的 。\n",
      "2023-01-21 00:00:00 | INFO | hw5.seq2seq | example reference: 大家要先瞭解玩耍的定義 ,\n",
      "2023-01-21 00:00:00 | INFO | hw5.seq2seq | validation loss:\t3.9221\n",
      "2023-01-21 00:00:00 | INFO | hw5.seq2seq | BLEU = 16.94 47.3/23.7/13.0/7.5 (BP = 0.930 ratio = 0.932 hyp_len = 103104 ref_len = 110574)\n",
      "2023-01-21 00:00:00 | INFO | hw5.seq2seq | saved epoch checkpoint: /root/yubin/checkpoint/lhy-homework/ML2021-HW5/rnn/checkpoint18.pt\n",
      "2023-01-21 00:00:00 | INFO | hw5.seq2seq | end of epoch 18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06cbde5d531e4ce080333574e46e8667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 19:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-21 00:00:57 | INFO | hw5.seq2seq | training loss: 4.0793\n",
      "2023-01-21 00:00:57 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "064fc7e29b3d4de58ec5e236b17defde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-21 00:01:17 | INFO | hw5.seq2seq | example source: it allows you to do it anonymously , it allows you to do it for free , and it's completely evidencebased .\n",
      "2023-01-21 00:01:17 | INFO | hw5.seq2seq | example hypothesis: 它允許你去做它 , 它允許你去做它 , 它允許你能做到它 , 完全證據 。\n",
      "2023-01-21 00:01:17 | INFO | hw5.seq2seq | example reference: 你可以暱名使用它 , 使用它是免費的 , 且它完全是以證據為根據 。\n",
      "2023-01-21 00:01:17 | INFO | hw5.seq2seq | validation loss:\t3.9157\n",
      "2023-01-21 00:01:17 | INFO | hw5.seq2seq | BLEU = 17.20 46.3/23.5/12.9/7.4 (BP = 0.957 ratio = 0.958 hyp_len = 105912 ref_len = 110574)\n",
      "2023-01-21 00:01:17 | INFO | hw5.seq2seq | saved epoch checkpoint: /root/yubin/checkpoint/lhy-homework/ML2021-HW5/rnn/checkpoint19.pt\n",
      "2023-01-21 00:01:17 | INFO | hw5.seq2seq | end of epoch 19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4508af9f3fb47699dcb2c89c430a1de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 20:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-21 00:02:17 | INFO | hw5.seq2seq | training loss: 4.0636\n",
      "2023-01-21 00:02:17 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ed8ec7844c43789d43c8cacc712b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-21 00:02:40 | INFO | hw5.seq2seq | example source: so , i was in the cape one time , and i typed the word \" simplicity , \" and i discovered , in this weird , m . night shyamalan way , that i discovered [the] letters , m , i , t . you know the word ?\n",
      "2023-01-21 00:02:40 | INFO | hw5.seq2seq | example hypothesis: 那時我在《簡易》中找到了 「 簡單 」 , 我發現了 「 簡直 」 , 我發現了 「 簡單 」 , 我發現了 「 簡直 」 , 我發現了 「 簡直 」 , 我發現了 「 簡直 」 , 我發現了 「 簡直 」 , 我發現了 「 簡直 」 , 我發現了 「 簡直 」 , 我發現了 「 簡直 」 , 我發現了 「 簡直 」 ,\n",
      "2023-01-21 00:02:40 | INFO | hw5.seq2seq | example reference: 有次我在鱈魚角 , 輸入 \" 簡單 \" 這個字我發現 , 用奇怪的奈特沙馬蘭方式 , 我發現字母 「 m-i-t 」 , 你知道這個字吧 ?\n",
      "2023-01-21 00:02:40 | INFO | hw5.seq2seq | validation loss:\t3.8939\n",
      "2023-01-21 00:02:40 | INFO | hw5.seq2seq | BLEU = 17.33 45.7/23.0/12.5/7.1 (BP = 0.991 ratio = 0.991 hyp_len = 109592 ref_len = 110574)\n",
      "2023-01-21 00:02:40 | INFO | hw5.seq2seq | saved epoch checkpoint: /root/yubin/checkpoint/lhy-homework/ML2021-HW5/rnn/checkpoint20.pt\n",
      "2023-01-21 00:02:40 | INFO | hw5.seq2seq | end of epoch 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a476b16e734b3e83a40272a44e671c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 21:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-21 00:03:39 | INFO | hw5.seq2seq | training loss: 4.0532\n",
      "2023-01-21 00:03:39 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a755c14a67c4c5eb89e61a370e209b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-21 00:04:01 | INFO | hw5.seq2seq | example source: they go to church , by which i mean , they do lots of things for free for each other .\n",
      "2023-01-21 00:04:01 | INFO | hw5.seq2seq | example hypothesis: 他們會去教堂 , 我說的 , 他們有很多東西可以自由彼此 。\n",
      "2023-01-21 00:04:01 | INFO | hw5.seq2seq | example reference: 我說到他們會上教堂的意思是 , 他們免費的為彼此做很多事 。\n",
      "2023-01-21 00:04:01 | INFO | hw5.seq2seq | validation loss:\t3.8907\n",
      "2023-01-21 00:04:01 | INFO | hw5.seq2seq | BLEU = 17.17 47.2/23.8/13.0/7.5 (BP = 0.943 ratio = 0.944 hyp_len = 104418 ref_len = 110574)\n",
      "2023-01-21 00:04:01 | INFO | hw5.seq2seq | saved epoch checkpoint: /root/yubin/checkpoint/lhy-homework/ML2021-HW5/rnn/checkpoint21.pt\n",
      "2023-01-21 00:04:01 | INFO | hw5.seq2seq | end of epoch 21\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae736b0350a4906af97b3e2e7c6868a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 22:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-21 00:04:59 | INFO | hw5.seq2seq | training loss: 4.0350\n",
      "2023-01-21 00:04:59 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5efe3347f1a0400ea8fa2fc5e117f588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_itr = load_data_iterator(task, \"train\", config.start_epoch, config.max_tokens, config.num_workers)\n",
    "try_load_checkpoint(model, optimizer, name=config.resume)\n",
    "while epoch_itr.next_epoch_idx <= config.max_epoch:\n",
    "    # train for one epoch\n",
    "    train_one_epoch(epoch_itr, model, task, criterion, optimizer, config.accum_steps)\n",
    "    stats = validate_and_save(model, task, criterion, optimizer, epoch=epoch_itr.epoch)\n",
    "    logger.info(\"end of epoch {}\".format(epoch_itr.epoch))    \n",
    "    epoch_itr = load_data_iterator(task, \"train\", epoch_itr.next_epoch_idx, config.max_tokens, config.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdpMZTVFsqlH"
   },
   "source": [
    "# Submission 繳交檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4w89HjHksqlH"
   },
   "outputs": [],
   "source": [
    "# 把幾個 checkpoint 平均起來可以達到 ensemble 的效果\n",
    "checkdir=config.savedir\n",
    "!python /root/sharespace/yubin/cloned/fairseq/scripts/average_checkpoints.py \\\n",
    "--inputs {checkdir} \\\n",
    "--num-epoch-checkpoints 5 \\\n",
    "--output {checkdir}/avg_last_5_checkpoint.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXMFDPpZsqlH"
   },
   "source": [
    "## 確認生成繳交檔案的模型參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TIPilIx5sqlI"
   },
   "outputs": [],
   "source": [
    "# checkpoint_last.pt : 最後一次檢驗的檔案\n",
    "# checkpoint_best.pt : 檢驗 BLEU 最高的檔案\n",
    "# avg_last_5_checkpoint.pt:　最5後個檔案平均\n",
    "try_load_checkpoint(model, name=\"checkpoint_best.pt\")\n",
    "validate(model, task, criterion, log_to_wandb=False)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQe97WdQsqlI"
   },
   "source": [
    "## 進行預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgC-nfWVsqlI"
   },
   "outputs": [],
   "source": [
    "def generate_prediction(model, task, split=\"test\", outfile=\"./prediction.txt\"):    \n",
    "    task.load_dataset(split=split, epoch=1)\n",
    "    itr = load_data_iterator(task, split, 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n",
    "    \n",
    "    idxs = []\n",
    "    hyps = []\n",
    "\n",
    "    model.eval()\n",
    "    progress = tqdm.tqdm(itr, desc=f\"prediction\")\n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(progress):\n",
    "            # validation loss\n",
    "            sample = utils.move_to_cuda(sample, device=device)\n",
    "\n",
    "            # 進行推論\n",
    "            s, h, r = inference_step(sample, model)\n",
    "            \n",
    "            hyps.extend(h)\n",
    "            idxs.extend(list(sample['id']))\n",
    "            \n",
    "    # 根據 preprocess 時的順序排列\n",
    "    hyps = [x for _,x in sorted(zip(idxs,hyps))]\n",
    "    \n",
    "    with open(outfile, \"w\") as f:\n",
    "        for h in hyps:\n",
    "            f.write(h+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8b1Y4DhMsqlI"
   },
   "outputs": [],
   "source": [
    "generate_prediction(model, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZLQN9HsAsqlI"
   },
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZcTQJXJsqlJ"
   },
   "source": [
    "# Back-translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWSkoh-RsqlJ"
   },
   "source": [
    "## 訓練一個反向的翻譯模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2Q4ceXAsqlJ"
   },
   "source": [
    "1. 將實驗的參數設定表中(config)的source_lang與target_lang互相交換\n",
    "2. 將實驗的參數設定表中(config)的savedir更改(ex. \"./checkpoints/rnn-back\")\n",
    "3. 訓練一個反向模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvuBOzKusqlJ"
   },
   "source": [
    "## 利用反向模型生成額外資料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIn3EHgcsqlK"
   },
   "source": [
    "### 下載 monolingual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jOkl5bahsqlK"
   },
   "outputs": [],
   "source": [
    "mono_dataset_name = 'mono'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8D7GbJ2sqlK"
   },
   "outputs": [],
   "source": [
    "mono_prefix = Path(data_dir).absolute() / mono_dataset_name\n",
    "mono_prefix.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "urls = (\n",
    "    '\"https://onedrive.live.com/download?cid=3E549F3B24B238B4&resid=3E549F3B24B238B4%214986&authkey=AANUKbGfZx0kM80\"',\n",
    "# # If the above links die, use the following instead. \n",
    "#     \"https://www.csie.ntu.edu.tw/~r09922057/ML2021-hw5/ted_zh_corpus.deduped.gz\",\n",
    "# # If the above links die, use the following instead. \n",
    "#     \"https://mega.nz/#!vMNnDShR!4eHDxzlpzIpdpeQTD-htatU_C7QwcBTwGDaSeBqH534\",\n",
    ")\n",
    "file_names = (\n",
    "    'ted_zh_corpus.deduped.gz',\n",
    ")\n",
    "\n",
    "for u, f in zip(urls, file_names):\n",
    "    path = mono_prefix/f\n",
    "    if not path.exists():\n",
    "        if 'mega' in u:\n",
    "            !megadl {u} --path {path}\n",
    "        else:\n",
    "            !wget {u} -O {path}\n",
    "    else:\n",
    "        print(f'{f} is exist, skip downloading')\n",
    "    if path.suffix == \".tgz\":\n",
    "        !tar -xvf {path} -C {prefix}\n",
    "    elif path.suffix == \".zip\":\n",
    "        !unzip -o {path} -d {prefix}\n",
    "    elif path.suffix == \".gz\":\n",
    "        !gzip -fkd {path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHIFeljKsqlK"
   },
   "source": [
    "### TODO: 清理資料集\n",
    "\n",
    "1. 將太長、太短的句子移除\n",
    "2. 統一標點符號\n",
    "\n",
    "hint: 可以使用clean_s()來協助"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7HYkgXXqsqlL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKHaAcCcsqlL"
   },
   "source": [
    "### TODO: Subword Units\n",
    "\n",
    "用反向模型的 spm model 將資料切成 subword units\n",
    "\n",
    "hint: spm model 的路徑為 DATA/raw-data/\\[dataset\\]/spm\\[vocab_num\\].model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-q6qS0KhsqlO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1F9jzmOsqlQ"
   },
   "source": [
    "### Binarize\n",
    "\n",
    "使用fairseq將資料轉為binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u5miKBb5sqlQ"
   },
   "outputs": [],
   "source": [
    "binpath = Path('./DATA/data-bin', mono_dataset_name)\n",
    "src_dict_file = './DATA/data-bin/ted2020/dict.en.txt'\n",
    "tgt_dict_file = src_dict_file\n",
    "monopref = str(mono_prefix/\"mono.tok\") # whatever filepath you get after applying subword tokenization\n",
    "if binpath.exists():\n",
    "    print(binpath, \"exists, will not overwrite!\")\n",
    "else:\n",
    "    !python -m fairseq_cli.preprocess\\\n",
    "        --source-lang 'zh'\\\n",
    "        --target-lang 'en'\\\n",
    "        --trainpref {monopref}\\\n",
    "        --destdir {binpath}\\\n",
    "        --srcdict {src_dict_file}\\\n",
    "        --tgtdict {tgt_dict_file}\\\n",
    "        --workers 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_xn2OKHsqlQ"
   },
   "source": [
    "### TODO: 生成反向翻譯資料\n",
    "\n",
    "將 binarized data 加入原本的資料夾中並用一個 split_name 取名\n",
    "\n",
    "ex. ./DATA/data-bin/ted2020/\\[split_name\\].zh-en.\\[\"en\", \"zh\"\\].\\[\"bin\", \"idx\"\\]\n",
    "\n",
    "便可以使用 generate_prediction(model, task, split=\"split_name\")來產生翻譯資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PB5kEBCIsqlR"
   },
   "outputs": [],
   "source": [
    "# 將 binarized data 加入原本的資料夾中並用一個 split_name 取名\n",
    "# ex. ./DATA/data-bin/ted2020/\\[split_name\\].zh-en.\\[\"en\", \"zh\"\\].\\[\"bin\", \"idx\"\\]\n",
    "!cp ./DATA/data-bin/mono/train.zh-en.zh.bin ./DATA/data-bin/ted2020/mono.zh-en.zh.bin\n",
    "!cp ./DATA/data-bin/mono/train.zh-en.zh.idx ./DATA/data-bin/ted2020/mono.zh-en.zh.idx\n",
    "!cp ./DATA/data-bin/mono/train.zh-en.en.bin ./DATA/data-bin/ted2020/mono.zh-en.en.bin\n",
    "!cp ./DATA/data-bin/mono/train.zh-en.en.idx ./DATA/data-bin/ted2020/mono.zh-en.en.idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqieDy5jsqlR"
   },
   "outputs": [],
   "source": [
    "# hint: 用反向模型在 split='mono' 上進行預測，生成 prediction_file\n",
    "# generate_prediction( ... ,split=... ,outfile=... )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7euUB3SsqlS"
   },
   "source": [
    "### TODO: 產生新的dataset\n",
    "\n",
    "1. 將翻譯出來的資料與原先的訓練資料結合\n",
    "2. 使用之前的spm model切出成Subword Units\n",
    "3. 重新使用fairseq將資料轉為binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xC2bnOH0sqlS"
   },
   "outputs": [],
   "source": [
    "# 合併剛剛生成的 prediction_file (.en) 以及中文 mono.zh (.zh)\n",
    "# \n",
    "# hint: 在此用剛剛的 spm model 對 prediction_file 進行切斷詞\n",
    "# spm_model.encode(line, out_type=str)\n",
    "# output: ./DATA/rawdata/mono/mono.tok.en & mono.tok.zh\n",
    "#\n",
    "# hint: 在此用 fairseq 把這些檔案再 binarize\n",
    "# binpath = Path('./DATA/data-bin/synthetic')\n",
    "# src_dict_file = './DATA/data-bin/ted2020/dict.en.txt'\n",
    "# tgt_dict_file = src_dict_file\n",
    "# monopref = ./DATA/rawdata/mono/mono.tok # or whatever path after applying subword tokenization, w/o the suffix (.zh/.en)\n",
    "# if binpath.exists():\n",
    "#     print(binpath, \"exists, will not overwrite!\")\n",
    "# else:\n",
    "#     !python -m fairseq_cli.preprocess\\\n",
    "#         --source-lang 'zh'\\\n",
    "#         --target-lang 'en'\\\n",
    "#         --trainpref {monopref}\\\n",
    "#         --destdir {binpath}\\\n",
    "#         --srcdict {src_dict_file}\\\n",
    "#         --tgtdict {tgt_dict_file}\\\n",
    "#         --workers 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9Zq_R8NsqlS"
   },
   "outputs": [],
   "source": [
    "# 這裡用剛剛準備的檔案合併原先 ted2020 來生成最終 back-translation 的資料\n",
    "!cp -r ./DATA/data-bin/ted2020/ ./DATA/data-bin/ted2020_with_mono/\n",
    "\n",
    "!cp ./DATA/data-bin/synthetic/train.zh-en.zh.bin ./DATA/data-bin/ted2020_with_mono/train1.en-zh.zh.bin\n",
    "!cp ./DATA/data-bin/synthetic/train.zh-en.zh.idx ./DATA/data-bin/ted2020_with_mono/train1.en-zh.zh.idx\n",
    "!cp ./DATA/data-bin/synthetic/train.zh-en.en.bin ./DATA/data-bin/ted2020_with_mono/train1.en-zh.en.bin\n",
    "!cp ./DATA/data-bin/synthetic/train.zh-en.en.idx ./DATA/data-bin/ted2020_with_mono/train1.en-zh.en.idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6BblDUe3sqlS"
   },
   "source": [
    "### TODO: 重新訓練\n",
    "\n",
    "當已經產生新的資料集\n",
    "\n",
    "1. 將實驗的參數設定表(config)中的datadir改為新的資料集(\"./DATA/data-bin/ted2020_with_mono\")\n",
    "2. 將實驗的參數設定表(config)中的source_lang與target_lang設定還原(\"en\", \"zh\")\n",
    "3. 將實驗的參數設定表(config)中的savedir更改(ex. \"./checkpoints/rnn-bt\")\n",
    "4. 重新訓練"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERYLO2LJsqlT"
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTa0ndb7sqlT"
   },
   "source": [
    "1. <a name=ott2019fairseq></a>Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., ... & Auli, M. (2019, June). fairseq: A Fast, Extensible Toolkit for Sequence Modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations) (pp. 48-53).\n",
    "2. <a name=vaswani2017></a>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017, December). Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems (pp. 6000-6010).\n",
    "3. <a name=reimers-2020-multilingual-sentence-bert></a>Reimers, N., & Gurevych, I. (2020, November). Making Monolingual Sentence Embeddings Multilingual Using Knowledge Distillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 4512-4525).\n",
    "4. <a name=tiedemann2012parallel></a>Tiedemann, J. (2012, May). Parallel Data, Tools and Interfaces in OPUS. In Lrec (Vol. 2012, pp. 2214-2218).\n",
    "5. <a name=kudo-richardson-2018-sentencepiece></a>Kudo, T., & Richardson, J. (2018, November). SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (pp. 66-71).\n",
    "6. <a name=sennrich-etal-2016-improving></a>Sennrich, R., Haddow, B., & Birch, A. (2016, August). Improving Neural Machine Translation Models with Monolingual Data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 86-96).\n",
    "7. <a name=edunov-etal-2018-understanding></a>Edunov, S., Ott, M., Auli, M., & Grangier, D. (2018). Understanding Back-Translation at Scale. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (pp. 489-500).\n",
    "8. https://github.com/ajinkyakulkarni14/TED-Multilingual-Parallel-Corpus\n",
    "9. https://ithelp.ithome.com.tw/articles/10233122\n",
    "10. https://nlp.seas.harvard.edu/2018/04/03/attention.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DiHzAjBPsqlT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "HW05_ZH.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "LHY",
   "language": "python",
   "name": "lhy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
