{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN 的原理及其手写复现\n",
    "\n",
    "+ 视频：[29、PyTorch RNN的原理及其手写复现](https://www.bilibili.com/video/BV13i4y1R7jB/)\n",
    "+ [PyTorch RNN 官方文档](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)\n",
    "\n",
    "![RNN 示意图](../imgs/rnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PyTorch 的使用示例\n",
    "\n",
    "### 1.1 单向、单层 RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "SEQ_LEN = 4  # 输入序列长度\n",
    "INPUT_FEATURE_SIZE = 5  # 输入的 feature 大小\n",
    "HIDDEN_SIZE = 3\n",
    "\n",
    "\n",
    "single_rnn = nn.RNN(INPUT_FEATURE_SIZE, HIDDEN_SIZE, num_layers=1, batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.RNN 的输出：\n",
    "\n",
    "+ `output`：各个时刻的 hidden state，shape  为 [B, seq_len, num_directions * hidden]\n",
    "    + 当使用双向时，在 output 最后一维的 num_directions * hidden 元素中，前 hidden 个属于前向 RNN 的结果，后 hidden 个属于反向 RNN 的结果\n",
    "+ `final_state`：最后一个时刻的最终 hidden state，当只有一层 RNN 时，它也就是 `output` 的最后一个结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 17:29:39.594 | INFO     | __main__:<module>:3 - output:\n",
      "tensor([[[ 0.7509,  0.5184, -0.6673],\n",
      "         [ 0.7321, -0.1399,  0.8266],\n",
      "         [ 0.7061,  0.6471, -0.2703],\n",
      "         [-0.6831,  0.5324, -0.1234]],\n",
      "\n",
      "        [[ 0.4595, -0.2751, -0.8516],\n",
      "         [ 0.0469,  0.2887,  0.7436],\n",
      "         [ 0.3225,  0.2615,  0.6178],\n",
      "         [ 0.0899,  0.9116,  0.8549]]], grad_fn=<TransposeBackward1>)\n",
      "2023-01-24 17:29:39.596 | INFO     | __main__:<module>:4 - h_n:\n",
      "tensor([[[-0.6831,  0.5324, -0.1234],\n",
      "         [ 0.0899,  0.9116,  0.8549]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(BATCH_SIZE, SEQ_LEN, INPUT_FEATURE_SIZE)  # batch_size * seq_len * feature_size\n",
    "output, final_state = single_rnn(input)\n",
    "logger.info(f'output:\\n{output}')  # [B, seq_len, num_directions * hidden]\n",
    "logger.info(f'final_state:\\n{final_state}')  # [B, num_directions * num_layers, hidden]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的结果中可以看出，简单 RNN 的最后时刻 output 就等于最终的 hidden state。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 双向、单层 RNN\n",
    "\n",
    "主要是在实例化 `nn.RNN` 时设置 `bidirectional=True`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_rnn = nn.RNN(INPUT_FEATURE_SIZE, HIDDEN_SIZE, num_layers=1, batch_first=True, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 17:29:40.920 | INFO     | __main__:<module>:2 - output:\n",
      "tensor([[[ 0.6008,  0.5499, -0.3351, -0.9113, -0.9562, -0.5820],\n",
      "         [ 0.5309,  0.5310, -0.5073, -0.1493, -0.9351, -0.7374],\n",
      "         [ 0.5353,  0.5634, -0.1619, -0.7628, -0.9164,  0.3288],\n",
      "         [ 0.9418,  0.5155, -0.0323,  0.9042, -0.3992,  0.4058]],\n",
      "\n",
      "        [[ 0.9219, -0.2853, -0.4261, -0.6801, -0.6480, -0.0786],\n",
      "         [-0.3604,  0.9525,  0.2256, -0.6828, -0.8556, -0.4593],\n",
      "         [ 0.5215,  0.5452, -0.0941,  0.8262, -0.7671, -0.5255],\n",
      "         [-0.0206,  0.9360, -0.9212, -0.0835,  0.2962, -0.9106]]],\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "2023-01-24 17:29:40.922 | INFO     | __main__:<module>:3 - output shape: torch.Size([2, 4, 6])\n",
      "2023-01-24 17:29:40.925 | INFO     | __main__:<module>:4 - h_n:\n",
      "tensor([[[ 0.9418,  0.5155, -0.0323],\n",
      "         [-0.0206,  0.9360, -0.9212]],\n",
      "\n",
      "        [[-0.9113, -0.9562, -0.5820],\n",
      "         [-0.6801, -0.6480, -0.0786]]], grad_fn=<StackBackward0>)\n",
      "2023-01-24 17:29:40.926 | INFO     | __main__:<module>:5 - h_n shape: torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "output, final_state = bi_rnn(input)\n",
    "logger.info(f'output:\\n{output}')\n",
    "logger.info(f'output shape: {output.shape}')  # [B, seq_len, num_directions * hidden]\n",
    "logger.info(f'final_state:\\n{final_state}')\n",
    "logger.info(f'final_state shape: {final_state.shape}')  # [num_directions * num_layers, B, hidden]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 单层单向 RNN 的逐行实现\n",
    "\n",
    "$h_t = \\tanh(x_t W_{ih}^T + b_{ih} + h_{t-1}W_{hh}^T + b_{hh})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 Parameter containing:\n",
      "tensor([[-0.2482, -0.0345,  0.1997, -0.2403,  0.4789],\n",
      "        [-0.0277,  0.1436, -0.5319, -0.0487,  0.0538],\n",
      "        [-0.5599,  0.5040, -0.0793,  0.3747,  0.0448]], requires_grad=True)\n",
      "weight_hh_l0 Parameter containing:\n",
      "tensor([[-0.3301,  0.0958, -0.0544],\n",
      "        [ 0.3288, -0.3701,  0.3510],\n",
      "        [ 0.4919,  0.2924, -0.4417]], requires_grad=True)\n",
      "bias_ih_l0 Parameter containing:\n",
      "tensor([ 0.4063,  0.1175, -0.0897], requires_grad=True)\n",
      "bias_hh_l0 Parameter containing:\n",
      "tensor([-0.2630,  0.1842, -0.0850], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 看一下 PyTorch 中 RNN 的参数：\n",
    "for k,v in single_rnn.named_parameters():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 逐行实现 RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 逐行实现 RNN 的前向传播过程\n",
    "def rnn_forward(\n",
    "    input: Tensor,  # [B, T, input_size]\n",
    "    weight_ih: Tensor,  # [hidden, input_size]\n",
    "    weight_hh: Tensor,  # [hidden, hidden]\n",
    "    bias_ih: Tensor,  # [h_dim]\n",
    "    bias_hh: Tensor,  # [h_dim]\n",
    "    h_prev: Tensor,  # 前一时刻的 hidden state, [B, hidden]\n",
    "):\n",
    "    bs, T, input_size = input.shape\n",
    "    h_dim = weight_ih.shape[0]  # 这个维度是根据公式来判断的\n",
    "    \n",
    "    h_out = torch.zeros(bs, T, h_dim)  # 初始化一个输出状态矩阵\n",
    "    for t in range(T):\n",
    "        x = input[:, t, :]  # 获取当前时刻的输入 feature, [bs, input_size]\n",
    "        x = x.unsqueeze(2)  # [bs, input_size, 1]\n",
    "        h_prev = h_prev.unsqueeze(2)  # [B, hidden, 1]\n",
    "        w_ih_batch = weight_ih.unsqueeze(0).tile([bs, 1, 1])  # [bs, h_dim, input_size]\n",
    "        w_hh_batch = weight_hh.unsqueeze(0).tile([bs, 1, 1])  # [bs, h_dim, h_dim]\n",
    "        \n",
    "        w_times_x = torch.bmm(w_ih_batch, x).squeeze(-1)  # [bs, h_dim]\n",
    "        w_times_h = torch.bmm(w_hh_batch, h_prev).squeeze(-1)  # [bs, h_dim]\n",
    "        h_prev = torch.tanh(w_times_x + bias_ih + w_times_h + bias_hh)\n",
    "        \n",
    "        h_out[:, t, :] = h_prev\n",
    "    \n",
    "    return h_out, h_prev.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 结果验证\n",
    "\n",
    "通过与 PyTorch 官方实现的运算结果进行对比，验证 RNN 的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 17:29:43.528 | INFO     | __main__:<module>:12 - 自己实现的 RNN 的 output:\n",
      "tensor([[[ 0.7509,  0.5184, -0.6673],\n",
      "         [ 0.7321, -0.1399,  0.8266],\n",
      "         [ 0.7061,  0.6471, -0.2703],\n",
      "         [-0.6831,  0.5324, -0.1234]],\n",
      "\n",
      "        [[ 0.4595, -0.2751, -0.8516],\n",
      "         [ 0.0469,  0.2887,  0.7436],\n",
      "         [ 0.3225,  0.2615,  0.6178],\n",
      "         [ 0.0899,  0.9116,  0.8549]]], grad_fn=<CopySlices>)\n",
      "2023-01-24 17:29:43.532 | INFO     | __main__:<module>:13 - PyTorch 的 RNN 的 output:\n",
      "tensor([[[ 0.7509,  0.5184, -0.6673],\n",
      "         [ 0.7321, -0.1399,  0.8266],\n",
      "         [ 0.7061,  0.6471, -0.2703],\n",
      "         [-0.6831,  0.5324, -0.1234]],\n",
      "\n",
      "        [[ 0.4595, -0.2751, -0.8516],\n",
      "         [ 0.0469,  0.2887,  0.7436],\n",
      "         [ 0.3225,  0.2615,  0.6178],\n",
      "         [ 0.0899,  0.9116,  0.8549]]], grad_fn=<TransposeBackward1>)\n",
      "2023-01-24 17:29:43.535 | INFO     | __main__:<module>:14 - 自己实现的 RNN 的 final_state:\n",
      "tensor([[[-0.6831,  0.5324, -0.1234],\n",
      "         [ 0.0899,  0.9116,  0.8549]]], grad_fn=<UnsqueezeBackward0>)\n",
      "2023-01-24 17:29:43.538 | INFO     | __main__:<module>:15 - PyTorch 的 RNN 的 final_state:\n",
      "tensor([[[-0.6831,  0.5324, -0.1234],\n",
      "         [ 0.0899,  0.9116,  0.8549]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 验证一下 rnn_forward 的正确性\n",
    "h_prev = torch.zeros(BATCH_SIZE, HIDDEN_SIZE)  # 初始 hidden state\n",
    "output1, final_state1 = rnn_forward(\n",
    "    input, \n",
    "    single_rnn.weight_ih_l0,\n",
    "    single_rnn.weight_hh_l0,\n",
    "    single_rnn.bias_ih_l0,\n",
    "    single_rnn.bias_hh_l0,\n",
    "    h_prev\n",
    ")\n",
    "output2, final_state2 = single_rnn(input, h_prev.unsqueeze(0))\n",
    "logger.info(f'自己实现的 RNN 的 output:\\n{output1}')\n",
    "logger.info(f'PyTorch 的 RNN 的 output:\\n{output2}')\n",
    "logger.info(f'自己实现的 RNN 的 final_state:\\n{final_state1}')\n",
    "logger.info(f'PyTorch 的 RNN 的 final_state:\\n{final_state2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 单层双向 RNN 的逐行实现\n",
    "\n",
    "### 3.1 逐行实现 RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectional_rnn_forward(\n",
    "    input: Tensor,\n",
    "    weight_ih: Tensor,\n",
    "    weight_hh: Tensor,\n",
    "    bias_ih: Tensor,\n",
    "    bias_hh: Tensor,\n",
    "    h_prev: Tensor,\n",
    "    weihgt_ih_reverse: Tensor,\n",
    "    weight_hh_reverse: Tensor,\n",
    "    bias_ih_reverse: Tensor,\n",
    "    bias_hh_reverse: Tensor,\n",
    "    h_prev_reverse: Tensor\n",
    "):\n",
    "    NUM_DIRECTIONS = 2  # 表示双向\n",
    "    bs, T, input_size = input.shape\n",
    "    h_dim = weight_ih.shape[0]  # 这个维度是根据公式来判断的\n",
    "    \n",
    "    forward_output, _ = rnn_forward(input, weight_ih, weight_hh, bias_ih, bias_hh, h_prev)\n",
    "    reverse_input = input.flip([1])  # 在 dim=1 上进行翻转\n",
    "    backward_output, _ = rnn_forward(reverse_input, weihgt_ih_reverse, weight_hh_reverse, bias_ih_reverse, bias_hh_reverse, h_prev_reverse)\n",
    "    \n",
    "    h_out = torch.zeros(bs, T, h_dim * NUM_DIRECTIONS)  # 初始化一个输出状态矩阵，在最后一维上，前 h_dim 表示前向 RNN 的，后 h_dim 表示反向 RNN 的\n",
    "    h_out[:, :, :h_dim] = forward_output\n",
    "    h_out[:, :, h_dim:] = torch.flip(backward_output, [1])\n",
    "    \n",
    "    h_n = torch.zeros(bs, NUM_DIRECTIONS, h_dim)\n",
    "    h_n[:, 0, :] = forward_output[:, -1, :]  # 前向 RNN 的最后时刻的 hidden state\n",
    "    h_n[:, 1, :] = backward_output[:, -1, :]  # 前向 RNN 的最后时刻的 hidden state\n",
    "    \n",
    "    # 为保持与 PyTorch 输出形状一致，对 h_n 进行简单的变换\n",
    "    h_n = h_n.transpose(0, 1)  # [num_directions, B, h_dim]\n",
    "    return h_out, h_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 结果验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 Parameter containing:\n",
      "tensor([[ 0.5102, -0.3869, -0.0193, -0.0843, -0.0105],\n",
      "        [-0.2907,  0.3793, -0.2157,  0.0909, -0.1167],\n",
      "        [ 0.2057,  0.1608,  0.5355, -0.1843, -0.1846]], requires_grad=True)\n",
      "weight_hh_l0 Parameter containing:\n",
      "tensor([[-0.2516,  0.4806, -0.0364],\n",
      "        [ 0.5271, -0.3576,  0.0826],\n",
      "        [ 0.4867,  0.0360,  0.4172]], requires_grad=True)\n",
      "bias_ih_l0 Parameter containing:\n",
      "tensor([ 0.4573,  0.2591, -0.2537], requires_grad=True)\n",
      "bias_hh_l0 Parameter containing:\n",
      "tensor([ 0.3143,  0.4186, -0.1830], requires_grad=True)\n",
      "weight_ih_l0_reverse Parameter containing:\n",
      "tensor([[ 0.3595,  0.4310,  0.0465,  0.3054,  0.0413],\n",
      "        [ 0.0251, -0.4296, -0.4924,  0.4548, -0.1660],\n",
      "        [ 0.4237, -0.2697,  0.3554, -0.1753, -0.0984]], requires_grad=True)\n",
      "weight_hh_l0_reverse Parameter containing:\n",
      "tensor([[-0.2429,  0.4347, -0.0359],\n",
      "        [ 0.4497,  0.5165, -0.2733],\n",
      "        [ 0.4508,  0.4567,  0.3245]], requires_grad=True)\n",
      "bias_ih_l0_reverse Parameter containing:\n",
      "tensor([ 0.0549, -0.5396,  0.0479], requires_grad=True)\n",
      "bias_hh_l0_reverse Parameter containing:\n",
      "tensor([-0.0503, -0.0547, -0.0525], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 先看一下 PyTorch 中的参数\n",
    "for k, v in bi_rnn.named_parameters():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 17:29:45.431 | INFO     | __main__:<module>:18 - 自己实现的 RNN 的 output:\n",
      "tensor([[[ 0.6008,  0.5499, -0.3351, -0.9113, -0.9562, -0.5820],\n",
      "         [ 0.5309,  0.5310, -0.5073, -0.1493, -0.9351, -0.7374],\n",
      "         [ 0.5353,  0.5634, -0.1619, -0.7628, -0.9164,  0.3288],\n",
      "         [ 0.9418,  0.5155, -0.0323,  0.9042, -0.3992,  0.4058]],\n",
      "\n",
      "        [[ 0.9219, -0.2853, -0.4261, -0.6801, -0.6480, -0.0786],\n",
      "         [-0.3604,  0.9525,  0.2256, -0.6828, -0.8556, -0.4593],\n",
      "         [ 0.5215,  0.5452, -0.0941,  0.8262, -0.7671, -0.5255],\n",
      "         [-0.0206,  0.9360, -0.9212, -0.0835,  0.2962, -0.9106]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "2023-01-24 17:29:45.434 | INFO     | __main__:<module>:19 - PyTorch 的 RNN 的 output:\n",
      "tensor([[[ 0.6008,  0.5499, -0.3351, -0.9113, -0.9562, -0.5820],\n",
      "         [ 0.5309,  0.5310, -0.5073, -0.1493, -0.9351, -0.7374],\n",
      "         [ 0.5353,  0.5634, -0.1619, -0.7628, -0.9164,  0.3288],\n",
      "         [ 0.9418,  0.5155, -0.0323,  0.9042, -0.3992,  0.4058]],\n",
      "\n",
      "        [[ 0.9219, -0.2853, -0.4261, -0.6801, -0.6480, -0.0786],\n",
      "         [-0.3604,  0.9525,  0.2256, -0.6828, -0.8556, -0.4593],\n",
      "         [ 0.5215,  0.5452, -0.0941,  0.8262, -0.7671, -0.5255],\n",
      "         [-0.0206,  0.9360, -0.9212, -0.0835,  0.2962, -0.9106]]],\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "2023-01-24 17:29:45.437 | INFO     | __main__:<module>:20 - 自己实现的 RNN 的 final_state:\n",
      "tensor([[[ 0.9418,  0.5155, -0.0323],\n",
      "         [-0.0206,  0.9360, -0.9212]],\n",
      "\n",
      "        [[-0.9113, -0.9562, -0.5820],\n",
      "         [-0.6801, -0.6480, -0.0786]]], grad_fn=<TransposeBackward0>)\n",
      "2023-01-24 17:29:45.439 | INFO     | __main__:<module>:21 - PyTorch 的 RNN 的 final_state:\n",
      "tensor([[[ 0.9418,  0.5155, -0.0323],\n",
      "         [-0.0206,  0.9360, -0.9212]],\n",
      "\n",
      "        [[-0.9113, -0.9562, -0.5820],\n",
      "         [-0.6801, -0.6480, -0.0786]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "NUM_DIRECTIONS = 2  # 双向\n",
    "h_prev = torch.zeros(NUM_DIRECTIONS, BATCH_SIZE, HIDDEN_SIZE)\n",
    "output1, final_state1 = bidirectional_rnn_forward(\n",
    "    input,\n",
    "    bi_rnn.weight_ih_l0,\n",
    "    bi_rnn.weight_hh_l0,\n",
    "    bi_rnn.bias_ih_l0,\n",
    "    bi_rnn.bias_hh_l0,\n",
    "    h_prev[0],\n",
    "    bi_rnn.weight_ih_l0_reverse,\n",
    "    bi_rnn.weight_hh_l0_reverse,\n",
    "    bi_rnn.bias_ih_l0_reverse,\n",
    "    bi_rnn.bias_hh_l0_reverse,\n",
    "    h_prev[1]\n",
    ")\n",
    "output2, final_state2 = bi_rnn(input, h_prev)\n",
    "\n",
    "logger.info(f'自己实现的 RNN 的 output:\\n{output1}')\n",
    "logger.info(f'PyTorch 的 RNN 的 output:\\n{output2}')\n",
    "logger.info(f'自己实现的 RNN 的 final_state:\\n{final_state1}')\n",
    "logger.info(f'PyTorch 的 RNN 的 final_state:\\n{final_state2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "0418effca45178467ac68c18e34d93809a092be692e0a4443d8690099b71f4bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
