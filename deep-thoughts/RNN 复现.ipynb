{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN 的原理及其手写复现\n",
    "\n",
    "+ 视频：[29、PyTorch RNN的原理及其手写复现](https://www.bilibili.com/video/BV13i4y1R7jB/)\n",
    "+ 视频：[30、PyTorch LSTM和LSTMP的原理及其手写复现](https://www.bilibili.com/video/BV1zq4y1m7aH/)\n",
    "+ [Gated RNN | yubinCloud](https://yubincloud.github.io/notebook/pages/nlp/gated-rnn/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from loguru import logger\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PyTorch 的使用示例\n",
    "\n",
    "+ [PyTorch RNN 官方文档](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)\n",
    "\n",
    "### 1.1 单向、单层 RNN\n",
    "\n",
    "![RNN 示例](../imgs/rnn.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "SEQ_LEN = 4  # 输入序列长度\n",
    "INPUT_FEATURE_SIZE = 5  # 输入的 feature 大小\n",
    "HIDDEN_SIZE = 3\n",
    "\n",
    "\n",
    "single_rnn = nn.RNN(INPUT_FEATURE_SIZE, HIDDEN_SIZE, num_layers=1, batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.RNN 的输出：\n",
    "\n",
    "+ `output`：各个时刻的 hidden state，shape  为 [B, seq_len, num_directions * hidden]\n",
    "    + 当使用双向时，在 output 最后一维的 num_directions * hidden 元素中，前 hidden 个属于前向 RNN 的结果，后 hidden 个属于反向 RNN 的结果\n",
    "+ `final_state`：最后一个时刻的最终 hidden state，当只有一层 RNN 时，它也就是 `output` 的最后一个结果\n",
    "\n",
    "对于 many-to-many 的 task，往往是使用 output，比如词性标注任务；对于 many-to-one 的 task，往往是使用 final_state，比如文本分类任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 22:32:26.784 | INFO     | __main__:<module>:3 - output:\n",
      "tensor([[[ 0.5458, -0.3049, -0.2295],\n",
      "         [ 0.7414, -0.5016,  0.9513],\n",
      "         [ 0.2096,  0.5185, -0.2949],\n",
      "         [ 0.5182,  0.0200,  0.2915]],\n",
      "\n",
      "        [[ 0.2186, -0.3699, -0.4636],\n",
      "         [ 0.4902, -0.7225,  0.4205],\n",
      "         [ 0.4375,  0.0294,  0.8530],\n",
      "         [ 0.7765,  0.1054, -0.9559]]], grad_fn=<TransposeBackward1>)\n",
      "2023-01-24 22:32:26.787 | INFO     | __main__:<module>:4 - final_state:\n",
      "tensor([[[ 0.5182,  0.0200,  0.2915],\n",
      "         [ 0.7765,  0.1054, -0.9559]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(BATCH_SIZE, SEQ_LEN, INPUT_FEATURE_SIZE)  # batch_size * seq_len * feature_size\n",
    "output, final_state = single_rnn(input)\n",
    "logger.info(f'output:\\n{output}')  # [B, seq_len, num_directions * hidden]\n",
    "logger.info(f'final_state:\\n{final_state}')  # [B, num_directions * num_layers, hidden]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的结果中可以看出，简单 RNN 的最后时刻 output 就等于最终的 hidden state。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 双向、单层 RNN\n",
    "\n",
    "主要是在实例化 `nn.RNN` 时设置 `bidirectional=True`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_rnn = nn.RNN(INPUT_FEATURE_SIZE, HIDDEN_SIZE, num_layers=1, batch_first=True, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 22:32:28.516 | INFO     | __main__:<module>:2 - output:\n",
      "tensor([[[-6.8374e-03, -4.0665e-01,  4.9632e-01, -7.8123e-01,  3.5904e-01,\n",
      "          -3.2676e-01],\n",
      "         [-9.6864e-02,  4.9356e-01,  9.1294e-01, -8.2427e-01, -2.3916e-01,\n",
      "           4.9574e-01],\n",
      "         [-1.6656e-01, -7.5501e-01,  3.4672e-04, -4.8054e-01,  4.9172e-01,\n",
      "          -4.0495e-01],\n",
      "         [ 7.7775e-01, -1.6314e-02,  2.2688e-01, -8.2076e-01, -5.5325e-01,\n",
      "          -6.6492e-01]],\n",
      "\n",
      "        [[-1.6269e-01, -9.5102e-01,  4.3767e-01, -9.4389e-01, -7.0420e-01,\n",
      "          -4.5849e-01],\n",
      "         [ 9.1083e-01,  9.5602e-02,  6.9448e-01, -7.2612e-01, -6.6763e-01,\n",
      "           4.7001e-01],\n",
      "         [-7.7787e-01, -9.8655e-03,  6.3926e-01,  3.7862e-01,  2.8350e-01,\n",
      "          -5.2208e-01],\n",
      "         [-1.5028e-01, -8.3701e-01,  7.5149e-01,  2.6977e-01,  8.1088e-01,\n",
      "           5.1595e-01]]], grad_fn=<TransposeBackward1>)\n",
      "2023-01-24 22:32:28.518 | INFO     | __main__:<module>:3 - output shape: torch.Size([2, 4, 6])\n",
      "2023-01-24 22:32:28.521 | INFO     | __main__:<module>:4 - final_state:\n",
      "tensor([[[ 0.7777, -0.0163,  0.2269],\n",
      "         [-0.1503, -0.8370,  0.7515]],\n",
      "\n",
      "        [[-0.7812,  0.3590, -0.3268],\n",
      "         [-0.9439, -0.7042, -0.4585]]], grad_fn=<StackBackward0>)\n",
      "2023-01-24 22:32:28.522 | INFO     | __main__:<module>:5 - final_state shape: torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "output, final_state = bi_rnn(input)\n",
    "logger.info(f'output:\\n{output}')\n",
    "logger.info(f'output shape: {output.shape}')  # [B, seq_len, num_directions * hidden]\n",
    "logger.info(f'final_state:\\n{final_state}')\n",
    "logger.info(f'final_state shape: {final_state.shape}')  # [num_directions * num_layers, B, hidden]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 单层单向 RNN 的逐行实现\n",
    "\n",
    "$h_t = \\tanh(x_t W_{ih}^T + b_{ih} + h_{t-1}W_{hh}^T + b_{hh})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 Parameter containing:\n",
      "tensor([[-0.0663,  0.2330,  0.0046,  0.3284,  0.1327],\n",
      "        [-0.0404, -0.0758,  0.1053, -0.2830,  0.0785],\n",
      "        [ 0.0597, -0.4974, -0.3926,  0.1548,  0.4810]], requires_grad=True)\n",
      "weight_hh_l0 Parameter containing:\n",
      "tensor([[ 0.4251,  0.3280, -0.1945],\n",
      "        [ 0.5345,  0.4590,  0.5039],\n",
      "        [ 0.2841, -0.5721, -0.3805]], requires_grad=True)\n",
      "bias_ih_l0 Parameter containing:\n",
      "tensor([ 0.2347, -0.0224, -0.1350], requires_grad=True)\n",
      "bias_hh_l0 Parameter containing:\n",
      "tensor([ 0.0991, -0.1741,  0.2732], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 看一下 PyTorch 中 RNN 的参数：\n",
    "for k,v in single_rnn.named_parameters():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 逐行实现 RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 逐行实现 RNN 的前向传播过程\n",
    "def rnn_forward(\n",
    "    input: Tensor,  # [B, T, input_size]\n",
    "    weight_ih: Tensor,  # [hidden, input_size]\n",
    "    weight_hh: Tensor,  # [hidden, hidden]\n",
    "    bias_ih: Tensor,  # [h_dim]\n",
    "    bias_hh: Tensor,  # [h_dim]\n",
    "    h_prev: Tensor,  # 前一时刻的 hidden state, [B, hidden]\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    bs, T, input_size = input.shape\n",
    "    h_dim = weight_ih.shape[0]  # 这个维度是根据公式来判断的\n",
    "    \n",
    "    h_out = torch.zeros(bs, T, h_dim)  # 初始化一个输出状态矩阵\n",
    "    for t in range(T):\n",
    "        x = input[:, t, :]  # 获取当前时刻的输入 feature, [bs, input_size]\n",
    "        x = x.unsqueeze(2)  # [bs, input_size, 1]\n",
    "        h_prev = h_prev.unsqueeze(2)  # [B, hidden, 1]\n",
    "        w_ih_batch = weight_ih.unsqueeze(0).tile([bs, 1, 1])  # [bs, h_dim, input_size]\n",
    "        w_hh_batch = weight_hh.unsqueeze(0).tile([bs, 1, 1])  # [bs, h_dim, h_dim]\n",
    "        \n",
    "        w_times_x = torch.bmm(w_ih_batch, x).squeeze(-1)  # [bs, h_dim]\n",
    "        w_times_h = torch.bmm(w_hh_batch, h_prev).squeeze(-1)  # [bs, h_dim]\n",
    "        h_prev = torch.tanh(w_times_x + bias_ih + w_times_h + bias_hh)\n",
    "        \n",
    "        h_out[:, t, :] = h_prev\n",
    "    \n",
    "    return h_out, h_prev.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 结果验证\n",
    "\n",
    "通过与 PyTorch 官方实现的运算结果进行对比，验证 RNN 的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 22:32:30.762 | INFO     | __main__:<module>:12 - 自己实现的 RNN 的 output:\n",
      "tensor([[[ 0.5458, -0.3049, -0.2295],\n",
      "         [ 0.7414, -0.5016,  0.9513],\n",
      "         [ 0.2096,  0.5185, -0.2949],\n",
      "         [ 0.5182,  0.0200,  0.2915]],\n",
      "\n",
      "        [[ 0.2186, -0.3699, -0.4636],\n",
      "         [ 0.4902, -0.7225,  0.4205],\n",
      "         [ 0.4375,  0.0294,  0.8530],\n",
      "         [ 0.7765,  0.1054, -0.9559]]], grad_fn=<CopySlices>)\n",
      "2023-01-24 22:32:30.766 | INFO     | __main__:<module>:13 - PyTorch 的 RNN 的 output:\n",
      "tensor([[[ 0.5458, -0.3049, -0.2295],\n",
      "         [ 0.7414, -0.5016,  0.9513],\n",
      "         [ 0.2096,  0.5185, -0.2949],\n",
      "         [ 0.5182,  0.0200,  0.2915]],\n",
      "\n",
      "        [[ 0.2186, -0.3699, -0.4636],\n",
      "         [ 0.4902, -0.7225,  0.4205],\n",
      "         [ 0.4375,  0.0294,  0.8530],\n",
      "         [ 0.7765,  0.1054, -0.9559]]], grad_fn=<TransposeBackward1>)\n",
      "2023-01-24 22:32:30.769 | INFO     | __main__:<module>:14 - 自己实现的 RNN 的 final_state:\n",
      "tensor([[[ 0.5182,  0.0200,  0.2915],\n",
      "         [ 0.7765,  0.1054, -0.9559]]], grad_fn=<UnsqueezeBackward0>)\n",
      "2023-01-24 22:32:30.771 | INFO     | __main__:<module>:15 - PyTorch 的 RNN 的 final_state:\n",
      "tensor([[[ 0.5182,  0.0200,  0.2915],\n",
      "         [ 0.7765,  0.1054, -0.9559]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 验证一下 rnn_forward 的正确性\n",
    "h_prev = torch.zeros(BATCH_SIZE, HIDDEN_SIZE)  # 初始 hidden state\n",
    "output1, final_state1 = rnn_forward(\n",
    "    input, \n",
    "    single_rnn.weight_ih_l0,\n",
    "    single_rnn.weight_hh_l0,\n",
    "    single_rnn.bias_ih_l0,\n",
    "    single_rnn.bias_hh_l0,\n",
    "    h_prev\n",
    ")\n",
    "output2, final_state2 = single_rnn(input, h_prev.unsqueeze(0))\n",
    "logger.info(f'自己实现的 RNN 的 output:\\n{output1}')\n",
    "logger.info(f'PyTorch 的 RNN 的 output:\\n{output2}')\n",
    "logger.info(f'自己实现的 RNN 的 final_state:\\n{final_state1}')\n",
    "logger.info(f'PyTorch 的 RNN 的 final_state:\\n{final_state2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 单层双向 RNN 的逐行实现\n",
    "\n",
    "### 3.1 逐行实现 RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectional_rnn_forward(\n",
    "    input: Tensor,\n",
    "    weight_ih: Tensor,\n",
    "    weight_hh: Tensor,\n",
    "    bias_ih: Tensor,\n",
    "    bias_hh: Tensor,\n",
    "    h_prev: Tensor,\n",
    "    weihgt_ih_reverse: Tensor,\n",
    "    weight_hh_reverse: Tensor,\n",
    "    bias_ih_reverse: Tensor,\n",
    "    bias_hh_reverse: Tensor,\n",
    "    h_prev_reverse: Tensor\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    NUM_DIRECTIONS = 2  # 表示双向\n",
    "    bs, T, input_size = input.shape\n",
    "    h_dim = weight_ih.shape[0]  # 这个维度是根据公式来判断的\n",
    "    \n",
    "    forward_output, _ = rnn_forward(input, weight_ih, weight_hh, bias_ih, bias_hh, h_prev)\n",
    "    reverse_input = input.flip([1])  # 在 dim=1 上进行翻转\n",
    "    backward_output, _ = rnn_forward(reverse_input, weihgt_ih_reverse, weight_hh_reverse, bias_ih_reverse, bias_hh_reverse, h_prev_reverse)\n",
    "    \n",
    "    h_out = torch.zeros(bs, T, h_dim * NUM_DIRECTIONS)  # 初始化一个输出状态矩阵，在最后一维上，前 h_dim 表示前向 RNN 的，后 h_dim 表示反向 RNN 的\n",
    "    h_out[:, :, :h_dim] = forward_output\n",
    "    h_out[:, :, h_dim:] = torch.flip(backward_output, [1])\n",
    "    \n",
    "    h_n = torch.zeros(bs, NUM_DIRECTIONS, h_dim)\n",
    "    h_n[:, 0, :] = forward_output[:, -1, :]  # 前向 RNN 的最后时刻的 hidden state\n",
    "    h_n[:, 1, :] = backward_output[:, -1, :]  # 前向 RNN 的最后时刻的 hidden state\n",
    "    \n",
    "    # 为保持与 PyTorch 输出形状一致，对 h_n 进行简单的变换\n",
    "    h_n = h_n.transpose(0, 1)  # [num_directions, B, h_dim]\n",
    "    return h_out, h_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 结果验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 Parameter containing:\n",
      "tensor([[-0.3353, -0.4725,  0.2578,  0.0378, -0.4286],\n",
      "        [-0.5393, -0.3727,  0.0892,  0.5427,  0.3393],\n",
      "        [ 0.1132,  0.2473, -0.4087,  0.5298,  0.1923]], requires_grad=True)\n",
      "weight_hh_l0 Parameter containing:\n",
      "tensor([[ 0.0203, -0.4756, -0.1851],\n",
      "        [ 0.3703, -0.0337, -0.0619],\n",
      "        [-0.1893, -0.1889,  0.1521]], requires_grad=True)\n",
      "bias_ih_l0 Parameter containing:\n",
      "tensor([-0.5668, -0.4108, -0.2017], requires_grad=True)\n",
      "bias_hh_l0 Parameter containing:\n",
      "tensor([ 0.5474, -0.3007,  0.5214], requires_grad=True)\n",
      "weight_ih_l0_reverse Parameter containing:\n",
      "tensor([[-0.2600,  0.4810,  0.0042, -0.3229,  0.2498],\n",
      "        [-0.3743,  0.5541,  0.3081, -0.0407,  0.2954],\n",
      "        [-0.0182,  0.2970, -0.2924,  0.5375, -0.1690]], requires_grad=True)\n",
      "weight_hh_l0_reverse Parameter containing:\n",
      "tensor([[ 0.5083,  0.5229, -0.5136],\n",
      "        [-0.4806,  0.4039, -0.3679],\n",
      "        [-0.2514,  0.4468, -0.4755]], requires_grad=True)\n",
      "bias_ih_l0_reverse Parameter containing:\n",
      "tensor([-0.4340,  0.0677, -0.3794], requires_grad=True)\n",
      "bias_hh_l0_reverse Parameter containing:\n",
      "tensor([-0.2032, -0.3876, -0.2332], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 先看一下 PyTorch 中的参数\n",
    "for k, v in bi_rnn.named_parameters():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 22:32:32.888 | INFO     | __main__:<module>:18 - 自己实现的 RNN 的 output:\n",
      "tensor([[[-6.8374e-03, -4.0665e-01,  4.9632e-01, -7.8123e-01,  3.5904e-01,\n",
      "          -3.2676e-01],\n",
      "         [-9.6864e-02,  4.9356e-01,  9.1294e-01, -8.2427e-01, -2.3916e-01,\n",
      "           4.9574e-01],\n",
      "         [-1.6656e-01, -7.5501e-01,  3.4672e-04, -4.8054e-01,  4.9172e-01,\n",
      "          -4.0495e-01],\n",
      "         [ 7.7775e-01, -1.6314e-02,  2.2688e-01, -8.2076e-01, -5.5325e-01,\n",
      "          -6.6492e-01]],\n",
      "\n",
      "        [[-1.6269e-01, -9.5102e-01,  4.3767e-01, -9.4389e-01, -7.0420e-01,\n",
      "          -4.5849e-01],\n",
      "         [ 9.1083e-01,  9.5602e-02,  6.9448e-01, -7.2612e-01, -6.6763e-01,\n",
      "           4.7001e-01],\n",
      "         [-7.7787e-01, -9.8655e-03,  6.3926e-01,  3.7862e-01,  2.8350e-01,\n",
      "          -5.2208e-01],\n",
      "         [-1.5028e-01, -8.3701e-01,  7.5149e-01,  2.6977e-01,  8.1088e-01,\n",
      "           5.1595e-01]]], grad_fn=<CopySlices>)\n",
      "2023-01-24 22:32:32.892 | INFO     | __main__:<module>:19 - PyTorch 的 RNN 的 output:\n",
      "tensor([[[-6.8374e-03, -4.0665e-01,  4.9632e-01, -7.8123e-01,  3.5904e-01,\n",
      "          -3.2676e-01],\n",
      "         [-9.6864e-02,  4.9356e-01,  9.1294e-01, -8.2427e-01, -2.3916e-01,\n",
      "           4.9574e-01],\n",
      "         [-1.6656e-01, -7.5501e-01,  3.4672e-04, -4.8054e-01,  4.9172e-01,\n",
      "          -4.0495e-01],\n",
      "         [ 7.7775e-01, -1.6314e-02,  2.2688e-01, -8.2076e-01, -5.5325e-01,\n",
      "          -6.6492e-01]],\n",
      "\n",
      "        [[-1.6269e-01, -9.5102e-01,  4.3767e-01, -9.4389e-01, -7.0420e-01,\n",
      "          -4.5849e-01],\n",
      "         [ 9.1083e-01,  9.5602e-02,  6.9448e-01, -7.2612e-01, -6.6763e-01,\n",
      "           4.7001e-01],\n",
      "         [-7.7787e-01, -9.8655e-03,  6.3926e-01,  3.7862e-01,  2.8350e-01,\n",
      "          -5.2208e-01],\n",
      "         [-1.5028e-01, -8.3701e-01,  7.5149e-01,  2.6977e-01,  8.1088e-01,\n",
      "           5.1595e-01]]], grad_fn=<TransposeBackward1>)\n",
      "2023-01-24 22:32:32.894 | INFO     | __main__:<module>:20 - 自己实现的 RNN 的 final_state:\n",
      "tensor([[[ 0.7777, -0.0163,  0.2269],\n",
      "         [-0.1503, -0.8370,  0.7515]],\n",
      "\n",
      "        [[-0.7812,  0.3590, -0.3268],\n",
      "         [-0.9439, -0.7042, -0.4585]]], grad_fn=<TransposeBackward0>)\n",
      "2023-01-24 22:32:32.897 | INFO     | __main__:<module>:21 - PyTorch 的 RNN 的 final_state:\n",
      "tensor([[[ 0.7777, -0.0163,  0.2269],\n",
      "         [-0.1503, -0.8370,  0.7515]],\n",
      "\n",
      "        [[-0.7812,  0.3590, -0.3268],\n",
      "         [-0.9439, -0.7042, -0.4585]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "NUM_DIRECTIONS = 2  # 双向\n",
    "h_prev = torch.zeros(NUM_DIRECTIONS, BATCH_SIZE, HIDDEN_SIZE)\n",
    "output1, final_state1 = bidirectional_rnn_forward(\n",
    "    input,\n",
    "    bi_rnn.weight_ih_l0,\n",
    "    bi_rnn.weight_hh_l0,\n",
    "    bi_rnn.bias_ih_l0,\n",
    "    bi_rnn.bias_hh_l0,\n",
    "    h_prev[0],\n",
    "    bi_rnn.weight_ih_l0_reverse,\n",
    "    bi_rnn.weight_hh_l0_reverse,\n",
    "    bi_rnn.bias_ih_l0_reverse,\n",
    "    bi_rnn.bias_hh_l0_reverse,\n",
    "    h_prev[1]\n",
    ")\n",
    "output2, final_state2 = bi_rnn(input, h_prev)\n",
    "\n",
    "logger.info(f'自己实现的 RNN 的 output:\\n{output1}')\n",
    "logger.info(f'PyTorch 的 RNN 的 output:\\n{output2}')\n",
    "logger.info(f'自己实现的 RNN 的 final_state:\\n{final_state1}')\n",
    "logger.info(f'PyTorch 的 RNN 的 final_state:\\n{final_state2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LSTM 手写实现\n",
    "\n",
    "![LSTM 示意图](../imgs/LSTM.png)\n",
    "\n",
    "计算公式：\n",
    "\n",
    "+ 输入门：$i_t = \\sigma(W_{ii}x_t + b_{ii} + W_{hi}h_{t-1}+b_{hi})$\n",
    "+ 遗忘门：$f_t = \\sigma(W_{if}x_t + b_{if} + W_{hf}h_{t-1}+b_{hf})$\n",
    "+ cell 门：$g_t = \\tanh(W_{ig}x_t + b_{ig} + W_{hg}h_{t-1}+b_{hg})$\n",
    "+ 输出门：$o_t = \\tanh(W_{io}x_t + b_{io} + W_{ho}h_{t-1}+b_{ho})$\n",
    "+ 记忆单元的更新：$c_t = f_t \\odot c_{t-1} + i_t \\odot g_t$\n",
    "+ 隐藏状态的更新：$h_t = o_t \\odot \\tanh(c_t)$\n",
    "\n",
    "### 4.1 PyTorch 官方 API\n",
    "\n",
    "+ [PyTorch LSTM 官方文档](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义常量\n",
    "BATCH_SIZE = 2\n",
    "SEQ_LEN = 3\n",
    "INPUT_SIZE = 4\n",
    "HIDDEN_SIZE = 5\n",
    "\n",
    "input = torch.randn(BATCH_SIZE, SEQ_LEN, INPUT_SIZE)  # 输入序列\n",
    "c0 = torch.randn(BATCH_SIZE, HIDDEN_SIZE)  # 初始记忆单元，不会参与训练\n",
    "h0 = torch.randn(BATCH_SIZE, HIDDEN_SIZE)  # 初始 hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 22:32:33.769 | INFO     | __main__:<module>:7 - LSTM 的 output shape: torch.Size([2, 3, 5])\n",
      "2023-01-24 22:32:33.771 | INFO     | __main__:<module>:8 - LSTM 的 h_final shape: torch.Size([1, 2, 5])\n",
      "2023-01-24 22:32:33.772 | INFO     | __main__:<module>:9 - LSTM 的 c_final shape: torch.Size([1, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "# 调用官方 API\n",
    "lstm_layer = nn.LSTM(INPUT_SIZE, HIDDEN_SIZE, batch_first=True)\n",
    "output, (h_final, c_final) = lstm_layer(\n",
    "    input,\n",
    "    (h0.unsqueeze(0), c0.unsqueeze(0))  # 调用 unsqueeze 是因为我们用的是单向的，需要符合 API 的 shape 要求\n",
    ")\n",
    "logger.info(f'LSTM 的 output shape: {output.shape}')   # [B, seq_len, h_dim]\n",
    "logger.info(f'LSTM 的 h_final shape: {h_final.shape}') # [1, B, h_dim]\n",
    "logger.info(f'LSTM 的 c_final shape: {c_final.shape}') # [1, B, h_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 torch.Size([20, 4])\n",
      "weight_hh_l0 torch.Size([20, 5])\n",
      "bias_ih_l0 torch.Size([20])\n",
      "bias_hh_l0 torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "# 查看官方实现的 LSTM 中的参数\n",
    "for k, v in lstm_layer.named_parameters():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 逐行实现 LSTM\n",
    "\n",
    "+ 输入门：$i_t = \\sigma(W_{ii}x_t + b_{ii} + W_{hi}h_{t-1}+b_{hi})$\n",
    "+ 遗忘门：$f_t = \\sigma(W_{if}x_t + b_{if} + W_{hf}h_{t-1}+b_{hf})$\n",
    "+ cell 门：$g_t = \\tanh(W_{ig}x_t + b_{ig} + W_{hg}h_{t-1}+b_{hg})$\n",
    "+ 输出门：$o_t = \\tanh(W_{io}x_t + b_{io} + W_{ho}h_{t-1}+b_{ho})$\n",
    "+ 记忆单元的更新：$c_t = f_t \\odot c_{t-1} + i_t \\odot g_t$\n",
    "+ 隐藏状态的更新：$h_t = o_t \\odot \\tanh(c_t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_forward(\n",
    "    input: Tensor,  # [B, seq_len, h_dim]\n",
    "    initial_states: Tuple[Tensor, Tensor],\n",
    "    w_ih: Tensor,  # [h_dim*4, input_size]\n",
    "    w_hh: Tensor,  # [h_dim*4, h_dim]\n",
    "    b_ih: Tensor,  # [h_dim*4]\n",
    "    b_hh: Tensor   # [h_dim*4]\n",
    ") -> Tuple[Tensor, Tuple[Tensor, Tensor]]:\n",
    "    h0, c0 = initial_states  # 初始状态\n",
    "    bs, T, input_size = input.shape\n",
    "    h_dim = w_ih.shape[0] // 4  # 因为 w_ih 是四个 W 拼接起来的\n",
    "    \n",
    "    prev_h = h0  # [bs, h_dim]\n",
    "    prev_c = c0\n",
    "    # 对 W 进行一下扩维，方便之后与 x 进行 mini-batch 的运算\n",
    "    batch_w_ih = w_ih.unsqueeze(0).tile([bs, 1, 1])  # [bs, h_dim*4, input_size]\n",
    "    batch_w_hh = w_hh.unsqueeze(0).tile([bs, 1, 1])  # [bs, h_dim*4, h_dim]\n",
    "    \n",
    "    h_out = torch.zeros(bs, T, h_dim)  # 输出序列\n",
    "    \n",
    "    for t in range(T):\n",
    "        x = input[:, t, :]  # 当前时刻的输入向量, [bs, input_size]\n",
    "        \n",
    "        x = x.unsqueeze(-1)  # [bs, input_size, 1]\n",
    "        w_times_x = torch.bmm(batch_w_ih, x).squeeze(-1)  # [bs, h_dim*4]\n",
    "        \n",
    "        prev_h = prev_h.unsqueeze(-1)  # [bs, h_dim, 1]\n",
    "        w_times_h = torch.bmm(batch_w_hh, prev_h).squeeze(-1)  #[bs. h_dim*4]\n",
    "        \n",
    "        # 分别计算输入门(i)、遗忘门(f)、cell 门(g)、输出门(o)\n",
    "        i_t = torch.sigmoid(\n",
    "            w_times_x[:, :h_dim] + w_times_h[:, :h_dim] + b_ih[:h_dim] + b_hh[:h_dim]\n",
    "        )  # 注意都是取前四分之一\n",
    "        f_t = torch.sigmoid(\n",
    "            w_times_x[:, h_dim:h_dim*2] + w_times_h[:, h_dim:h_dim*2] + b_ih[h_dim:h_dim*2] + b_hh[h_dim:h_dim*2]\n",
    "        )\n",
    "        g_t = torch.tanh(\n",
    "            w_times_x[:, h_dim*2:h_dim*3] + w_times_h[:, h_dim*2:h_dim*3] + b_ih[h_dim*2:h_dim*3] + b_hh[h_dim*2:h_dim*3]\n",
    "        )\n",
    "        o_t = torch.sigmoid(\n",
    "            w_times_x[:, h_dim*3:] + w_times_h[:, h_dim*3:] + b_ih[h_dim*3:] + b_hh[h_dim*3:]\n",
    "        )\n",
    "        # 更新 h 和 c\n",
    "        prev_c = f_t * prev_c + i_t * g_t\n",
    "        prev_h = o_t * torch.tanh(prev_c)\n",
    "        \n",
    "        h_out[:, t, :] = prev_h\n",
    "        \n",
    "    return h_out, (prev_h, prev_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 结果验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 22:32:34.953 | INFO     | __main__:<module>:12 - 自己实现的 LSTM 的 output:\n",
      "tensor([[[-0.0311,  0.1824,  0.1643,  0.1683, -0.0137],\n",
      "         [ 0.0815,  0.1817,  0.4121,  0.0662, -0.1147],\n",
      "         [ 0.1193,  0.0724,  0.0465, -0.1205,  0.0842]],\n",
      "\n",
      "        [[-0.5180, -0.0444, -0.1061,  0.0254,  0.1351],\n",
      "         [-0.1397,  0.0100, -0.0605, -0.0520,  0.2134],\n",
      "         [-0.0461,  0.0964,  0.0451, -0.1317,  0.1228]]], grad_fn=<CopySlices>)\n",
      "2023-01-24 22:32:34.957 | INFO     | __main__:<module>:13 - PyTorch 的 LSTM 的 output:\n",
      "tensor([[[-0.0311,  0.1824,  0.1643,  0.1683, -0.0137],\n",
      "         [ 0.0815,  0.1817,  0.4121,  0.0662, -0.1147],\n",
      "         [ 0.1193,  0.0724,  0.0465, -0.1205,  0.0842]],\n",
      "\n",
      "        [[-0.5180, -0.0444, -0.1061,  0.0254,  0.1351],\n",
      "         [-0.1397,  0.0100, -0.0605, -0.0520,  0.2134],\n",
      "         [-0.0461,  0.0964,  0.0451, -0.1317,  0.1228]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "2023-01-24 22:32:34.960 | INFO     | __main__:<module>:14 - 自己实现的 LSTM 的 h_final:\n",
      "tensor([[ 0.1193,  0.0724,  0.0465, -0.1205,  0.0842],\n",
      "        [-0.0461,  0.0964,  0.0451, -0.1317,  0.1228]], grad_fn=<MulBackward0>)\n",
      "2023-01-24 22:32:34.962 | INFO     | __main__:<module>:15 - PyTorch 的 LSTM 的 h_final:\n",
      "tensor([[[ 0.1193,  0.0724,  0.0465, -0.1205,  0.0842],\n",
      "         [-0.0461,  0.0964,  0.0451, -0.1317,  0.1228]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "2023-01-24 22:32:34.965 | INFO     | __main__:<module>:16 - 自己实现的 LSTM 的 c_final:\n",
      "tensor([[ 0.2016,  0.1874,  0.0864, -0.2599,  0.1177],\n",
      "        [-0.0712,  0.2053,  0.0703, -0.3729,  0.3283]], grad_fn=<AddBackward0>)\n",
      "2023-01-24 22:32:34.967 | INFO     | __main__:<module>:17 - PyTorch 的 LSTM 的 c_final:\n",
      "tensor([[[ 0.2016,  0.1874,  0.0864, -0.2599,  0.1177],\n",
      "         [-0.0712,  0.2053,  0.0703, -0.3729,  0.3283]]],\n",
      "       grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output1, (h_final1, c_final1) = lstm_forward(\n",
    "    input,\n",
    "    (h0, c0),\n",
    "    lstm_layer.weight_ih_l0,\n",
    "    lstm_layer.weight_hh_l0,\n",
    "    lstm_layer.bias_ih_l0,\n",
    "    lstm_layer.bias_hh_l0\n",
    ")\n",
    "\n",
    "output2, (h_final2, c_final2) = lstm_layer(input, (h0.unsqueeze(0), c0.unsqueeze(0)))\n",
    "\n",
    "logger.info(f'自己实现的 LSTM 的 output:\\n{output1}')\n",
    "logger.info(f'PyTorch 的 LSTM 的 output:\\n{output2}')\n",
    "logger.info(f'自己实现的 LSTM 的 h_final:\\n{h_final1}')\n",
    "logger.info(f'PyTorch 的 LSTM 的 h_final:\\n{h_final2}')\n",
    "logger.info(f'自己实现的 LSTM 的 c_final:\\n{c_final1}')\n",
    "logger.info(f'PyTorch 的 LSTM 的 c_final:\\n{c_final2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LSTMP 手写实现\n",
    "\n",
    "### 5.1 PyTorch 官方 API\n",
    "\n",
    "在 PyTorch 的 API 中，只需要在 `nn.LSTM` 实例化时加上一个 `proj_size` 参数即可。\n",
    "\n",
    "这个 projection 的作用就是对 h_dim 进行压缩。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 torch.Size([20, 4])\n",
      "weight_hh_l0 torch.Size([20, 3])\n",
      "bias_ih_l0 torch.Size([20])\n",
      "bias_hh_l0 torch.Size([20])\n",
      "weight_hr_l0 torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "PROJ_SIZE = 3\n",
    "\n",
    "proj_lstm_layer = nn.LSTM(INPUT_SIZE, HIDDEN_SIZE, batch_first=True, proj_size=PROJ_SIZE)\n",
    "\n",
    "for k, v in proj_lstm_layer.named_parameters():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看一下 `proj_lstm_layer` 的参数，可以看到它就是比 `lstm_layer` 多了一个 `weight_hr_l0` 的参数，这个参数就是用来对 hidden state 进行压缩的。\n",
    "\n",
    "因此现在 hidden state 的大小变成了 3 (PROJ_SIZE)，而不是之前的 5 (HIDDEN_SIZE)。\n",
    "\n",
    "从运行结果可以看到，只是对 hidden state 进行了压缩，并没有对记忆单元 c 进行压缩。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 22:32:36.477 | INFO     | __main__:<module>:9 - LSTMP 的 output shape: torch.Size([2, 3, 3])\n",
      "2023-01-24 22:32:36.479 | INFO     | __main__:<module>:10 - LSTMP 的 h_final shape: torch.Size([1, 2, 3])\n",
      "2023-01-24 22:32:36.480 | INFO     | __main__:<module>:11 - LSTMP 的 c_final shape: torch.Size([1, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "c0 = torch.randn(BATCH_SIZE, HIDDEN_SIZE)  # c0 的 shape 与之前一样\n",
    "h0 = torch.randn(BATCH_SIZE, PROJ_SIZE)    # h0 的 shape 由之前的 HIDDEN_SIZE 变成 PROJ_SIZE\n",
    "\n",
    "output, (h_final, c_final) = proj_lstm_layer(\n",
    "    input,\n",
    "    (h0.unsqueeze(0), c0.unsqueeze(0))  # 调用 unsqueeze 是因为我们用的是单向的，需要符合 API 的 shape 要求\n",
    ")\n",
    "\n",
    "logger.info(f'LSTMP 的 output shape: {output.shape}')   # [B, seq_len, proj_size]\n",
    "logger.info(f'LSTMP 的 h_final shape: {h_final.shape}') # [1, B, proj_size]\n",
    "logger.info(f'LSTMP 的 c_final shape: {c_final.shape}') # [1, B, h_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 逐行实现 LSTMP\n",
    "\n",
    "这里只需要对 `lstm_forward` 进行简单修改即可实现：\n",
    "\n",
    "+ 参数中增加一个 `w_hr`，表示 projection，并通过这个参数可以获得 `proj_size`\n",
    "+ 对 `w_hr` 进行扩维，获得 `batch_w_hr`\n",
    "+ 最后的输出 `h_out` 的 shape：(bs, T, h_dim) -> (bs, T, proj_size)\n",
    "+ 在之前计算完 `prev_h` 后，再通过 `w_hr` 对 `prev_h` 进行降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_lstm_forward(\n",
    "    input: Tensor,  # [B, seq_len, h_dim]\n",
    "    initial_states: Tuple[Tensor, Tensor],\n",
    "    w_ih: Tensor,  # [h_dim*4, input_size]\n",
    "    w_hh: Tensor,  # [h_dim*4, h_dim]\n",
    "    b_ih: Tensor,  # [h_dim*4]\n",
    "    b_hh: Tensor,  # [h_dim*4]\n",
    "    w_hr: Tensor   # [proj_size, h_dim*4]\n",
    ") -> Tuple[Tensor, Tuple[Tensor, Tensor]]:\n",
    "    h0, c0 = initial_states  # 初始状态\n",
    "    bs, T, input_size = input.shape\n",
    "    h_dim = w_ih.shape[0] // 4  # 因为 w_ih 是四个 W 拼接起来的\n",
    "    proj_size = w_hr.shape[0]\n",
    "    \n",
    "    prev_h = h0  # [bs, h_dim]\n",
    "    prev_c = c0\n",
    "    # 对 W 进行一下扩维，方便之后与 x 进行 mini-batch 的运算\n",
    "    batch_w_ih = w_ih.unsqueeze(0).tile([bs, 1, 1])  # [bs, h_dim*4, input_size]\n",
    "    batch_w_hh = w_hh.unsqueeze(0).tile([bs, 1, 1])  # [bs, h_dim*4, h_dim]\n",
    "    batch_w_hr = w_hr.unsqueeze(0).tile([bs, 1, 1])  # [bs, proj_size, h_dim]\n",
    "    \n",
    "    h_out = torch.zeros(bs, T, proj_size)  # 输出序列\n",
    "    \n",
    "    for t in range(T):\n",
    "        x = input[:, t, :]  # 当前时刻的输入向量, [bs, input_size]\n",
    "        \n",
    "        x = x.unsqueeze(-1)  # [bs, input_size, 1]\n",
    "        w_times_x = torch.bmm(batch_w_ih, x).squeeze(-1)  # [bs, h_dim*4]\n",
    "        \n",
    "        prev_h = prev_h.unsqueeze(-1)  # [bs, h_dim, 1]\n",
    "        w_times_h = torch.bmm(batch_w_hh, prev_h).squeeze(-1)  #[bs. h_dim*4]\n",
    "        \n",
    "        # 分别计算输入门(i)、遗忘门(f)、cell 门(g)、输出门(o)\n",
    "        i_t = torch.sigmoid(\n",
    "            w_times_x[:, :h_dim] + w_times_h[:, :h_dim] + b_ih[:h_dim] + b_hh[:h_dim]\n",
    "        )  # 注意都是取前四分之一\n",
    "        f_t = torch.sigmoid(\n",
    "            w_times_x[:, h_dim:h_dim*2] + w_times_h[:, h_dim:h_dim*2] + b_ih[h_dim:h_dim*2] + b_hh[h_dim:h_dim*2]\n",
    "        )\n",
    "        g_t = torch.tanh(\n",
    "            w_times_x[:, h_dim*2:h_dim*3] + w_times_h[:, h_dim*2:h_dim*3] + b_ih[h_dim*2:h_dim*3] + b_hh[h_dim*2:h_dim*3]\n",
    "        )\n",
    "        o_t = torch.sigmoid(\n",
    "            w_times_x[:, h_dim*3:] + w_times_h[:, h_dim*3:] + b_ih[h_dim*3:] + b_hh[h_dim*3:]\n",
    "        )\n",
    "        # 更新 h 和 c\n",
    "        prev_c = f_t * prev_c + i_t * g_t\n",
    "        prev_h = o_t * torch.tanh(prev_c)  # [bs, h_dim]\n",
    "        \n",
    "        # 进行 projection\n",
    "        prev_h.unsqueeze_(-1)  # [bs, h_dim, 1]\n",
    "        prev_h = torch.bmm(batch_w_hr, prev_h).squeeze(-1)  # [bs, proj_size]\n",
    "        \n",
    "        h_out[:, t, :] = prev_h\n",
    "        \n",
    "    return h_out, (prev_h, prev_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 结果验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 22:32:37.538 | INFO     | __main__:<module>:16 - 自己实现的 LSTMP 的 output:\n",
      "tensor([[[-0.0081,  0.0343,  0.1460],\n",
      "         [ 0.0099, -0.1058,  0.0905],\n",
      "         [ 0.0034, -0.0762,  0.0565]],\n",
      "\n",
      "        [[-0.1582, -0.1119,  0.0804],\n",
      "         [ 0.0805, -0.1801, -0.0130],\n",
      "         [ 0.1176, -0.1727, -0.0920]]], grad_fn=<CopySlices>)\n",
      "2023-01-24 22:32:37.541 | INFO     | __main__:<module>:17 - PyTorch 的 LSTMP 的 output:\n",
      "tensor([[[-0.0081,  0.0343,  0.1460],\n",
      "         [ 0.0099, -0.1058,  0.0905],\n",
      "         [ 0.0034, -0.0762,  0.0565]],\n",
      "\n",
      "        [[-0.1582, -0.1119,  0.0804],\n",
      "         [ 0.0805, -0.1801, -0.0130],\n",
      "         [ 0.1176, -0.1727, -0.0920]]], grad_fn=<TransposeBackward0>)\n",
      "2023-01-24 22:32:37.544 | INFO     | __main__:<module>:18 - 自己实现的 LSTMP 的 h_final:\n",
      "tensor([[ 0.0034, -0.0762,  0.0565],\n",
      "        [ 0.1176, -0.1727, -0.0920]], grad_fn=<SqueezeBackward1>)\n",
      "2023-01-24 22:32:37.546 | INFO     | __main__:<module>:19 - PyTorch 的 LSTMP 的 h_final:\n",
      "tensor([[[ 0.0034, -0.0762,  0.0565],\n",
      "         [ 0.1176, -0.1727, -0.0920]]], grad_fn=<StackBackward0>)\n",
      "2023-01-24 22:32:37.549 | INFO     | __main__:<module>:20 - 自己实现的 LSTMP 的 c_final:\n",
      "tensor([[-0.1759,  0.8397,  0.0971,  0.0149,  0.2820],\n",
      "        [ 0.0997, -0.2912, -0.7476, -0.2234,  0.3438]], grad_fn=<AddBackward0>)\n",
      "2023-01-24 22:32:37.551 | INFO     | __main__:<module>:21 - PyTorch 的 LSTMP 的 c_final:\n",
      "tensor([[[-0.1759,  0.8397,  0.0971,  0.0149,  0.2820],\n",
      "         [ 0.0997, -0.2912, -0.7476, -0.2234,  0.3438]]],\n",
      "       grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output1, (h_final1, c_final1) = proj_lstm_forward(\n",
    "    input,\n",
    "    (h0, c0),\n",
    "    proj_lstm_layer.weight_ih_l0,\n",
    "    proj_lstm_layer.weight_hh_l0,\n",
    "    proj_lstm_layer.bias_ih_l0,\n",
    "    proj_lstm_layer.bias_hh_l0,\n",
    "    proj_lstm_layer.weight_hr_l0\n",
    ")\n",
    "\n",
    "output2, (h_final2, c_final2) = proj_lstm_layer(\n",
    "    input,\n",
    "    (h0.unsqueeze(0), c0.unsqueeze(0))  # 调用 unsqueeze 是因为我们用的是单向的，需要符合 API 的 shape 要求\n",
    ")\n",
    "\n",
    "logger.info(f'自己实现的 LSTMP 的 output:\\n{output1}')\n",
    "logger.info(f'PyTorch 的 LSTMP 的 output:\\n{output2}')\n",
    "logger.info(f'自己实现的 LSTMP 的 h_final:\\n{h_final1}')\n",
    "logger.info(f'PyTorch 的 LSTMP 的 h_final:\\n{h_final2}')\n",
    "logger.info(f'自己实现的 LSTMP 的 c_final:\\n{c_final1}')\n",
    "logger.info(f'PyTorch 的 LSTMP 的 c_final:\\n{c_final2}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. GRU 手写实现\n",
    "\n",
    "+ 视频：[31、PyTorch GRU的原理及其手写复现](https://www.bilibili.com/video/BV1jm4y1Q7uh/)\n",
    "+ [PyTorch GRU 官方文档](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html)\n",
    "\n",
    "相比于 LSTM，GRU 只有一个初始状态 `h`，而且参数量也少于 LSTM（通过模型内部的计算公式可以看出）。\n",
    "\n",
    "关于在 PyTorch 中如何计算 model 的参数量，可以参考 [num_of_params.py](https://gist.github.com/yubinCloud/3e09dd71437d1ecfefbd54250a029da1)。\n",
    "\n",
    "GRU 的计算公式：\n",
    "\n",
    "+ 重置门：$r_t = \\sigma(W_{ir}x_t + b_{ir} + W_{hr}h_{t-1}+b_{hr})$\n",
    "+ 更新门：$z_t = \\sigma(W_{iz}x_t + b_{iz} + W_{hz}h_{t-1}+b_{hz})$\n",
    "+ 候选状态：$n_t = \\tanh(W_{in}x_t + b_{in} + r_t * (W_{hn}h_{(t-1)} + b_{hn}))$\n",
    "+ 隐藏状态的增量更新：$h_t = (1 - z_t) * n_t + z_t * h_{(t-1)}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义常量\n",
    "BATCH_SIZE = 2\n",
    "SEQ_LEN = 3\n",
    "INPUT_SIZE = 4\n",
    "HIDDEN_SIZE = 5\n",
    "\n",
    "input = torch.randn(BATCH_SIZE, SEQ_LEN, INPUT_SIZE)  # 输入序列\n",
    "h0 = torch.randn(BATCH_SIZE, HIDDEN_SIZE)  # 初始值，不需要训练"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 PyTorch 的官方 API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 22:32:38.655 | INFO     | __main__:<module>:4 - GRU 的 output shape: torch.Size([2, 3, 5])\n",
      "2023-01-24 22:32:38.656 | INFO     | __main__:<module>:5 - GRU 的 h_final shape: torch.Size([1, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "gru_layer = nn.GRU(INPUT_SIZE, HIDDEN_SIZE, batch_first=True)\n",
    "output, h_final = gru_layer(input, h0.unsqueeze(0))\n",
    "\n",
    "logger.info(f'GRU 的 output shape: {output.shape}')  # [bs, seq_len, h_dim]\n",
    "logger.info(f'GRU 的 h_final shape: {h_final.shape}') # [1, bs, h_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 torch.Size([15, 4])\n",
      "weight_hh_l0 torch.Size([15, 5])\n",
      "bias_ih_l0 torch.Size([15])\n",
      "bias_hh_l0 torch.Size([15])\n"
     ]
    }
   ],
   "source": [
    "for k, v in gru_layer.named_parameters():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 逐行实现 GRU\n",
    "\n",
    "GRU 的计算公式：\n",
    "\n",
    "+ 重置门：$r_t = \\sigma(W_{ir}x_t + b_{ir} + W_{hr}h_{t-1}+b_{hr})$\n",
    "+ 更新门：$z_t = \\sigma(W_{iz}x_t + b_{iz} + W_{hz}h_{t-1}+b_{hz})$\n",
    "+ 候选状态：$n_t = \\tanh(W_{in}x_t + b_{in} + r_t * (W_{hn}h_{(t-1)} + b_{hn}))$\n",
    "+ 隐藏状态的增量更新：$h_t = (1 - z_t) * n_t + z_t * h_{(t-1)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_forward(\n",
    "    input: Tensor,  # [bs, seq_len, input_size]\n",
    "    initial_states: Tensor,\n",
    "    w_ih: Tensor,  # [h_dim*3, input_size]\n",
    "    w_hh: Tensor,\n",
    "    b_ih: Tensor,\n",
    "    b_hh: Tensor\n",
    "):\n",
    "    prev_h = initial_states\n",
    "    bs, T, input_size = input.shape\n",
    "    h_dim = w_ih.shape[0] // 3\n",
    "    \n",
    "    # 对权重扩维，复制成 batch_size 倍\n",
    "    batch_w_ih = w_ih.unsqueeze(0).tile([bs, 1, 1])\n",
    "    batch_w_hh = w_hh.unsqueeze(0).tile([bs, 1, 1])\n",
    "    \n",
    "    h_out = torch.zeros(bs, T, h_dim)  # GRU 网络的输出状态序列\n",
    "    \n",
    "    for t in range(T):\n",
    "        x = input[:, t, :]  # 此时 GRU cell 的输入 feature vector, [bs, input_size]\n",
    "        \n",
    "        w_times_x = torch.bmm(batch_w_ih, x.unsqueeze(-1)).squeeze(-1)  # [bs, 3*h_dim]\n",
    "        w_times_h = torch.bmm(batch_w_hh, prev_h.unsqueeze(-1)).squeeze(-1)  # [bs, 3*h_dim]\n",
    "        \n",
    "        # 计算重置门和更新门\n",
    "        r_t = torch.sigmoid(\n",
    "            w_times_x[:, :h_dim] + w_times_h[:, :h_dim] + b_ih[:h_dim] + b_hh[:h_dim]\n",
    "        )\n",
    "        z_t = torch.sigmoid(\n",
    "            w_times_x[:, h_dim:h_dim*2] + w_times_h[:, h_dim:h_dim*2] + b_ih[h_dim:h_dim*2] + b_hh[h_dim:h_dim*2]\n",
    "        )\n",
    "        # 计算候选状态\n",
    "        n_t = torch.tanh(\n",
    "            w_times_x[:, 2*h_dim:3*h_dim] + b_ih[2*h_dim:3*h_dim] + r_t * (w_times_h[:, 2*h_dim:3*h_dim] + b_hh[2*h_dim:3*h_dim])\n",
    "        )\n",
    "        # 增量更新得到当前时刻的最新隐藏状态\n",
    "        prev_h = (1 - z_t) * n_t + z_t * prev_h\n",
    "        \n",
    "        h_out[:, t, :] = prev_h\n",
    "    \n",
    "    return h_out, prev_h"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 结果验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 22:32:39.890 | INFO     | __main__:<module>:14 - 自己实现的 GRU 的 output:\n",
      "tensor([[[ 0.3792, -0.0641, -0.4533,  0.1585, -0.4716],\n",
      "         [ 0.4597, -0.0958,  0.2241, -0.0230, -0.2573],\n",
      "         [ 0.3011,  0.0567, -0.5498,  0.1720,  0.3509]],\n",
      "\n",
      "        [[ 0.8877,  0.5805, -0.5134,  0.0334,  0.0081],\n",
      "         [ 0.7638,  0.6401, -0.5307,  0.2876, -0.0665],\n",
      "         [ 0.5887,  0.2752, -0.6436, -0.2113,  0.2936]]], grad_fn=<CopySlices>)\n",
      "2023-01-24 22:32:39.894 | INFO     | __main__:<module>:15 - PyTorch 的 GRU 的 output:\n",
      "tensor([[[ 0.3792, -0.0641, -0.4533,  0.1585, -0.4716],\n",
      "         [ 0.4597, -0.0958,  0.2241, -0.0230, -0.2573],\n",
      "         [ 0.3011,  0.0567, -0.5498,  0.1720,  0.3509]],\n",
      "\n",
      "        [[ 0.8877,  0.5805, -0.5134,  0.0334,  0.0081],\n",
      "         [ 0.7638,  0.6401, -0.5307,  0.2876, -0.0665],\n",
      "         [ 0.5887,  0.2752, -0.6436, -0.2113,  0.2936]]],\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "2023-01-24 22:32:39.897 | INFO     | __main__:<module>:16 - 自己实现的 GRU 的 h_final:\n",
      "tensor([[ 0.3011,  0.0567, -0.5498,  0.1720,  0.3509],\n",
      "        [ 0.5887,  0.2752, -0.6436, -0.2113,  0.2936]], grad_fn=<AddBackward0>)\n",
      "2023-01-24 22:32:39.899 | INFO     | __main__:<module>:17 - PyTorch 的 GRU 的 h_final:\n",
      "tensor([[[ 0.3011,  0.0567, -0.5498,  0.1720,  0.3509],\n",
      "         [ 0.5887,  0.2752, -0.6436, -0.2113,  0.2936]]],\n",
      "       grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 调用自定义的 GRU\n",
    "output1, h_final1 = gru_forward(\n",
    "    input,\n",
    "    h0,\n",
    "    gru_layer.weight_ih_l0,\n",
    "    gru_layer.weight_hh_l0,\n",
    "    gru_layer.bias_ih_l0,\n",
    "    gru_layer.bias_hh_l0\n",
    ")\n",
    "\n",
    "# 调用 PyTorch 官网 API\n",
    "output2, h_final2 = gru_layer(input, h0.unsqueeze(0))\n",
    "\n",
    "logger.info(f'自己实现的 GRU 的 output:\\n{output1}')\n",
    "logger.info(f'PyTorch 的 GRU 的 output:\\n{output2}')\n",
    "logger.info(f'自己实现的 GRU 的 h_final:\\n{h_final1}')\n",
    "logger.info(f'PyTorch 的 GRU 的 h_final:\\n{h_final2}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LHY",
   "language": "python",
   "name": "lhy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "0418effca45178467ac68c18e34d93809a092be692e0a4443d8690099b71f4bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
