{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention-based Seq2Seq\n",
    "\n",
    "+ 视频：[35、基于PyTorch手写Attention-based Seq2Seq模型](https://www.bilibili.com/video/BV1ML411w7k7/)\n",
    "\n",
    "以 NMT 任务为例，实现基于注意力机制的 Seq2Seq 模型。\n",
    "\n",
    "**NMT**：Nerual Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "此处 Encoder 使用 LSTM 来实现，将 input sequence 建模为上下文相关的 vector sequence。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    实现基于 LSTM 的 encoder，也可以是其他类型的，比如 CNN、TransformerEncoder\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int, hidden_size: int, src_vocab_size: int) -> None:\n",
    "        \"\"\"\n",
    "        :param embed_dim: token 的嵌入维度\n",
    "        :param hidden_size: LSTM 的隐藏状态的大小\n",
    "        :param src_vocab_size: 输入词汇表的大小\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm_layer = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.embedding_table = nn.Embedding(src_vocab_size, embed_dim)\n",
    "    \n",
    "    def forward(self, input_ids: Tensor):\n",
    "        \"\"\"\n",
    "        :param input_ids: 由 token id 组成的输入序列\n",
    "        :return: LSTM 输出的 output 和 h_final\n",
    "        LSTM 的 output: 各时刻输出的 hidden state，形状 [batch, seq_len, num_directions * hidden_size]\n",
    "        h_final: LSTM 最后时刻的 hidden state，形状 [num_directions * num_layers, hidden_size]\n",
    "        \"\"\"\n",
    "        input_seq = self.embedding_table(input_ids)  # [batch, seq_len, embed_dim]\n",
    "        output_states, (h_final, c_final) = self.lstm_layer(input_seq)\n",
    "        return output_states, h_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意力机制\n",
    "\n",
    "这里使用 dot-product 的 Attention 机制，其中没有可学习的参数。\n",
    "\n",
    "我们假设 input 不是流式的，也就是可以一次性拿到一整个 input sequence，进而经过 encoder 直接得到整个 `encoder_states`。\n",
    "\n",
    "由于 decoder 是自回归形式来产生结果的，所以 decoder 每次运算都会调用一次 AttentionMachanism 的 forward 函数来获得 context vector，并使用它来进行 decode。\n",
    "\n",
    "这里的 Attention 是假设了 encoder 与 decoder 的 hidden_size 是大小一致的。如果不一致的话，可以在实现中加一层 MLP 映射一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqAttentionMachanism(nn.Module):\n",
    "    \"\"\"\n",
    "    实现了 dot-product 的 Attention\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, decoder_state_t: Tensor, encoder_states: Tensor):\n",
    "        \"\"\"\n",
    "\n",
    "        :param decoder_state_t: t 时刻的 decoder state，[batch, hidden_size]\n",
    "        :param encoder_states: 输入序列的全部位置的 encoder states，[batch, src_len, hidden_size]\n",
    "        \"\"\"\n",
    "        bs, src_len, hidden_size = encoder_states.shape\n",
    "        \n",
    "        # 计算 t 时刻的 decoder state 与全部位置的 encoder states 的 attention scores\n",
    "        # 由于 decoder_state_t 只是一个时刻的 state，因此需要先对 decoder_state_t 进行扩维\n",
    "        decoder_state_t = decoder_state_t.unsqueeze(1)  # [batch, 1, hidden_size]\n",
    "        decoder_state_t = decoder_state_t.tile([1, src_len, 1])  # [batch, src_len, hidden_size]\n",
    "        # 计算 scores 与 scores 的规范化\n",
    "        scores = torch.sum(decoder_state_t * encoder_states, dim=-1)  # [bs, src_len]\n",
    "        attn_probs = F.softmax(scores, dim=-1)  # 也就是 attention weights，[bs, src_len]\n",
    "        \n",
    "        # 获得 context vector，它就是 t 时刻 decoder 所需要的上下文向量\n",
    "        context = torch.sum(attn_probs.unsqueeze(-1) * encoder_states, dim=1)  # 按照 attention weights 加权求和\n",
    "        \n",
    "        return attn_probs, context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "训练阶段使用 **Teacher Force** 的训练方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        hidden_size: int,\n",
    "        num_classes: int,\n",
    "        target_vocab_size: int,\n",
    "        start_id: int,\n",
    "        end_id: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes  # 目标领域的 token 个数，在 NMT 任务中，其实就是 target_vocab_size\n",
    "        self.start_id = start_id  # 最开始的输入\n",
    "        self.end_id = end_id  # 指示预测的结束\n",
    "        \n",
    "        self.lstm_cell = nn.LSTMCell(embed_dim, hidden_size)\n",
    "        self.proj_layer = nn.Linear(hidden_size * 2, num_classes)  # 映射到分类的分布上，这里输入选择 hidden_size * 2，是因为我们参考论文将 context 和 decoder state 一起拼起来送给 linear 层来分类\n",
    "        self.attn_machanism = Seq2SeqAttentionMachanism()  # 用于获得 decoder 所需要的 context vector\n",
    "        self.embedding_table = nn.Embedding(target_vocab_size, embed_dim)  # 目标序列的 embedding\n",
    "    \n",
    "    def forward(self, shifted_target_ids: Tensor, encoder_states: Tensor):\n",
    "        \"\"\"\n",
    "        训练阶段调用\n",
    "        这里每一时刻都是把 target token 输给 decoder，因此这种训练方式也叫“Teacher Force”的训练\n",
    "        :param shifted_target_ids: Decoder 的输入，这与预测向右偏移了一位，因此这个序列的第一个位置是一个 start_id\n",
    "        :param encoder_states: 完整的 encoder 的输出 states\n",
    "        \"\"\"\n",
    "        shifted_target = self.embedding_table(shifted_target_ids)\n",
    "        \n",
    "        bs, target_len, embed_dim = shifted_target.shape\n",
    "        bs, src_len, hidden_size = encoder_states.shape\n",
    "        \n",
    "        logits = torch.zeros(bs, target_len, self.num_classes)  # 每一个时刻的分类的分布\n",
    "        probs = torch.zeros(bs, target_len, src_len)\n",
    "        \n",
    "        # 在训练阶段，我们的 target length 是知道的，因此这里可以用一个 for 循环来完成一个 sequence 的训练\n",
    "        # 若在 inference 阶段，那么 target length 是不知道的，那么无法使用 for 循环\n",
    "        for t in range(target_len):\n",
    "            decoder_input_t = shifted_target[:, t, :]  # 当前时刻的 decoder 的 input，[bs, embed_dim]\n",
    "            # 由于只有一步，所以我们才使用 LSTMCell\n",
    "            if t == 0:\n",
    "                h_t, c_t = self.lstm_cell(decoder_input_t)\n",
    "            else:\n",
    "                h_t, c_t = self.lstm_cell(decoder_input_t, (h_t, c_t))\n",
    "            \n",
    "            # 让当前时刻 decoder state 与 encoder states 做 Attention，得到 context vector\n",
    "            attn_prob, context = self.attn_machanism(h_t, encoder_states)\n",
    "            \n",
    "            # 将 context 和 decoder state 拼接在一起，一同丢给 linear 来做分类\n",
    "            decoder_output = torch.cat([context, h_t], dim=-1)\n",
    "            \n",
    "            logits[:, t, :] = self.proj_layer(decoder_output)  # 当前时刻的分类分布\n",
    "            probs[:, t, :] = attn_prob  # 当前 decoder state 与 encoder states 的 attention weights\n",
    "            \n",
    "        return probs, logits\n",
    "    \n",
    "    def inference(self, encoder_states: Tensor):\n",
    "        \"\"\"\n",
    "        推理阶段使用\n",
    "        :param encoder_states: encoder 的完整 states\n",
    "        \"\"\"\n",
    "        target_id = torch.tensor([self.start_id], dtype=torch.int32)  # 固定以 start_id 开始\n",
    "        print('target_id', target_id)\n",
    "        h_t = None\n",
    "        result = []\n",
    "        \n",
    "        while True:\n",
    "            decoder_input_t = self.embedding_table(target_id)\n",
    "            if h_t is None:\n",
    "                h_t, c_t = self.lstm_cell(decoder_input_t)\n",
    "            else:\n",
    "                h_t, c_t = self.lstm_cell(decoder_input_t, (h_t, c_t))\n",
    "            \n",
    "            attn_prob, context = self.attn_machanism(h_t, encoder_states)\n",
    "            \n",
    "            decoder_output = torch.cat([context, h_t], dim=-1)\n",
    "            logits = self.proj_layer(decoder_output)\n",
    "            \n",
    "            target_id = torch.argmax(logits, -1)  # 分类结果\n",
    "            print('target_id', target_id)\n",
    "            result.append(target_id)\n",
    "            \n",
    "            if torch.any(target_id == self.end_id):  # 解码的终止条件\n",
    "                # stop decoding\n",
    "                break\n",
    "        \n",
    "        predicted_ids = torch.tensor(result).reshape([-1])\n",
    "        return predicted_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq\n",
    "\n",
    "将 Encoder 与 Decoder 结合起来，形成一个完整的 Seq2Seq model。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        hidden_size: int,\n",
    "        num_classes: int,\n",
    "        src_vocab_size: int,\n",
    "        target_vocab_size: int,\n",
    "        start_id: int,\n",
    "        end_id: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Seq2SeqEncoder(embed_dim, hidden_size, src_vocab_size)\n",
    "        \n",
    "        self.decoder = Seq2SeqDecoder(embed_dim, hidden_size, num_classes,\n",
    "                                      target_vocab_size, start_id, end_id)\n",
    "        \n",
    "    def forward(self, input_seq_ids: Tensor, shifted_target_ids: Tensor):\n",
    "        \"\"\"\n",
    "        训练阶段使用\n",
    "        :param input_seq_ids: src sequence 的 id 序列，[batch, src_seq_len]\n",
    "        :param shifted_target_ids: target sequence 的向右偏移一个单位的 id 序列，[batch, target_seq_len]\n",
    "        \"\"\"\n",
    "        encoder_states, h_final = self.encoder(input_seq_ids)\n",
    "        probs, logits = self.decoder(shifted_target_ids, encoder_states)\n",
    "        return probs, logits\n",
    "    \n",
    "    def inference(self, input_seq_ids: Tensor):\n",
    "        \"\"\"\n",
    "        推理阶段使用\n",
    "        :param input_seq_ids: src sequence 的 id 序列，[]\n",
    "        \"\"\"\n",
    "        encoder_states, h_final = self.encoder(input_seq_ids)\n",
    "        predicted_ids = self.decoder.inference(encoder_states)\n",
    "        return predicted_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主程序\n",
    "\n",
    "这里没有进行训练，在实际任务，根据任务类型来选择 loss function：\n",
    "\n",
    "+ 若是分类任务，则采用 cross-entropy；\n",
    "+ 若是回归任务，则采用欧氏距离 L1 或 L2。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs shape:torch.Size([2, 5, 3])\n",
      "logits shape:torch.Size([2, 5, 100])\n"
     ]
    }
   ],
   "source": [
    "SRC_LEN = 3  # src seq length\n",
    "TGT_LEN = 4  # target seq length\n",
    "EMBED_DIM = 8\n",
    "HIDDEN_SIZE = 16\n",
    "BATCH_SIZE = 2\n",
    "START_ID = 0\n",
    "END_ID = 0\n",
    "SRC_VOCAB_SIZE = 100  # src 的词表大小\n",
    "TGT_VOCAB_SIZE = 100  # tgt 的词表大小\n",
    "NUM_CLASSES = TGT_VOCAB_SIZE\n",
    "\n",
    "input_seq_ids = torch.randint(low=0, high=SRC_VOCAB_SIZE, size=[BATCH_SIZE, SRC_LEN]).to(torch.int32)\n",
    "\n",
    "target_ids = torch.randint(low=0, high=TGT_VOCAB_SIZE, size=[BATCH_SIZE, TGT_LEN])\n",
    "target_ids = torch.cat([target_ids, END_ID * torch.ones(BATCH_SIZE, 1)], dim=1)  # 在每个 seq 的结尾加了个 end_id\n",
    "\n",
    "# 将 target seq 右移一位，这里丢弃了第一个 token，实际上不应该丢弃的，但为了方便\n",
    "shifted_target_ids = torch.cat([START_ID * torch.ones(BATCH_SIZE, 1), target_ids[:, 1:]], dim=1).to(torch.int32)\n",
    "\n",
    "model = Seq2Seq(EMBED_DIM, HIDDEN_SIZE, NUM_CLASSES, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, START_ID, END_ID)\n",
    "probs, logits = model(input_seq_ids, shifted_target_ids)\n",
    "\n",
    "print(f'probs shape:{probs.shape}')\n",
    "print(f'logits shape:{logits.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "往下就是 inference 过程了，但由于 model 没有被训练，这会导致出现不正常的输出，因此下面就不运行了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_seq_ids = torch.randint(low=0, high=SRC_VOCAB_SIZE, size=[1, SRC_LEN]).to(torch.int32)\n",
    "print('input: ', test_input_seq_ids)\n",
    "output_seq = model.inference(test_input_seq_ids)\n",
    "print('output: ', output_seq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "0418effca45178467ac68c18e34d93809a092be692e0a4443d8690099b71f4bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
