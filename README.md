# MLtoys

Machine learning toys that help me learn.

## Description

+ [deep-thoughts](./deep-thoughts/): The code of bilibili up owner [deep-thoughts](https://space.bilibili.com/373596439) in action.

| Loc   | Description | Tags |
| :---: | :---       | :---  |
|[Transformer 难点逐行实现](./deep-thoughts/Transformer%20%E9%9A%BE%E7%82%B9%E7%90%86%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0.ipynb) | 使用 PyTorch 逐行实现了 Transformer 中关键的 Self-Attention、Position Embedding、三种不同的 mask 和 mask loss | Transformer, Self-Attention, Mask |
| [RNN 复现](./deep-thoughts/RNN%20%E5%A4%8D%E7%8E%B0.ipynb) | 使用 PyTorch 逐行实现了 RNN 和双向 RNN，并与官方实现的运算结果进行了对比，结果一致 | RNN, BiRNN |

+ [LHY-HW](./LHY-HW/): Homework for Hongyi Lee's course


| Loc   | Description | Tags |
| :---: | :---       | :---  |
|[HW4-2021-speaker-classification](./LHY-HW/HW5-2021-Seq2Seq.ipynb) | Classify the speakers of given features. | Transformer, speech processing |
|[HW5-2021-Seq2Seq](./LHY-HW/HW5-2021-Seq2Seq.ipynb) | Using the seq2seq architecture for machine translation tasks. | NMT, RNN, Attention, Transformer, seq2seq |
